///fold:
var softplus = function(x) {
    return Math.log(Math.exp(x) + 1);
};

var initList = function(n, val) {
  return repeat(n, function() {return val;});
};

var uniformPs = function(vs) {
  return initList(vs.length, 1/vs.length);
};

var getTrajectories = function(data) {
  var keys = _.keys(data[0]);
  return reduce(function(key, memo) {
    var timeBasedKeys = map(function(i) {return key + "." + i;}, _.range(data.length));
    var vals = _.map(data, key);
    return _.extend(_.zipObject(timeBasedKeys, vals), memo);
  }, [], keys);
};
///

// possible states of the world
var states = [{type: 0, color: 0}, 
              {type: 1, color: 1}];
var statePrior =  Categorical({vs: states, ps: [1/2, 1/2]});

// possible base utterances, and possible conjunctions
var unconstrainedUtterances = ['type_a', 'type_b', 'color_a','color_b'];
var derivedUtterances = ['type_a type_b', 'type_a color_a','type_a color_b',
                         'type_b color_a','type_b color_b','color_a color_b'];
var utterances = unconstrainedUtterances.concat(derivedUtterances);
var utterancePrior = Categorical({vs: utterances, ps: uniformPs(utterances)});

// takes a sample from a logit normal distribution for each word,
// representing the extent to which that word describes each object
// at an implementational level, we use the params from the previous
// step as the prior for next step, so that we only condition on the most recent obs
var lexiconPrior = function(){
  var meanings = map(function(utt) {
    var uniqueID = [utt, globalStore.originAgent].join('_');
    var aPrior = utt.split('_')[1] === 'a' ? 2 : 1;
    var bPrior = utt.split('_')[1] === 'a' ? 1 : 2;
    
    var betaGuide = function() {
      return LogitNormal({
        a: 0, b: 1,
      	mu: param({name: ['mu', uniqueID].join('_')}),
      	sigma: softplus(param({name: ['sigma', uniqueID].join('_')}))
      });
    };
    
    return sample(Beta({a: aPrior, b: bPrior}), {guide: betaGuide});
  }, unconstrainedUtterances);
  return _.zipObject(unconstrainedUtterances, meanings);
};

// length-based cost
var uttCost = function(utt) {
  return utt.split(' ').length;
};

// Looks up the meaning of an utterance in a lexicon object
var uttFitness = function(utt, state, lexicon) {
  return Math.log(reduce(function(subUtt, memo) {
    var lexiconProb = lexicon[subUtt] : 1- lexicon[subUtt];
    return lexiconProb * memo;
  }, 1, utt.split(' ')));
};

// literal listener
var L0 = function(utt, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    factor(uttFitness(utt, state, lexicon));
    return state;
  });
};

// pragmatic speaker
var S1 = function(state, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var utt = sample(utterancePrior);
    factor(params.alpha[0] * L0(utt, lexicon).score(state)
           - params.beta[0] * uttCost(utt));
    return utt;
  });
};

// conventional listener
var L1 = function(utt, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    observe(S1(state, lexicon), utt);
    return state;
  });
};

var lexiconModel = function() {
  var lexicon = lexiconPrior();
  if(globalStore.data.length > 0) {
    mapData({data: globalStore.data}, function(datum){
      if(globalStore.originAgent == 'L')
	observe(S1(datum.response, lexicon), datum['utt']);
      else if(globalStore.originAgent == 'S')
	observe(L1(datum.utt, lexicon), datum['response']);
    });
  }
  return lexicon;
};

// compute lexicon posterior, taking into account some previous observations
// speakers do this by assuming data came from knowledgable listener, and vice versa
var updateBeliefs = function(originAgent, data) {
  // Only condition on most recent datapoint, since we're using learned params as prior
  globalStore.data = data;
  globalStore.originAgent = originAgent;
  globalStore.roundNum = data.length;

  Optimize({model: lexiconModel,
	    method: 'optimize', steps: 10000, verbose: false,
	    optMethod: {adam: {stepSize: .001}}});
};

// conventional listener (L1, marginalizing over lexicons)
var L = cache(function(utt, data) {
  updateBeliefs('L', data);
  return Infer({method: 'forward', samples: 10000, guide: true, model: function() {
    var dist = L1(utt, lexiconPrior());
    return sample(dist, {guide() {return dist;}});
  }});
});

// conventional speaker (S1, reasoning about expected L1 behavior across lexicons)
// use importance sampling from optimized lexicon prior
var S = cache(function(state, data) {
  updateBeliefs('S', data);
  return Infer({method: 'enumerate'}, function(){
    var utt = sample(utterancePrior);

    var expectedUtility = expectation(Infer({
      method: 'forward', samples: 10000, guide: true, model: function() {
	var listener = L1(utt, lexiconPrior());
	return params.alpha[0]  * listener.score(state) - params.beta[0] * uttCost(utt);
      }
    }));
    
    factor(expectedUtility);

    return utt;
  });
});

var model = function() {
  setFreshParamsId();
  console.log('sample...');
  var step = function(data) {
    if(data.length > params.numSteps[0]) return getTrajectories(data);
    var state = sample(statePrior);
    var utt = sample(S(state, data));
    var response = sample(L(utt, data));
    var newDatum = {utt, response, intended: state, acc: state == response};
    return step(data.concat(newDatum));
  };
  step([]);
};
