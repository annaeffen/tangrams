{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import csv\n",
    "sys.path.append('../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.attrs import POS\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "from utils.nlp_utils import makeMyPMI, memoize, lemmatize_doc\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to first round\n",
    "version_to_use = 'tangramsSequential_collapsed'\n",
    "d = pd.read_csv('../data/{}.csv'.format(version_to_use))#.rename(index=str, columns={\"contents\": \"text\"})\n",
    "d['text'] = [nlp(text) for text in d_raw['contents']]\n",
    "d['lemmas'] = [lemmatize_doc(parsed_text) for parsed_text in d['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.set_index(['gameid','intendedName', 'repetitionNum'])\n",
    "mux = pd.MultiIndex.from_product([d.index.levels[0], d.index.levels[1],d.index.levels[2]], names=['gameid','intendedName', 'repetitionNum'])\n",
    "d = d.reindex(mux, fill_value=[np.nan]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "tangrams_list = ['A','B','C','D','E','F','G','H','I','J','K','L']\n",
    "for name, group in d.query('repetitionNum == 1').groupby('gameid') :\n",
    "    group_inds = np.array(group.index)\n",
    "    group = group.sort_values('intendedName').reset_index()\n",
    "    # Handle nans\n",
    "    nan_rows = [i for i in range(group.shape[0]) if pd.isna(group.iloc[i,6])]\n",
    "    nan_insert_rows = [k - lag for (lag, k) in enumerate(nan_rows)]\n",
    "    # run tf-idf\n",
    "    docs_dict = Dictionary(doc for doc in group['lemmas'] if not np.any(pd.isna(doc)))\n",
    "    docs_corpus = [docs_dict.doc2bow(doc) for doc in group['lemmas'] if not np.any(pd.isna(doc))]\n",
    "    # note: 'btn' uses boolean for tf term\n",
    "    model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict, smartirs='btn')\n",
    "    docs_tfidf  = model_tfidf[docs_corpus]\n",
    "    docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "    docs_vecs   = np.insert(docs_vecs, nan_insert_rows, np.nan, axis=0)\n",
    "    # map back to df\n",
    "    for i, row in group.iterrows() :\n",
    "        words = [docs_dict[i] for i in range(len(docs_vecs[0,]))]\n",
    "        for j, word in enumerate(words) :\n",
    "            tfidf = docs_vecs[i,j]\n",
    "            rows.append([row['gameid'], row['intendedName'], word, tfidf])\n",
    "#            PMI_df = PMI_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_df = pd.DataFrame(rows,\n",
    "    columns = ['gameid', 'intendedName', 'word', 'tf-idf']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_df.to_csv('PMI.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out PMIs & matching rates for all words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7f77ef5a99fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'POS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'match'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pmi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordList\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mpmi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordList' is not defined"
     ]
    }
   ],
   "source": [
    "with open('outputs/sequential_matchAndPMI.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['word', 'POS', 'match', 'pmi', 'total'])\n",
    "    for word in wordList :\n",
    "        pmi = 0\n",
    "        match = 0\n",
    "        total = 0\n",
    "        for gameid in gameidList:  \n",
    "            memoizedCounts = {}\n",
    "            for tangram in tangramList:\n",
    "                memoizedCounts = memoize(d, gameid, memoizedCounts)\n",
    "                round1WordList = selectTangramRoundWords(d, tangram, \"1\", gameid)\n",
    "                total = total + 1 if word in round1WordList else total\n",
    "                if word in round1WordList :\n",
    "                    PMI_df = makeMyPMI(d, tangram, \"1\", gameid, memoizedCounts)\n",
    "                    pmi = pmi + PMI_df[PMI_df['word'] == word]['logPMI'].tolist()[0]\n",
    "                    round6WordList = selectTangramRoundWords(d, tangram, \"6\", gameid)\n",
    "                    match = (match + 1 if (word in round1WordList and word in round6WordList)\n",
    "                             else match)\n",
    "        writer.writerow([word, POSdict[word], float(match) / float(total), pmi/total, total])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSamples = 1000\n",
    "\n",
    "#grab words with highestPMI for a given tangram/gameid\n",
    "def highestPMIWords(d, tangram, roundNum, gameid):\n",
    "    allTangramCounts = {}\n",
    "    allTangramCounts['counts'] = getWordCounts(d, gameid, \"1\")\n",
    "    allTangramCounts['numWords'] = float(sum(allTangramCounts[\"counts\"].values()))\n",
    "\n",
    "    PMIdf = (makeMyPMI(d, tangram, roundNum, gameid, allTangramCounts))\n",
    "\n",
    "    # Remove numbers\n",
    "    PMIdf['POS'] = [POSdict[word] for word in PMIdf['word']]\n",
    "\n",
    "    #if PMIdf has words, pull out max values, it is empty return it as is\n",
    "    if len(PMIdf.index) > 0:\n",
    "        PMI_values = PMIdf.logPMI.unique()\n",
    "        maxPMI = PMI_values.max()\n",
    "        PMIdf = PMIdf.loc[PMIdf['logPMI'] == maxPMI]\n",
    "        PMIdfword = PMIdf['word']\n",
    "        return PMIdfword.tolist()\n",
    "    else: \n",
    "        return PMIdf\n",
    "\n",
    "with open('outputs/PMIbootstrap.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['sampleNum', 'tangram', 'gameid', 'numCandidates', 'match', 'highest'])\n",
    "    for gameid in gameidList :\n",
    "        for tangram in tangramList :\n",
    "            round1Words = selectTangramRoundWords(d, tangram, \"1\", gameid)\n",
    "            if len(round1Words) > 0:\n",
    "                # First, write highest PMI match\n",
    "                highPMIWords = highestPMIWords(d, tangram, \"1\", gameid)\n",
    "                round6Words = selectTangramRoundWords(d, tangram, \"6\", gameid)\n",
    "                match = np.mean([1 if word in round6Words else 0 for word in highPMIWords ])\n",
    "                writer.writerow([0, tangram, gameid, len(highPMIWords), match, \"highest\"])\n",
    "\n",
    "                # Next, take a bunch of null samples\n",
    "                for i in range(numSamples) :\n",
    "                    randomWord = np.random.choice(round1Words)\n",
    "                    match = np.mean([1 if randomWord in round6Words else 0])\n",
    "                    writer.writerow([i + 1, tangram, gameid, 1, match, \"null\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
