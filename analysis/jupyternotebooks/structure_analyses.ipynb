{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import lots of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial.distance as distance\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "import textacy\n",
    "import textacy.io\n",
    "from utils.nlp_utils import lemmatize_doc\n",
    "from sklearn import manifold\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from spacy.lang.en import English\n",
    "from spacy.attrs import POS\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_to_use = 'tangramsSequential_collapsed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import annotated file & tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_raw = pd.read_csv('../../data/{}.csv'.format(version_to_use))#.rename(index=str, columns={\"contents\": \"text\"})\n",
    "d_raw['text'] = [nlp(text) for text in d_raw['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man on one leg arm left and one leg right\n"
     ]
    }
   ],
   "source": [
    "for s in (d_raw.iloc[-1].text.sents) :\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run spellchecker (using conservative vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import nlp_utils as utils\n",
    "conservative_vectors = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(conservative_vectors.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.building_spell_correction_dictionary(\n",
    "    d_raw.query('taskVersion == \"cued\"'), \n",
    "    conservative_vectors.vocab, \n",
    "    []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all game ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gameid', 'trialNum', 'repetitionNum', 'intendedName', 'contents',\n",
       "       'numRawWords', 'correct', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0057-414228f8-c268-40d6-9349-b35df4f080d9', '0349-951c1418-40e9-48b3-8290-7ed4461f4d54', '0413-e4a76b36-4367-4e30-abf9-93e823913630', '0461-f522f8f4-37dc-4bb0-89bf-9f6bcf43274a', '0711-b03679d3-9904-4263-bd2f-8ec8e7a45af7']\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "gameidList = pd.unique(d.gameid.ravel()).tolist()\n",
    "print(gameidList[0:5])\n",
    "print(len(gameidList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all tangram names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n"
     ]
    }
   ],
   "source": [
    "tangramList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
    "print(tangramList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1: What are most common words & phrases to reduce? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, spacy has no n-gram function, so we use textacy, a convenience wrapper around spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['doc'] = [textacy.make_spacy_doc(row, lang='en_core_web_lg') for row in d['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['tokens'] = [[token.text for token in l if not token.is_punct] for l in d['text']]\n",
    "d['lemmas'] = [lemmatize_doc(text) for text in d['text']]\n",
    "d['pos'] = [[token.pos_ for token in l if not token.is_punct] for l in d['text']]\n",
    "d['noun_chunks'] = [list(l.noun_chunks) for l in d['text']]\n",
    "d['numWords'] = [len([token for token in l if not token.is_punct]) for l in d['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['unigrams'] = [list(textacy.extract.ngrams(nlp(' '.join(doc)), 1, filter_stops = False)) for doc in d['lemmas']]\n",
    "\n",
    "d['bigrams'] = [list(textacy.extract.ngrams(nlp(' '.join(doc)), 2, filter_stops = False)) for doc in d['lemmas']]\n",
    "\n",
    "d['trigrams'] = [list(textacy.extract.ngrams(nlp(' '.join(doc)), 3, filter_stops = False)) for doc in d['lemmas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>trialNum</th>\n",
       "      <th>repetitionNum</th>\n",
       "      <th>intendedName</th>\n",
       "      <th>contents</th>\n",
       "      <th>numRawWords</th>\n",
       "      <th>correct</th>\n",
       "      <th>text</th>\n",
       "      <th>doc</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>numWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0057-414228f8-c268-40d6-9349-b35df4f080d9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>looking for a diamond at the top, triangle poi...</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>(looking, for, a, diamond, at, the, top, ,, tr...</td>\n",
       "      <td>(looking, for, a, diamond, at, the, top, ,, tr...</td>\n",
       "      <td>[(look), (for), (a), (diamond), (at), (the), (...</td>\n",
       "      <td>[(look, for), (for, a), (a, diamond), (diamond...</td>\n",
       "      <td>[(look, for, a), (for, a, diamond), (a, diamon...</td>\n",
       "      <td>[looking, for, a, diamond, at, the, top, trian...</td>\n",
       "      <td>[look, for, a, diamond, at, the, top, triangle...</td>\n",
       "      <td>[VERB, ADP, DET, NOUN, ADP, DET, NOUN, NOUN, V...</td>\n",
       "      <td>[(a, diamond), (the, left), (a, person), (thei...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0057-414228f8-c268-40d6-9349-b35df4f080d9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>G</td>\n",
       "      <td>this one looks like a seal</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>(this, one, looks, like, a, seal)</td>\n",
       "      <td>(this, one, looks, like, a, seal)</td>\n",
       "      <td>[(this), (one), (look), (like), (a), (seal)]</td>\n",
       "      <td>[(this, one), (one, look), (look, like), (like...</td>\n",
       "      <td>[(this, one, look), (one, look, like), (look, ...</td>\n",
       "      <td>[this, one, looks, like, a, seal]</td>\n",
       "      <td>[this, one, look, like, a, seal]</td>\n",
       "      <td>[DET, NUM, VERB, SCONJ, DET, NOUN]</td>\n",
       "      <td>[(a, seal)]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0057-414228f8-c268-40d6-9349-b35df4f080d9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>K</td>\n",
       "      <td>this one looks like a small dog balancing a ba...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>(this, one, looks, like, a, small, dog, balanc...</td>\n",
       "      <td>(this, one, looks, like, a, small, dog, balanc...</td>\n",
       "      <td>[(this), (one), (look), (like), (a), (small), ...</td>\n",
       "      <td>[(this, one), (one, look), (look, like), (like...</td>\n",
       "      <td>[(this, one, look), (one, look, like), (look, ...</td>\n",
       "      <td>[this, one, looks, like, a, small, dog, balanc...</td>\n",
       "      <td>[this, one, look, like, a, small, dog, balance...</td>\n",
       "      <td>[DET, NUM, VERB, SCONJ, DET, ADJ, NOUN, VERB, ...</td>\n",
       "      <td>[(a, small, dog), (a, ball), (its, nose)]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0057-414228f8-c268-40d6-9349-b35df4f080d9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>this looks like one of the spy vs spy guys loo...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>(this, looks, like, one, of, the, spy, vs, spy...</td>\n",
       "      <td>(this, looks, like, one, of, the, spy, vs, spy...</td>\n",
       "      <td>[(this), (look), (like), (one), (of), (the), (...</td>\n",
       "      <td>[(this, look), (look, like), (like, one), (one...</td>\n",
       "      <td>[(this, look, like), (look, like, one), (like,...</td>\n",
       "      <td>[this, looks, like, one, of, the, spy, vs, spy...</td>\n",
       "      <td>[this, look, like, one, of, the, spy, vs, spy,...</td>\n",
       "      <td>[DET, VERB, SCONJ, NUM, ADP, DET, NOUN, ADP, N...</td>\n",
       "      <td>[(the, spy), (spy, guys), (a, flag), (his, rig...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0057-414228f8-c268-40d6-9349-b35df4f080d9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>J</td>\n",
       "      <td>this is a diamond on top of what looks like a ...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>(this, is, a, diamond, on, top, of, what, look...</td>\n",
       "      <td>(this, is, a, diamond, on, top, of, what, look...</td>\n",
       "      <td>[(this), (be), (a), (diamond), (on), (top), (o...</td>\n",
       "      <td>[(this, be), (be, a), (a, diamond), (diamond, ...</td>\n",
       "      <td>[(this, be, a), (be, a, diamond), (a, diamond,...</td>\n",
       "      <td>[this, is, a, diamond, on, top, of, what, look...</td>\n",
       "      <td>[this, be, a, diamond, on, top, of, what, look...</td>\n",
       "      <td>[DET, AUX, DET, NOUN, ADP, NOUN, ADP, PRON, VE...</td>\n",
       "      <td>[(a, diamond), (top), (what), (a, state, -, to...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      gameid  trialNum  repetitionNum  \\\n",
       "0  0057-414228f8-c268-40d6-9349-b35df4f080d9         1              1   \n",
       "1  0057-414228f8-c268-40d6-9349-b35df4f080d9         2              1   \n",
       "2  0057-414228f8-c268-40d6-9349-b35df4f080d9         3              1   \n",
       "3  0057-414228f8-c268-40d6-9349-b35df4f080d9         4              1   \n",
       "4  0057-414228f8-c268-40d6-9349-b35df4f080d9         5              1   \n",
       "\n",
       "  intendedName                                           contents  \\\n",
       "0            B  looking for a diamond at the top, triangle poi...   \n",
       "1            G                         this one looks like a seal   \n",
       "2            K  this one looks like a small dog balancing a ba...   \n",
       "3            A  this looks like one of the spy vs spy guys loo...   \n",
       "4            J  this is a diamond on top of what looks like a ...   \n",
       "\n",
       "   numRawWords  correct                                               text  \\\n",
       "0           47        1  (looking, for, a, diamond, at, the, top, ,, tr...   \n",
       "1            6        0                  (this, one, looks, like, a, seal)   \n",
       "2           13        0  (this, one, looks, like, a, small, dog, balanc...   \n",
       "3           21        1  (this, looks, like, one, of, the, spy, vs, spy...   \n",
       "4           20        1  (this, is, a, diamond, on, top, of, what, look...   \n",
       "\n",
       "                                                 doc  \\\n",
       "0  (looking, for, a, diamond, at, the, top, ,, tr...   \n",
       "1                  (this, one, looks, like, a, seal)   \n",
       "2  (this, one, looks, like, a, small, dog, balanc...   \n",
       "3  (this, looks, like, one, of, the, spy, vs, spy...   \n",
       "4  (this, is, a, diamond, on, top, of, what, look...   \n",
       "\n",
       "                                            unigrams  \\\n",
       "0  [(look), (for), (a), (diamond), (at), (the), (...   \n",
       "1       [(this), (one), (look), (like), (a), (seal)]   \n",
       "2  [(this), (one), (look), (like), (a), (small), ...   \n",
       "3  [(this), (look), (like), (one), (of), (the), (...   \n",
       "4  [(this), (be), (a), (diamond), (on), (top), (o...   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(look, for), (for, a), (a, diamond), (diamond...   \n",
       "1  [(this, one), (one, look), (look, like), (like...   \n",
       "2  [(this, one), (one, look), (look, like), (like...   \n",
       "3  [(this, look), (look, like), (like, one), (one...   \n",
       "4  [(this, be), (be, a), (a, diamond), (diamond, ...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  [(look, for, a), (for, a, diamond), (a, diamon...   \n",
       "1  [(this, one, look), (one, look, like), (look, ...   \n",
       "2  [(this, one, look), (one, look, like), (look, ...   \n",
       "3  [(this, look, like), (look, like, one), (like,...   \n",
       "4  [(this, be, a), (be, a, diamond), (a, diamond,...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [looking, for, a, diamond, at, the, top, trian...   \n",
       "1                  [this, one, looks, like, a, seal]   \n",
       "2  [this, one, looks, like, a, small, dog, balanc...   \n",
       "3  [this, looks, like, one, of, the, spy, vs, spy...   \n",
       "4  [this, is, a, diamond, on, top, of, what, look...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [look, for, a, diamond, at, the, top, triangle...   \n",
       "1                   [this, one, look, like, a, seal]   \n",
       "2  [this, one, look, like, a, small, dog, balance...   \n",
       "3  [this, look, like, one, of, the, spy, vs, spy,...   \n",
       "4  [this, be, a, diamond, on, top, of, what, look...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [VERB, ADP, DET, NOUN, ADP, DET, NOUN, NOUN, V...   \n",
       "1                 [DET, NUM, VERB, SCONJ, DET, NOUN]   \n",
       "2  [DET, NUM, VERB, SCONJ, DET, ADJ, NOUN, VERB, ...   \n",
       "3  [DET, VERB, SCONJ, NUM, ADP, DET, NOUN, ADP, N...   \n",
       "4  [DET, AUX, DET, NOUN, ADP, NOUN, ADP, PRON, VE...   \n",
       "\n",
       "                                         noun_chunks  numWords  \n",
       "0  [(a, diamond), (the, left), (a, person), (thei...        48  \n",
       "1                                        [(a, seal)]         6  \n",
       "2          [(a, small, dog), (a, ball), (its, nose)]        13  \n",
       "3  [(the, spy), (spy, guys), (a, flag), (his, rig...        21  \n",
       "4  [(a, diamond), (top), (what), (a, state, -, to...        19  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(countType, df, gameid, repetitionNum, tangram = None) :\n",
    "    roundCond = 'repetitionNum == ' + repetitionNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'tangramRef == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([str(item) for sublist in relevantRow[countType]\n",
    "                    for item in sublist])\n",
    "\n",
    "for countType in ['unigrams', 'bigrams', 'trigrams'] :\n",
    "    flattenedContents = [str(item) for sublist in d[countType]\n",
    "                         for item in sublist]\n",
    "    countDict = Counter(flattenedContents)\n",
    "    wordList = [v for (v,count) in countDict.items() if count > 20]\n",
    "    with open('../outputs/' + countType + 'Counts.csv', 'w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['gameid', 'repetitionNum', 'word', 'count'])\n",
    "        for gameid in gameidList:  \n",
    "            for repetitionNum in ['1', '2', '3', '4', '5', '6'] :\n",
    "                counts = getCounts(countType, d, gameid, repetitionNum)\n",
    "                for word in wordList :\n",
    "                    writer.writerow([gameid, repetitionNum, word, counts[word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1 Extract parts of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get counts for each POS label and export for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VERB': 6656, 'ADP': 5657, 'DET': 4390, 'NOUN': 12834, 'PUNCT': 3773, 'ADV': 2346, 'PRON': 1599, 'AUX': 1331, 'CCONJ': 790, 'SCONJ': 1038, 'NUM': 1045, 'ADJ': 2936, 'SYM': 74, 'PROPN': 1559, 'INTJ': 231, 'PART': 288, 'SPACE': 116, 'X': 48}\n"
     ]
    }
   ],
   "source": [
    "tag_dict = {}\n",
    "tag_counts = {}\n",
    "for doc in d['text'] :\n",
    "    for w in doc :\n",
    "        if w.pos_ not in tag_dict :\n",
    "            tag_dict[w.pos_] = w.pos\n",
    "            tag_counts[w.pos_] = 0\n",
    "        tag_counts[w.pos_] = tag_counts[w.pos_] + 1\n",
    "\n",
    "d['posCounts'] = [doc.count_by(POS) for doc in d['text']]\n",
    "for posStr in [\"NOUN\", \"PROPN\", \"DET\", \"PRON\", \"VERB\", \"ADJ\", \"CCONJ\", \"ADP\", 'ADV', 'AUX', 'SCONJ', 'NUM'] :\n",
    "    key_id = tag_dict[posStr]\n",
    "    d[posStr + 'count'] = [counts[key_id] if key_id in counts else 0 for counts in d['posCounts']]\n",
    "\n",
    "# the spacy one likes to call article-less initial nouns proper nouns, so we combine them\n",
    "d['NOUNcount'] = d['NOUNcount'] + d['PROPNcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['unigramCounts' 'bigramCounts' 'trigramCounts'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-dab2e4bd80b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m (d.drop([\"tokens\", 'posCounts', 'text', 'lemmas', 'noun_chunks', 'numRawWords', 'PROPNcount', 'unigramCounts', \n\u001b[0;32m----> 2\u001b[0;31m         'bigramCounts', 'trigramCounts'], 1)\n\u001b[0m\u001b[1;32m      3\u001b[0m  .to_csv(\"../outputs/posTagged_{}.csv\".format(version_to_use), index = False))\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4115\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4117\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4118\u001b[0m         )\n\u001b[1;32m   4119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3912\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3913\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3914\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3945\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3946\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5340\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found in axis\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['unigramCounts' 'bigramCounts' 'trigramCounts'] not found in axis\""
     ]
    }
   ],
   "source": [
    "(d.drop([\"tokens\", 'posCounts', 'text', 'lemmas', 'noun_chunks', 'numRawWords', 'PROPNcount', 'unigramCounts', \n",
    "        'bigramCounts', 'trigramCounts'], 1)\n",
    " .to_csv(\"../outputs/posTagged_{}.csv\".format(version_to_use), index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.2 syntactic analyses\n",
    "\n",
    "look at which words are dropped on each round and whether they are more closely related to one another than expected under null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dep_graph(text) :\n",
    "    # Load spacy's dependency tree into a networkx graph\n",
    "    edges = []\n",
    "    for token in text:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                          '{0}-{1}'.format(child.lower_,child.i)))\n",
    "\n",
    "    return nx.Graph(edges)\n",
    "\n",
    "def get_shortest_dependency_path (graph, word1, word2) :\n",
    "    # https://networkx.github.io/documentation/networkx-1.10/reference/algorithms.shortest_paths.html\n",
    "    return nx.shortest_path_length(graph, source=word1, target=word2)\n",
    "\n",
    "def flatten(list) :\n",
    "    return [x for y in list for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_function = ['CCONJ', 'SCONJ', \"PRON\", \"DET\", \"ADP\", \"AUX\", \"PART\"]\n",
    "def get_mean_dependency_lengths(null = None) :\n",
    "    \"\"\"\n",
    "    null can be: \n",
    "        'random' to make baseline where random words are dropped, or \n",
    "        'functionOnly' where function words are dropped\n",
    "    \"\"\"\n",
    "    assert null in [None, 'random', 'functionOnly']\n",
    "    dependency_lengths = {1: [], 2 : [], 3: [], 4: [], 5: []}\n",
    "    for name, df in d.groupby(['gameid', 'intendedName']) :\n",
    "        df['next_lemmas'] = df['lemmas'].shift(-1)\n",
    "        for _, row in df.iloc[0:-1].iterrows() :\n",
    "            trial_dependency_lengths = []\n",
    "            # handle case where we've auto-parsed single message into multiple 'sentences'\n",
    "            for i, sent in enumerate(row['text'].sents):\n",
    "                graph = make_dep_graph(sent)\n",
    "                dropped_words = ['{}-{}'.format(token.lower_,token.i) for token in sent\n",
    "                                 if token.lemma_ not in row['next_lemmas'] and not token.is_punct]\n",
    "                # for null model, we randomly sample words instead of using the real dropped ones\n",
    "                if null == 'random' : \n",
    "                    num_words_dropped = len(dropped_words)\n",
    "                    random_words = np.random.choice([token.lemma_ for token in sent], \n",
    "                                                    num_words_dropped, replace=False)\n",
    "                    dropped_words = ['{}-{}'.format(token.lower_,token.i) for token in sent\n",
    "                                     if token.lemma_ in random_words]\n",
    "                elif null == 'functionOnly' : \n",
    "                    num_words_dropped = len(dropped_words)\n",
    "                    function_words = [token.lemma_ for token in sent if token.pos_ in pos_function]\n",
    "                    non_function_words = [token.lemma_ for token in sent if token.pos_ not in pos_function]\n",
    "                    random_function_words = np.random.choice(function_words, min(num_words_dropped, len(function_words)), replace=False)\n",
    "                    if len(random_function_words) < num_words_dropped : \n",
    "                        random_function_words = np.append(random_function_words,\n",
    "                                  np.random.choice(non_function_words, num_words_dropped - len(random_function_words), replace=False))\n",
    "                    assert(len(random_function_words) == num_words_dropped)\n",
    "                    dropped_words = ['{}-{}'.format(token.lower_,token.i) for token in sent\n",
    "                                     if token.lemma_ in random_function_words]\n",
    "                for word1, word2 in combinations(dropped_words, 2) :\n",
    "                    try:\n",
    "                        dep_length = get_shortest_dependency_path(graph, word1,word2) \n",
    "                        trial_dependency_lengths.append(dep_length)\n",
    "                    except :\n",
    "                        nx.draw(graph)\n",
    "                        plt.show()\n",
    "            if len(trial_dependency_lengths) > 0 :\n",
    "                dependency_lengths[row.repetitionNum].append(np.array(trial_dependency_lengths).mean())\n",
    "    dependency_length_dict = {k: np.array(v).mean() for k,v in dependency_lengths.items()}\n",
    "    dependency_length_dict['avg'] = np.concatenate([np.array(v) for v in dependency_lengths.values()]).mean()\n",
    "    return dependency_length_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rxdh/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3.2741593728016953, 2: 2.740748592851781, 3: 2.4596191052032403, 4: 2.2019557759496617, 5: 2.036402329105833, 'avg': 2.76721910072433}\n"
     ]
    }
   ],
   "source": [
    "true = get_mean_dependency_lengths(null = False)\n",
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /1000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rxdh/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 /1000\r"
     ]
    }
   ],
   "source": [
    "random_null = []\n",
    "for i in range(1000) :\n",
    "    print(i, '/1000', end=\"\\r\")\n",
    "    random_null.append(get_mean_dependency_lengths(null = 'random'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /1000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rxdh/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/rxdh/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 /1000\r"
     ]
    }
   ],
   "source": [
    "function_null = []\n",
    "for i in range(1000) :\n",
    "    print(i, '/1000', end=\"\\r\")\n",
    "    function_null.append(get_mean_dependency_lengths(null = 'functionOnly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0' '1' '3.2741593728016953' 'true']\n",
      " ['0' '2' '2.740748592851781' 'true']\n",
      " ['0' '3' '2.4596191052032403' 'true']\n",
      " ...\n",
      " ['999' '4' '2.519571299660951' 'function']\n",
      " ['999' '5' '2.4984974605534434' 'function']\n",
      " ['999' 'avg' '3.063485309360986' 'function']]\n"
     ]
    }
   ],
   "source": [
    "true_rows = np.array([[0, k, v, 'true'] for k,v in true.items()])\n",
    "random_rows =  np.array([[i, k, v, 'random']  for i,rs in enumerate(random_null) for k,v in rs.items()])\n",
    "function_rows = np.array([[i, k, v, 'function']  for i,rs in enumerate(function_null) for k,v in rs.items()])\n",
    "rows = np.vstack([true_rows, random_rows, function_rows])\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(rows, columns = ['sampleNum', 'repetitionNum', 'value', 'baselineName'])\n",
    "   .to_csv('../outputs/permuted_dependency_distribution.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplemental and aborted analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from nltk import Tree\n",
    "archive = load_archive(\n",
    "            \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\"\n",
    "        )\n",
    "predictor = Predictor.from_archive(archive, 'constituency-parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edit distances on successive rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: it would be nice to use an algorithm like Selkow that uses operations for deleting entire subtrees at once (rather than attaching children to parent, which means that deleting a whole subtree requires as many operations as there are *nodes* in that subtree)\n",
    "\n",
    "see http://www.aclweb.org/anthology/R13-1002 for a way of altering ZS output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distance (label1, label2) :\n",
    "    if label1 == label2 :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "\n",
    "def get_root(doc) :\n",
    "    asdf = dict(('label' if key == 'nodeType' else key, value) for (key, value) in doc.items())\n",
    "    return asdf\n",
    "\n",
    "def get_children(subtree) :\n",
    "    if 'children' in subtree.keys() :\n",
    "        return [dict(('label' if key == 'nodeType' else key, value) for (key, value) in d.items()) for d in subtree['children']]\n",
    "    else :\n",
    "        return []\n",
    "\n",
    "def get_label(node) :\n",
    "    return node['label']\n",
    "\n",
    "def edit_distance(tree1, tree2, return_operations = False) :\n",
    "    return simple_distance(get_root(tree1), get_root(tree2), \n",
    "                           get_children, get_label, label_distance, return_operations=return_operations)\n",
    "\n",
    "def example() :\n",
    "    s1 = predictor.predict_json({\"sentence\": \"I am a cat with a big bone\"})\n",
    "    s2 = predictor.predict_json({\"sentence\": \"I am a cat\"})\n",
    "    ed = edit_distance(s1['hierplane_tree']['root'], s2['hierplane_tree']['root'], return_operations=True)\n",
    "    print('tree1:', Tree.fromstring(s1['trees']))\n",
    "    print('tree2:', Tree.fromstring(s2['trees']))\n",
    "    print('operations:', ed[1])\n",
    "    print('cost:', ed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we could use an algorithm that would give a cost of 1 for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree.fromstring(predictor.predict_json({\"sentence\" : \"a guy who looks like one of those wavy tube guys leaning towards the left\"})['trees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "tiny_d = d[['gameid', 'repetitionNum', 'contents', 'intendedName']]\n",
    "parses = []\n",
    "for i, s in enumerate(tiny_d['contents']) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(tiny_d['contents']))\n",
    "    parses.append(predictor.predict_json({'sentence' : s})['hierplane_tree']['root'])\n",
    "tiny_d['tree_parse'] = parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finditem(obj, value, initLevel = True):\n",
    "    if obj['nodeType'] == value and not initLevel:\n",
    "        return True\n",
    "    elif 'children' in obj :\n",
    "        for child in obj['children'] :\n",
    "            item = finditem(child, value, initLevel = False)\n",
    "            if item is not None:\n",
    "                return item\n",
    "tiny_d['SBAR'] = [finditem(s, 'SBAR') for s in tiny_d['tree_parse']]\n",
    "tiny_d['PP'] = [finditem(s, 'PP') for s in tiny_d['tree_parse']]\n",
    "tiny_d['CC'] = [finditem(s, 'CC') for s in tiny_d['tree_parse']]\n",
    "tiny_d['NP'] = [finditem(s, 'NP') for s in tiny_d['tree_parse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_d.to_json('./outputs/constituency_parses.json')\n",
    "tiny_d.drop('tree_parse', 1).to_csv('./outputs/constituency_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_d = pd.read_json('./outputs/constituency_parses.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gameids = np.unique(tiny_d['gameid'])\n",
    "tangramids = np.unique(tiny_d['intendedName'])\n",
    "\n",
    "transitions = np.zeros([len(gameids), len(tangramids) ,5])\n",
    "for i, gameid in enumerate(gameids) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(gameids))\n",
    "    game_d = tiny_d.query('gameid == \"{0}\"'.format(gameid))\n",
    "    for j, intendedName in enumerate(tangramids) :\n",
    "        for k, init_occurrenceNum in enumerate(range(1,6)) :\n",
    "            dist = []\n",
    "            sub1 = game_d.query('intendedName == \"{0}\" and repetitionNum == {1}'.format(intendedName, init_occurrenceNum))['tree_parse']\n",
    "            sub2 = game_d.query('intendedName == \"{0}\" and repetitionNum == {1}'.format(\n",
    "                intendedName, init_occurrenceNum+1\n",
    "            ))['tree_parse']\n",
    "            for tree1 in sub1 :\n",
    "                for tree2 in sub2 :\n",
    "                    dist.append(edit_distance(tree1, tree2))\n",
    "            transitions[i, j, k] = np.max(dist) if dist else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: normalized by tree size\n",
    "TODO: maybe can show this more straightforward by doing permutation test thing on POS tags instead of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(transitions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Across-game version\n",
    "Instead of looking at edit distance from round $i$ to $i + 1$ for pair $j$, we look at average edit distances between pairs $j$ to $j+1$ on round $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.sample([1,2], len([1,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acrossgame = np.zeros([len(tangramids),6, len(gameids)])\n",
    "shuffled_gameids = random.sample(list(gameids), len(gameids))\n",
    "for i, intendedName in enumerate(tangramids) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(tangramids))\n",
    "    for repetitionNum in range(1,7) :\n",
    "        mini_d = tiny_d.query('intendedName == \"{0}\" and repetitionNum == {1}'.format(intendedName, repetitionNum))      \n",
    "        for k in range(len(gameids) -1) :\n",
    "            dist = []\n",
    "#            print(gameids[k], 'to', gameids[k+1])\n",
    "            sub1 = mini_d.query('gameid == \"{0}\"'.format(gameids[k]))['tree_parse']\n",
    "            sub2 = mini_d.query('gameid == \"{0}\"'.format(gameids[k+1]))['tree_parse']\n",
    "            for tree1 in sub1 :\n",
    "                for tree2 in sub2 :\n",
    "                    dist.append(edit_distance(tree1, tree2))\n",
    "#             print(sub1)\n",
    "#             print(sub2)\n",
    "            acrossgame[i, repetitionNum-1, k] = np.max(dist) if dist else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(acrossgame, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate indicator words for tangrams/rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, get list of words in first round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to first round\n",
    "d_round1 = d[d['repetitionNum'] == 1]\n",
    "\n",
    "# Pull out all tokens and collapse into count dict\n",
    "tokenDict = Counter([item for sublist in d_round1['tokens'].tolist()\n",
    "                     for item in sublist])\n",
    "# Pull out all words that occur more than once\n",
    "wordList = [word for (word,count) in tokenDict.items() if count > 1 and not word.isdigit()]\n",
    "print(wordList[0:10])\n",
    "print(len(wordList))\n",
    "\n",
    "# Get POS map; will be longer because it doesn't require count > 1, but it doesn't matter\n",
    "POSdict = {word.text: word.pos_ for text in d_round1['text'] for word in text}\n",
    "print(len(POSdict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to select words & counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCounts(df, gameid, occurrenceNum, tangram = None) :\n",
    "    roundCond = 'repetitionNum == ' + occurrenceNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'intendedName == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([item for sublist in relevantRow['tokens'].tolist() \n",
    "                    for item in sublist])\n",
    "\n",
    "#creates mini dataframe that grabs the words used in round n for a given tangram and gameid\n",
    "def selectTangramRoundWords(df, tangram, roundNum, gameid):\n",
    "    wordCounts = getWordCounts(df, gameid, roundNum, tangram)\n",
    "    return list(wordCounts.keys())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
