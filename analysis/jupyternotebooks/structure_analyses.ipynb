{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import lots of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial.distance as distance\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "import textacy\n",
    "import textacy.io\n",
    "from utils.nlp_utils import lemmatize_doc\n",
    "from sklearn import manifold\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from spacy.lang.en import English\n",
    "from spacy.attrs import POS\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_to_use = 'tangramsSequential_collapsed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import annotated file & tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_raw = pd.read_csv('../../data/{}.csv'.format(version_to_use))#.rename(index=str, columns={\"contents\": \"text\"})\n",
    "d_raw['text'] = [nlp(text) for text in d_raw['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in (d_raw.iloc[-1].text.sents) :\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run spellchecker (using conservative vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import nlp_utils as utils\n",
    "conservative_vectors = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(conservative_vectors.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.building_spell_correction_dictionary(\n",
    "    d_raw.query('taskVersion == \"cued\"'), \n",
    "    conservative_vectors.vocab, \n",
    "    []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all game ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameidList = pd.unique(d.gameid.ravel()).tolist()\n",
    "print(gameidList[0:5])\n",
    "print(len(gameidList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all tangram names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangramList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
    "print(tangramList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1: What are most common words & phrases to reduce? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, spacy has no n-gram function, so we use textacy, a convenience wrapper around spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['doc'] = [textacy.make_spacy_doc(row, lang='en_core_web_lg') for row in d['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['tokens'] = [[token.text for token in l if not token.is_punct] for l in d['text']]\n",
    "d['lemmas'] = [lemmatize_doc(text) for text in d['text']]\n",
    "d['pos'] = [[token.pos_ for token in l if not token.is_punct] for l in d['text']]\n",
    "d['noun_chunks'] = [list(l.noun_chunks) for l in d['text']]\n",
    "d['numWords'] = [len([token for token in l if not token.is_punct]) for l in d['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['unigrams'] = [list(textacy.extract.ngrams(nlp(' '.join(doc)), 1, filter_stops = False)) for doc in d['lemmas']]\n",
    "\n",
    "d['bigrams'] = [list(textacy.extract.ngrams(nlp(' '.join(doc)), 2, filter_stops = False)) for doc in d['lemmas']]\n",
    "\n",
    "d['trigrams'] = [list(textacy.extract.ngrams(nlp(' '.join(doc)), 3, filter_stops = False)) for doc in d['lemmas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(countType, df, gameid, repetitionNum, tangram = None) :\n",
    "    roundCond = 'repetitionNum == ' + repetitionNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'tangramRef == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([str(item) for sublist in relevantRow[countType]\n",
    "                    for item in sublist])\n",
    "\n",
    "for countType in ['unigrams', 'bigrams', 'trigrams'] :\n",
    "    flattenedContents = [str(item) for sublist in d[countType]\n",
    "                         for item in sublist]\n",
    "    countDict = Counter(flattenedContents)\n",
    "    wordList = [v for (v,count) in countDict.items() if count > 20]\n",
    "    with open('../outputs/' + countType + 'Counts.csv', 'w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['gameid', 'repetitionNum', 'word', 'count'])\n",
    "        for gameid in gameidList:  \n",
    "            for repetitionNum in ['1', '2', '3', '4', '5', '6'] :\n",
    "                counts = getCounts(countType, d, gameid, repetitionNum)\n",
    "                for word in wordList :\n",
    "                    writer.writerow([gameid, repetitionNum, word, counts[word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1 Extract parts of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get counts for each POS label and export for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {}\n",
    "tag_counts = {}\n",
    "for doc in d['text'] :\n",
    "    for w in doc :\n",
    "        if w.pos_ not in tag_dict :\n",
    "            tag_dict[w.pos_] = w.pos\n",
    "            tag_counts[w.pos_] = 0\n",
    "        tag_counts[w.pos_] = tag_counts[w.pos_] + 1\n",
    "\n",
    "d['posCounts'] = [doc.count_by(POS) for doc in d['text']]\n",
    "for posStr in [\"NOUN\", \"PROPN\", \"DET\", \"PRON\", \"VERB\", \"ADJ\", \"CCONJ\", \"ADP\", 'ADV', 'AUX', 'SCONJ', 'NUM'] :\n",
    "    key_id = tag_dict[posStr]\n",
    "    d[posStr + 'count'] = [counts[key_id] if key_id in counts else 0 for counts in d['posCounts']]\n",
    "\n",
    "# the spacy one likes to call article-less initial nouns proper nouns, so we combine them\n",
    "d['NOUNcount'] = d['NOUNcount'] + d['PROPNcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d.drop([\"tokens\", 'posCounts', 'text', 'lemmas', 'noun_chunks', 'numRawWords', 'PROPNcount', 'unigramCounts', \n",
    "        'bigramCounts', 'trigramCounts'], 1)\n",
    " .to_csv(\"../outputs/posTagged_{}.csv\".format(version_to_use), index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.2 syntactic analyses\n",
    "\n",
    "look at which words are dropped on each round and whether they are more closely related to one another than expected under null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dep_graph(text) :\n",
    "    # Load spacy's dependency tree into a networkx graph\n",
    "    edges = []\n",
    "    for token in text:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                          '{0}-{1}'.format(child.lower_,child.i)))\n",
    "\n",
    "    return nx.Graph(edges)\n",
    "\n",
    "def get_shortest_dependency_path (graph, word1, word2) :\n",
    "    # https://networkx.github.io/documentation/networkx-1.10/reference/algorithms.shortest_paths.html\n",
    "    return nx.shortest_path_length(graph, source=word1, target=word2)\n",
    "\n",
    "def flatten(list) :\n",
    "    return [x for y in list for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_function = ['CCONJ', 'SCONJ', \"PRON\", \"DET\", \"ADP\", \"AUX\", \"PART\"]\n",
    "def get_mean_dependency_lengths(null = None) :\n",
    "    \"\"\"\n",
    "    null can be: \n",
    "        'random' to make baseline where random words are dropped, or \n",
    "        'functionOnly' where function words are dropped\n",
    "    \"\"\"\n",
    "    assert null in [None, 'random', 'functionOnly']\n",
    "    dependency_lengths = {1: [], 2 : [], 3: [], 4: [], 5: []}\n",
    "    for name, df in d.groupby(['gameid', 'intendedName']) :\n",
    "        df['next_lemmas'] = df['lemmas'].shift(-1)\n",
    "        for _, row in df.iloc[0:-1].iterrows() :\n",
    "            trial_dependency_lengths = []\n",
    "            # handle case where we've auto-parsed single message into multiple 'sentences'\n",
    "            for i, sent in enumerate(row['text'].sents):\n",
    "                graph = make_dep_graph(sent)\n",
    "                dropped_words = ['{}-{}'.format(token.lower_,token.i) for token in sent\n",
    "                                 if token.lemma_ not in row['next_lemmas'] and not token.is_punct]\n",
    "                # for null model, we randomly sample words instead of using the real dropped ones\n",
    "                if null == 'random' : \n",
    "                    num_words_dropped = len(dropped_words)\n",
    "                    random_words = np.random.choice([token.lemma_ for token in sent], \n",
    "                                                    num_words_dropped, replace=False)\n",
    "                    dropped_words = ['{}-{}'.format(token.lower_,token.i) for token in sent\n",
    "                                     if token.lemma_ in random_words]\n",
    "                elif null == 'functionOnly' : \n",
    "                    num_words_dropped = len(dropped_words)\n",
    "                    function_words = [token.lemma_ for token in sent if token.pos_ in pos_function]\n",
    "                    non_function_words = [token.lemma_ for token in sent if token.pos_ not in pos_function]\n",
    "                    random_function_words = np.random.choice(function_words, min(num_words_dropped, len(function_words)), replace=False)\n",
    "                    if len(random_function_words) < num_words_dropped : \n",
    "                        random_function_words = np.append(random_function_words,\n",
    "                                  np.random.choice(non_function_words, num_words_dropped - len(random_function_words), replace=False))\n",
    "                    assert(len(random_function_words) == num_words_dropped)\n",
    "                    dropped_words = ['{}-{}'.format(token.lower_,token.i) for token in sent\n",
    "                                     if token.lemma_ in random_function_words]\n",
    "                for word1, word2 in combinations(dropped_words, 2) :\n",
    "                    try:\n",
    "                        dep_length = get_shortest_dependency_path(graph, word1,word2) \n",
    "                        trial_dependency_lengths.append(dep_length)\n",
    "                    except :\n",
    "                        nx.draw(graph)\n",
    "                        plt.show()\n",
    "            if len(trial_dependency_lengths) > 0 :\n",
    "                dependency_lengths[row.repetitionNum].append(np.array(trial_dependency_lengths).mean())\n",
    "    dependency_length_dict = {k: np.array(v).mean() for k,v in dependency_lengths.items()}\n",
    "    dependency_length_dict['avg'] = np.concatenate([np.array(v) for v in dependency_lengths.values()]).mean()\n",
    "    return dependency_length_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = get_mean_dependency_lengths(null = False)\n",
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_null = []\n",
    "for i in range(1000) :\n",
    "    print(i, '/1000', end=\"\\r\")\n",
    "    random_null.append(get_mean_dependency_lengths(null = 'random'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_null = []\n",
    "for i in range(1000) :\n",
    "    print(i, '/1000', end=\"\\r\")\n",
    "    function_null.append(get_mean_dependency_lengths(null = 'functionOnly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_rows = np.array([[0, k, v, 'true'] for k,v in true.items()])\n",
    "random_rows =  np.array([[i, k, v, 'random']  for i,rs in enumerate(random_null) for k,v in rs.items()])\n",
    "function_rows = np.array([[i, k, v, 'function']  for i,rs in enumerate(function_null) for k,v in rs.items()])\n",
    "rows = np.vstack([true_rows, random_rows, function_rows])\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(rows, columns = ['sampleNum', 'repetitionNum', 'value', 'baselineName'])\n",
    "   .to_csv('../outputs/permuted_dependency_distribution.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplemental and aborted analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from nltk import Tree\n",
    "archive = load_archive(\n",
    "            \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\"\n",
    "        )\n",
    "predictor = Predictor.from_archive(archive, 'constituency-parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edit distances on successive rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: it would be nice to use an algorithm like Selkow that uses operations for deleting entire subtrees at once (rather than attaching children to parent, which means that deleting a whole subtree requires as many operations as there are *nodes* in that subtree)\n",
    "\n",
    "see http://www.aclweb.org/anthology/R13-1002 for a way of altering ZS output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distance (label1, label2) :\n",
    "    if label1 == label2 :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "\n",
    "def get_root(doc) :\n",
    "    asdf = dict(('label' if key == 'nodeType' else key, value) for (key, value) in doc.items())\n",
    "    return asdf\n",
    "\n",
    "def get_children(subtree) :\n",
    "    if 'children' in subtree.keys() :\n",
    "        return [dict(('label' if key == 'nodeType' else key, value) for (key, value) in d.items()) for d in subtree['children']]\n",
    "    else :\n",
    "        return []\n",
    "\n",
    "def get_label(node) :\n",
    "    return node['label']\n",
    "\n",
    "def edit_distance(tree1, tree2, return_operations = False) :\n",
    "    return simple_distance(get_root(tree1), get_root(tree2), \n",
    "                           get_children, get_label, label_distance, return_operations=return_operations)\n",
    "\n",
    "def example() :\n",
    "    s1 = predictor.predict_json({\"sentence\": \"I am a cat with a big bone\"})\n",
    "    s2 = predictor.predict_json({\"sentence\": \"I am a cat\"})\n",
    "    ed = edit_distance(s1['hierplane_tree']['root'], s2['hierplane_tree']['root'], return_operations=True)\n",
    "    print('tree1:', Tree.fromstring(s1['trees']))\n",
    "    print('tree2:', Tree.fromstring(s2['trees']))\n",
    "    print('operations:', ed[1])\n",
    "    print('cost:', ed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we could use an algorithm that would give a cost of 1 for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree.fromstring(predictor.predict_json({\"sentence\" : \"a guy who looks like one of those wavy tube guys leaning towards the left\"})['trees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "tiny_d = d[['gameid', 'repetitionNum', 'contents', 'intendedName']]\n",
    "parses = []\n",
    "for i, s in enumerate(tiny_d['contents']) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(tiny_d['contents']))\n",
    "    parses.append(predictor.predict_json({'sentence' : s})['hierplane_tree']['root'])\n",
    "tiny_d['tree_parse'] = parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finditem(obj, value, initLevel = True):\n",
    "    if obj['nodeType'] == value and not initLevel:\n",
    "        return True\n",
    "    elif 'children' in obj :\n",
    "        for child in obj['children'] :\n",
    "            item = finditem(child, value, initLevel = False)\n",
    "            if item is not None:\n",
    "                return item\n",
    "tiny_d['SBAR'] = [finditem(s, 'SBAR') for s in tiny_d['tree_parse']]\n",
    "tiny_d['PP'] = [finditem(s, 'PP') for s in tiny_d['tree_parse']]\n",
    "tiny_d['CC'] = [finditem(s, 'CC') for s in tiny_d['tree_parse']]\n",
    "tiny_d['NP'] = [finditem(s, 'NP') for s in tiny_d['tree_parse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_d.to_json('./outputs/constituency_parses.json')\n",
    "tiny_d.drop('tree_parse', 1).to_csv('./outputs/constituency_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_d = pd.read_json('./outputs/constituency_parses.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gameids = np.unique(tiny_d['gameid'])\n",
    "tangramids = np.unique(tiny_d['intendedName'])\n",
    "\n",
    "transitions = np.zeros([len(gameids), len(tangramids) ,5])\n",
    "for i, gameid in enumerate(gameids) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(gameids))\n",
    "    game_d = tiny_d.query('gameid == \"{0}\"'.format(gameid))\n",
    "    for j, intendedName in enumerate(tangramids) :\n",
    "        for k, init_occurrenceNum in enumerate(range(1,6)) :\n",
    "            dist = []\n",
    "            sub1 = game_d.query('intendedName == \"{0}\" and repetitionNum == {1}'.format(intendedName, init_occurrenceNum))['tree_parse']\n",
    "            sub2 = game_d.query('intendedName == \"{0}\" and repetitionNum == {1}'.format(\n",
    "                intendedName, init_occurrenceNum+1\n",
    "            ))['tree_parse']\n",
    "            for tree1 in sub1 :\n",
    "                for tree2 in sub2 :\n",
    "                    dist.append(edit_distance(tree1, tree2))\n",
    "            transitions[i, j, k] = np.max(dist) if dist else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: normalized by tree size\n",
    "TODO: maybe can show this more straightforward by doing permutation test thing on POS tags instead of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(transitions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Across-game version\n",
    "Instead of looking at edit distance from round $i$ to $i + 1$ for pair $j$, we look at average edit distances between pairs $j$ to $j+1$ on round $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.sample([1,2], len([1,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acrossgame = np.zeros([len(tangramids),6, len(gameids)])\n",
    "shuffled_gameids = random.sample(list(gameids), len(gameids))\n",
    "for i, intendedName in enumerate(tangramids) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(tangramids))\n",
    "    for repetitionNum in range(1,7) :\n",
    "        mini_d = tiny_d.query('intendedName == \"{0}\" and repetitionNum == {1}'.format(intendedName, repetitionNum))      \n",
    "        for k in range(len(gameids) -1) :\n",
    "            dist = []\n",
    "#            print(gameids[k], 'to', gameids[k+1])\n",
    "            sub1 = mini_d.query('gameid == \"{0}\"'.format(gameids[k]))['tree_parse']\n",
    "            sub2 = mini_d.query('gameid == \"{0}\"'.format(gameids[k+1]))['tree_parse']\n",
    "            for tree1 in sub1 :\n",
    "                for tree2 in sub2 :\n",
    "                    dist.append(edit_distance(tree1, tree2))\n",
    "#             print(sub1)\n",
    "#             print(sub2)\n",
    "            acrossgame[i, repetitionNum-1, k] = np.max(dist) if dist else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(acrossgame, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate indicator words for tangrams/rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, get list of words in first round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to first round\n",
    "d_round1 = d[d['repetitionNum'] == 1]\n",
    "\n",
    "# Pull out all tokens and collapse into count dict\n",
    "tokenDict = Counter([item for sublist in d_round1['tokens'].tolist()\n",
    "                     for item in sublist])\n",
    "# Pull out all words that occur more than once\n",
    "wordList = [word for (word,count) in tokenDict.items() if count > 1 and not word.isdigit()]\n",
    "print(wordList[0:10])\n",
    "print(len(wordList))\n",
    "\n",
    "# Get POS map; will be longer because it doesn't require count > 1, but it doesn't matter\n",
    "POSdict = {word.text: word.pos_ for text in d_round1['text'] for word in text}\n",
    "print(len(POSdict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to select words & counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCounts(df, gameid, occurrenceNum, tangram = None) :\n",
    "    roundCond = 'repetitionNum == ' + occurrenceNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'intendedName == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([item for sublist in relevantRow['tokens'].tolist() \n",
    "                    for item in sublist])\n",
    "\n",
    "#creates mini dataframe that grabs the words used in round n for a given tangram and gameid\n",
    "def selectTangramRoundWords(df, tangram, roundNum, gameid):\n",
    "    wordCounts = getWordCounts(df, gameid, roundNum, tangram)\n",
    "    return list(wordCounts.keys())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
