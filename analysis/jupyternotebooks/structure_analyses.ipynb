{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import lots of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.spatial.distance as distance\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "# import textacy\n",
    "# import textacy.io\n",
    "from utils.nlp_utils import lemmatize_doc\n",
    "from sklearn import manifold\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from spacy.lang.en import English\n",
    "from spacy.attrs import POS\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_to_use = 'tangramsSequential_collapsed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import annotated file & tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_raw = pd.read_csv('../../data/{}.csv'.format(version_to_use))#.rename(index=str, columns={\"contents\": \"text\"})\n",
    "d_raw['text'] = [nlp(text) for text in d_raw['contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run spellchecker (using conservative vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import nlp_utils as utils\n",
    "conservative_vectors = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(conservative_vectors.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.building_spell_correction_dictionary(\n",
    "    d_raw.query('taskVersion == \"cued\"'), \n",
    "    conservative_vectors.vocab, \n",
    "    []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all game ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gameid', 'trialNum', 'repetitionNum', 'intendedName', 'contents',\n",
       "       'numRawWords', 'correct', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0057-414228f8-c268-40d6-9349-b35df4f080d9', '0349-951c1418-40e9-48b3-8290-7ed4461f4d54', '0413-e4a76b36-4367-4e30-abf9-93e823913630', '0461-f522f8f4-37dc-4bb0-89bf-9f6bcf43274a', '0711-b03679d3-9904-4263-bd2f-8ec8e7a45af7']\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "gameidList = pd.unique(d.gameid.ravel()).tolist()\n",
    "print(gameidList[0:5])\n",
    "print(len(gameidList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all tangram names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n"
     ]
    }
   ],
   "source": [
    "tangramList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
    "print(tangramList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are most common words & phrases to reduce? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, spacy has no n-gram function, so we use textacy, a convenience wrapper around spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['doc'] = [textacy.Doc(textacy.preprocess_text(row, lowercase = True), lang='en_core_web_md') for row in d['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['unigrams'] = [list(doc.to_terms_list(ngrams=1, as_strings=True, filter_stops = False, named_entities = False)) for doc in d['doc']]\n",
    "\n",
    "d['bigrams'] = [list(doc.to_terms_list(ngrams=2, as_strings=True, filter_stops = False, named_entities = False)) for doc in d['doc']]\n",
    "\n",
    "d['trigrams'] = [list(doc.to_terms_list(ngrams=3, as_strings=True, filter_stops = False, named_entities = False)) for doc in d['doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(countType, df, gameid, repetitionNum, tangram = None) :\n",
    "    roundCond = 'repetitionNum == ' + repetitionNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'tangramRef == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([item for sublist in relevantRow[countType]\n",
    "                    for item in sublist])\n",
    "\n",
    "for countType in ['unigrams', 'bigrams', 'trigrams'] :\n",
    "    countDict = Counter([item for sublist in d[countType]\n",
    "                         for item in sublist])\n",
    "    wordList = [v for (v,count) in countDict.items() if count > 20]\n",
    "\n",
    "    with open('outputs/' + countType + 'Counts.csv', 'w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['gameid', 'repetitionNum', 'word', 'count'])\n",
    "        for gameid in gameidList:  \n",
    "            for repetitionNum in ['1', '2', '3', '4', '5', '6'] :\n",
    "                counts = getCounts(countType, d, gameid, repetitionNum)\n",
    "                for word in wordList :\n",
    "                    writer.writerow([gameid, repetitionNum, word, counts[word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract parts of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get counts for each POS label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VERB': 100, 'ADP': 85, 'DET': 90, 'NOUN': 92, 'PUNCT': 97, 'ADV': 86, 'PRON': 95, 'AUX': 87, 'CCONJ': 89, 'SCONJ': 98, 'NUM': 93, 'ADJ': 84, 'SYM': 99, 'PROPN': 96, 'INTJ': 91, 'PART': 94, 'SPACE': 103, 'X': 101}\n"
     ]
    }
   ],
   "source": [
    "tag_dict = {}\n",
    "for doc in d['text'] :\n",
    "    for w in doc :\n",
    "        if w.pos not in tag_dict :\n",
    "                tag_dict[w.pos_] = w.pos\n",
    "d['posCounts'] = [doc.count_by(POS) for doc in d['text']]\n",
    "print(tag_dict)\n",
    "for posStr in [\"NOUN\", \"DET\", \"PRON\", \"VERB\", \"ADJ\", \"CCONJ\", \"ADP\", 'ADV'] :\n",
    "    key_id = tag_dict[posStr]\n",
    "    d[posStr + 'count'] = [counts[key_id] if key_id in counts else 0 for counts in d['posCounts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d['tokens'] = [[token.text for token in l if not token.is_punct] for l in d['text']]\n",
    "d['lemmas'] = [lemmatize_doc(text) for text in d['text']]\n",
    "d['pos'] = [[token.pos_ for token in l if not token.is_punct] for l in d['text']]\n",
    "d['noun_chunks'] = [list(l.noun_chunks) for l in d['text']]\n",
    "d['numWords'] = [len([token for token in l if not token.is_punct]) for l in d['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to csv for plotting in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d.drop([\"tokens\", 'posCounts', 'text', 'lemmas', 'noun_chunks', 'numRawWords'], 1)\n",
    " .to_csv(\"../outputs/posTagged_{}.csv\".format(version_to_use), index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# syntactic analyses\n",
    "\n",
    "look at which words are dropped on each round and whether they are more closely related to one another than expected under null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dep_graph(text) :\n",
    "    # Load spacy's dependency tree into a networkx graph\n",
    "    edges = []\n",
    "    for token in text:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                          '{0}-{1}'.format(child.lower_,child.i)))\n",
    "\n",
    "    return nx.Graph(edges)\n",
    "\n",
    "def get_shortest_dependency_path (graph, word1, word2) :\n",
    "    # https://networkx.github.io/documentation/networkx-1.10/reference/algorithms.shortest_paths.html\n",
    "    return nx.shortest_path_length(graph, source=word1, target=word2)\n",
    "\n",
    "def flatten(list) :\n",
    "    return [x for y in list for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rxdh/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.362526534753067\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n"
     ]
    }
   ],
   "source": [
    "def get_mean_dependency_lengths(null = False) :\n",
    "    dependency_lengths = []\n",
    "    for name, df in d.groupby(['gameid', 'intendedName']) :\n",
    "        df['next_lemmas'] = df['lemmas'].shift(-1)\n",
    "        for _, row in df.iloc[0:-1].iterrows() :\n",
    "            # handle case where we've auto-parsed single message into multiple 'sentences'\n",
    "            for i, sent in enumerate(row['text'].sents):\n",
    "                graph = make_dep_graph(sent)\n",
    "                dropped_words = ['{}-{}'.format(token.lower_,token.i) for token in sent\n",
    "                                 if token.lemma_ not in row['next_lemmas'] and not token.is_punct]\n",
    "                # for null model, we randomly sample words instead of using the real dropped ones\n",
    "                if null : \n",
    "                    num_words_dropped = len(dropped_words)\n",
    "                    random_words = np.random.choice([token.lemma_ for token in sent], \n",
    "                                                    num_words_dropped, replace=False)\n",
    "                    dropped_words = ['{}-{}'.format(token.lower_,token.i) for token in sent\n",
    "                                     if token.lemma_ in random_words]\n",
    "                for word1, word2 in combinations(dropped_words, 2) :\n",
    "                    try:\n",
    "                        dep_length = get_shortest_dependency_path(graph, word1,word2) \n",
    "                        dependency_lengths.append(dep_length)\n",
    "                    except :\n",
    "                        nx.draw(graph)\n",
    "                        plt.show()\n",
    "    return np.array(dependency_lengths).mean()\n",
    "true = get_mean_dependency_lengths(null = False)\n",
    "print(true)\n",
    "null = [get_mean_dependency_lengths(null = True) for i in range(100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.362526534753067\n"
     ]
    }
   ],
   "source": [
    "print(true)\n",
    "np.savetxt('../outputs/permuted_dependency_distribution.txt', sorted(null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplemental analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from nltk import Tree\n",
    "archive = load_archive(\n",
    "            \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\"\n",
    "        )\n",
    "predictor = Predictor.from_archive(archive, 'constituency-parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edit distances on successive rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: it would be nice to use an algorithm like Selkow that uses operations for deleting entire subtrees at once (rather than attaching children to parent, which means that deleting a whole subtree requires as many operations as there are *nodes* in that subtree)\n",
    "\n",
    "see http://www.aclweb.org/anthology/R13-1002 for a way of altering ZS output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distance (label1, label2) :\n",
    "    if label1 == label2 :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "\n",
    "def get_root(doc) :\n",
    "    asdf = dict(('label' if key == 'nodeType' else key, value) for (key, value) in doc.items())\n",
    "    return asdf\n",
    "\n",
    "def get_children(subtree) :\n",
    "    if 'children' in subtree.keys() :\n",
    "        return [dict(('label' if key == 'nodeType' else key, value) for (key, value) in d.items()) for d in subtree['children']]\n",
    "    else :\n",
    "        return []\n",
    "\n",
    "def get_label(node) :\n",
    "    return node['label']\n",
    "\n",
    "def edit_distance(tree1, tree2, return_operations = False) :\n",
    "    return simple_distance(get_root(tree1), get_root(tree2), \n",
    "                           get_children, get_label, label_distance, return_operations=return_operations)\n",
    "\n",
    "def example() :\n",
    "    s1 = predictor.predict_json({\"sentence\": \"I am a cat with a big bone\"})\n",
    "    s2 = predictor.predict_json({\"sentence\": \"I am a cat\"})\n",
    "    ed = edit_distance(s1['hierplane_tree']['root'], s2['hierplane_tree']['root'], return_operations=True)\n",
    "    print('tree1:', Tree.fromstring(s1['trees']))\n",
    "    print('tree2:', Tree.fromstring(s2['trees']))\n",
    "    print('operations:', ed[1])\n",
    "    print('cost:', ed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we could use an algorithm that would give a cost of 1 for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree.fromstring(predictor.predict_json({\"sentence\" : \"a guy who looks like one of those wavy tube guys leaning towards the left\"})['trees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "tiny_d = d[['gameid', 'repetitionNum', 'contents', 'intendedName']]\n",
    "parses = []\n",
    "for i, s in enumerate(tiny_d['contents']) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(tiny_d['contents']))\n",
    "    parses.append(predictor.predict_json({'sentence' : s})['hierplane_tree']['root'])\n",
    "tiny_d['tree_parse'] = parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finditem(obj, value, initLevel = True):\n",
    "    if obj['nodeType'] == value and not initLevel:\n",
    "        return True\n",
    "    elif 'children' in obj :\n",
    "        for child in obj['children'] :\n",
    "            item = finditem(child, value, initLevel = False)\n",
    "            if item is not None:\n",
    "                return item\n",
    "tiny_d['SBAR'] = [finditem(s, 'SBAR') for s in tiny_d['tree_parse']]\n",
    "tiny_d['PP'] = [finditem(s, 'PP') for s in tiny_d['tree_parse']]\n",
    "tiny_d['CC'] = [finditem(s, 'CC') for s in tiny_d['tree_parse']]\n",
    "tiny_d['NP'] = [finditem(s, 'NP') for s in tiny_d['tree_parse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_d.to_json('./outputs/constituency_parses.json')\n",
    "tiny_d.drop('tree_parse', 1).to_csv('./outputs/constituency_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_d = pd.read_json('./outputs/constituency_parses.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gameids = np.unique(tiny_d['gameid'])\n",
    "tangramids = np.unique(tiny_d['intendedName'])\n",
    "\n",
    "transitions = np.zeros([len(gameids), len(tangramids) ,5])\n",
    "for i, gameid in enumerate(gameids) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(gameids))\n",
    "    game_d = tiny_d.query('gameid == \"{0}\"'.format(gameid))\n",
    "    for j, intendedName in enumerate(tangramids) :\n",
    "        for k, init_occurrenceNum in enumerate(range(1,6)) :\n",
    "            dist = []\n",
    "            sub1 = game_d.query('intendedName == \"{0}\" and repetitionNum == {1}'.format(intendedName, init_occurrenceNum))['tree_parse']\n",
    "            sub2 = game_d.query('intendedName == \"{0}\" and repetitionNum == {1}'.format(\n",
    "                intendedName, init_occurrenceNum+1\n",
    "            ))['tree_parse']\n",
    "            for tree1 in sub1 :\n",
    "                for tree2 in sub2 :\n",
    "                    dist.append(edit_distance(tree1, tree2))\n",
    "            transitions[i, j, k] = np.max(dist) if dist else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: normalized by tree size\n",
    "TODO: maybe can show this more straightforward by doing permutation test thing on POS tags instead of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(transitions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Across-game version\n",
    "Instead of looking at edit distance from round $i$ to $i + 1$ for pair $j$, we look at average edit distances between pairs $j$ to $j+1$ on round $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.sample([1,2], len([1,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acrossgame = np.zeros([len(tangramids),6, len(gameids)])\n",
    "shuffled_gameids = random.sample(list(gameids), len(gameids))\n",
    "for i, intendedName in enumerate(tangramids) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(tangramids))\n",
    "    for repetitionNum in range(1,7) :\n",
    "        mini_d = tiny_d.query('intendedName == \"{0}\" and repetitionNum == {1}'.format(intendedName, repetitionNum))      \n",
    "        for k in range(len(gameids) -1) :\n",
    "            dist = []\n",
    "#            print(gameids[k], 'to', gameids[k+1])\n",
    "            sub1 = mini_d.query('gameid == \"{0}\"'.format(gameids[k]))['tree_parse']\n",
    "            sub2 = mini_d.query('gameid == \"{0}\"'.format(gameids[k+1]))['tree_parse']\n",
    "            for tree1 in sub1 :\n",
    "                for tree2 in sub2 :\n",
    "                    dist.append(edit_distance(tree1, tree2))\n",
    "#             print(sub1)\n",
    "#             print(sub2)\n",
    "            acrossgame[i, repetitionNum-1, k] = np.max(dist) if dist else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(acrossgame, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate indicator words for tangrams/rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, get list of words in first round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to first round\n",
    "d_round1 = d[d['repetitionNum'] == 1]\n",
    "\n",
    "# Pull out all tokens and collapse into count dict\n",
    "tokenDict = Counter([item for sublist in d_round1['tokens'].tolist()\n",
    "                     for item in sublist])\n",
    "# Pull out all words that occur more than once\n",
    "wordList = [word for (word,count) in tokenDict.items() if count > 1 and not word.isdigit()]\n",
    "print(wordList[0:10])\n",
    "print(len(wordList))\n",
    "\n",
    "# Get POS map; will be longer because it doesn't require count > 1, but it doesn't matter\n",
    "POSdict = {word.text: word.pos_ for text in d_round1['text'] for word in text}\n",
    "print(len(POSdict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to select words & counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCounts(df, gameid, occurrenceNum, tangram = None) :\n",
    "    roundCond = 'repetitionNum == ' + occurrenceNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'intendedName == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([item for sublist in relevantRow['tokens'].tolist() \n",
    "                    for item in sublist])\n",
    "\n",
    "#creates mini dataframe that grabs the words used in round n for a given tangram and gameid\n",
    "def selectTangramRoundWords(df, tangram, roundNum, gameid):\n",
    "    wordCounts = getWordCounts(df, gameid, roundNum, tangram)\n",
    "    return list(wordCounts.keys())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
