{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Annotate messages with tangram\n",
    "\n",
    "## Pull in raw data\n",
    "\n",
    "d_msgs_raw = (pd.read_csv('../../data/tangrams_unconstrained/message/tangramsMessages.csv', escapechar='\\\\')\n",
    "              .assign(tangramRef = 'None'))\n",
    "d_boards = (pd.read_csv('reformattedBoards.csv'))\n",
    "\n",
    "## Tag with super simple, conservative heuristic\n",
    "\n",
    "The most obvious strategy is to (on a first pass) assume that the tangram the matcher moves in response to a message is the one the message is referring to. The second pass is to skip the ones where we know they got it wrong. We'll probably end up hand-tagging those or using some other strategy depending on how many there are.\n",
    "\n",
    "# There are a few obvious problems here:\n",
    "\n",
    "# 1. The director will sometimes send several messages before the matcher moves anything. So we can't just use the closest move in time... \n",
    "# 2. instead, we could use the *first* move action after the message and then rule it out so that we won't use it again even if it's the first after later message as well\n",
    "# 3. **that**, though, also has a problem. Multiple messages are sent per tangram, and some messages are meta-chatter (e.g. \"hello\", \"thanks\", \"good job\", \"this HIT is terrible\"). If we assign the drop actions to the first $N$ messages, we'll have a bunch of actual messages about tangrams that aren't tagged and a bunch of messages **not** about tangrams incorrectly tagged.\n",
    "\n",
    "# So... we'll do a simpler thing. Check for numbers occuring in the text and look them up in the board data...\n",
    "\n",
    "pattern = re.compile('[\\W_]+')\n",
    "for index, row in d_msgs_raw.iterrows():\n",
    "    stripedStr = pattern.sub(' ', row.contents)\n",
    "    numbers = [int(s) for s in stripedStr.split() if s.isdigit()]\n",
    "    gameid = row.gameid\n",
    "    roundNum = row.roundNum\n",
    "    if len(numbers) == 1 and 0 < numbers[0] <= 12 and row.sender == 'director':\n",
    "        boardRow = d_boards.query('gameid == \"{0}\" and roundNum == {1} and trueLoc == {2}'\n",
    "                                  .format(gameid, roundNum, numbers[0]))\n",
    "        d_msgs_raw.set_value(index, 'tangramRef', boardRow.tangramName.tolist()[0])\n",
    "\n",
    "# Check to see how many we tagged...\n",
    "\n",
    "1 - Counter(d_msgs_raw['tangramRef'])['None'] / float(d_msgs_raw.shape[0])\n",
    "\n",
    "# not bad for a conservative heuristic! Now we're going to use the tagged data to train a classifier that will make predictions for the other 40%.\n",
    "\n",
    "## Train classifier\n",
    "\n",
    "###  Set up training set\n",
    "\n",
    "# Used `d_msgs_raw` in `d_combined` the first time and subsequently used the updated hand-tagged version\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "d_handtagged = pd.read_csv('handTagged.csv')\n",
    "d_nicki = (pd.read_csv('../../data/tangrams_unconstrained/old/oldTangrams.csv')\n",
    "    .query('tangram != \"*\"')\n",
    "    .drop('sender', 1)\n",
    "    .rename(columns = {'tangram' : 'tangramRef'}))\n",
    "d_combined = (d_handtagged # d_msgs_raw\n",
    "  .query('tangramRef != \"None\"')\n",
    "  .query('tangramRef != \"*\"')\n",
    "  .drop('sender', 1)\n",
    "  .append(pd.DataFrame(data = d_nicki), ignore_index=True))\n",
    "train_msg, test_msg = train_test_split(d_combined, test_size = 0.2)\n",
    "\n",
    "len(d_nicki['tangramRef'])\n",
    "\n",
    "### Build pipeline\n",
    "\n",
    "# Largely drawn from [here](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).\n",
    "\n",
    "# Import necessary sklearn modules and grid search params\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)],\n",
    "              'vect__stop_words': (None, 'english'),\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3, 1e-4, 1e-5)\n",
    "}\n",
    "\n",
    "# Train bag-of-words LR classifier \n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='log', penalty='l2',n_iter=5)),\n",
    "                    ])\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "_ = gs_clf.fit(train_msg.contents, train_msg.tangramRef)\n",
    "\n",
    "### Look at performance on held-out test set\n",
    "\n",
    "# Look at success!\n",
    "\n",
    "predicted = gs_clf.predict(test_msg.contents)\n",
    "correct = predicted == test_msg.tangramRef\n",
    "print(\"test-split accuracy is...\")\n",
    "print(sum(correct)/float(len(correct)))\n",
    "\n",
    "# Plot ROC curve\n",
    "\n",
    "test_msg.loc[:, 'predicted'] = predicted\n",
    "test_msg.loc[:, 'correct'] = test_msg['predicted'] == test_msg['tangramRef']\n",
    "test_msg.loc[:, 'maxProb'] = [max(row) for row in gs_clf.predict_proba(test_msg['contents'])]\n",
    "# We could also measure confidence using the distance between the top two categories, but this\n",
    "# turns out not to be quite as good a metric\n",
    "test_msg.loc[:, 'probDiff'] = [sorted(row)[-1] - sorted(row)[-2] \n",
    "                               for row in gs_clf.predict_proba(test_msg['contents'])]\n",
    "\n",
    "actualNumPos= float(sum(test_msg['correct']))\n",
    "actualNumNeg= len(test_msg['correct']) - float(sum(test_msg['correct']))\n",
    "\n",
    "TPRs, FPRs, thresholds = [], [], []\n",
    "for threshold in np.arange(0,1,.05) :\n",
    "    thresholds.append(threshold)\n",
    "    # Get the ones that our policy tags as \"correct\"\n",
    "    predYes = test_msg.query('maxProb > {0}'.format(threshold))['correct']\n",
    "    # TPR: number *correct* positive results relative to overall number positive samples \n",
    "    TPRs.append(sum(predYes)/actualNumPos)\n",
    "    # FPR: number *incorrect* positive results relative to overall number negative samples \n",
    "    FPRs.append((len(predYes)-sum(predYes))/actualNumNeg)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect = 'equal')\n",
    "ax.plot([0,1], [0,1])\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.plot(FPRs, TPRs, label = 'maxProb') \n",
    "\n",
    "cautiousThreshold = [threshold for threshold, FPR in zip(thresholds, FPRs) if FPR < 0.05 ][0]\n",
    "print(cautiousThreshold)\n",
    "\n",
    "# What are best params?\n",
    "\n",
    "best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "from sklearn import metrics\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plt.figure()\n",
    "cm = metrics.confusion_matrix(test_msg.tangramRef, predicted)\n",
    "tangramLabels = sorted(list(set(test_msg.tangramRef)))\n",
    "plot_confusion_matrix(cm, tangramLabels)\n",
    "\n",
    "## Tag full dataset using ROC threshold \n",
    "\n",
    "predicted_myData = gs_clf.predict(d_handtagged.contents)\n",
    "maxProbs = [max(row) for row in gs_clf.predict_proba(d_handtagged.contents)]\n",
    "existingTags = d_handtagged.tangramRef\n",
    "autoTags = [prediction if maxProb > cautiousThreshold and existing == 'None' else existing\n",
    "            for (existing, maxProb, prediction) \n",
    "            in zip(existingTags, maxProbs, predicted_myData)]\n",
    "print(sum(autoTags != existingTags))\n",
    "d_handtagged.loc[:, 'autoTags'] = autoTags\n",
    "\n",
    "d_handtagged.drop('tangramRef', axis = 1).to_csv(\"autoTagged.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
