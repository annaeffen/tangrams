{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import lots of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import nlp_utils as utils\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import annotated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_raw = pd.read_csv('handTagged.csv')\n",
    "d_nicki = (pd.read_csv('../../data/tangrams_unconstrained/old/oldTangrams.csv')\n",
    "    .query('tangram != \"*\"')\n",
    "    .drop('sender', 1)\n",
    "    .rename(columns = {'tangram' : 'tangramRef'}))\n",
    "\n",
    "# Drop time column\n",
    "d = (d_raw\n",
    "    .copy()\n",
    "    .drop('time', 1)\n",
    "    .query('tangramRef != \"0\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result 1: Generate file for POS analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Stanford CoreNLP server\n",
    "\n",
    "Before running this notebook, [get CoreNLP](http://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip), go into its directory, and run\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer 9000`\n",
    "\n",
    "If you're using port 9000 for something else, change that value and then change `PORT` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORT = 9000\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:{}'.format(PORT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: follow Will's advice to parse unicode..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picture 12 is like a guy with both hands in the air, kind of like  ¯_(ツ)_/¯ : cannot parse\n",
      "11 is  ¯_(ツ)_/¯ : cannot parse\n",
      "12 is  ¯_(ツ)_/¯ : cannot parse\n",
      "7 is  ¯_(ツ)_/¯ : cannot parse\n",
      "9 is  ¯_(ツ)_/¯ : cannot parse\n",
      "The crouching guy´s feet are a triangle?: cannot parse\n",
      "2 is  ¯_(ツ)_/¯ : cannot parse\n"
     ]
    }
   ],
   "source": [
    "# A lemma is a (word, pos) tag pair.\n",
    "d['lemmas'] = [utils.stanford_pos(text) for text in d['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d['tokens'] = [[element[0] for element in l] for l in d['lemmas']]\n",
    "d['pos'] = [[element[1] for element in l] for l in d['lemmas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d['numWords'] = [pd.value_counts(words).sum() for words in d['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what I have in order: bunny ears (1), arms in air (2), standing on one leg, other leg to the left (3), robe pointing left (4), tilted square, rectangle stuff cut out of it (5), standing on one leg, other leg to the right (6), sitting with knees (7), sitting without knees (8), spike coming out of stomach, straight back (9), tilted square, rectangle with no stuff cut out (10), spike coming out of both back and stomach (11) and funky chicken (12). : cannot parse\n",
      "picture 12 is like a guy with both hands in the air, kind of like  ¯_(ツ)_/¯ : cannot parse\n",
      "11 is  ¯_(ツ)_/¯ : cannot parse\n",
      "12 is  ¯_(ツ)_/¯ : cannot parse\n",
      "7 is  ¯_(ツ)_/¯ : cannot parse\n",
      "9 is  ¯_(ツ)_/¯ : cannot parse\n",
      "The crouching guy´s feet are a triangle?: cannot parse\n",
      "2 is  ¯_(ツ)_/¯ : cannot parse\n"
     ]
    }
   ],
   "source": [
    "d['tags'] = [[w['dep'] for w in utils.stanford_constituency(text)] for text in d['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['num_sbar'] = [utils.sbar_count(parse[0]) for parse in d['parse']]\n",
    "d['num_pp'] = [utils.pp_count(parse[0]) for parse in d['parse']]\n",
    "d['num_cc'] = [utils.cc_count(parse[0]) for parse in d['parse']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get counts for each POS label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['nouns'] = [sum([1 if utils.is_noun(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['prepositions'] = [sum([1 if utils.is_prep(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['verbs'] = [sum([1 if utils.is_verb(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['determiners'] = [sum([1 if utils.is_det(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['pronouns'] = [sum([1 if utils.is_pronoun(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['adjectives'] = [sum([1 if utils.is_adjective(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['adverbs'] = [sum([1 if utils.is_adverb(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['numbers'] = [sum([1 if utils.is_num(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]\n",
    "d['others'] = [sum([1 if utils.is_other(*lem) else 0 for lem in lemmas])\n",
    "                     for lemmas in d['lemmas']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to csv for plotting in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d.drop([\"lemmas\", \"contents\", \"tokens\"], 1)\n",
    " .to_csv(\"posTagged.csv\", index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['bigrams'] = [list(bigrams(l)) for l in d['tokens']]\n",
    "bigramDict = Counter([item for sublist in d['bigrams'].tolist()\n",
    "                     for item in sublist])\n",
    "bigramList = [bigram for (bigram,count) in bigramDict.items() if count > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBigramCounts(df, gameid, roundNum) :\n",
    "    roundCond = 'roundNum == ' + roundNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([item for sublist in relevantRow['bigrams'].tolist() \n",
    "                    for item in sublist])\n",
    "\n",
    "with open('bigramCounts.csv', 'a') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['gameid', 'roundNum', 'word', 'count'])\n",
    "    for gameid in gameidList:  \n",
    "        for roundNum in ['1', '2', '3', '4', '5', '6'] :\n",
    "            counts = getBigramCounts(d, gameid, roundNum)\n",
    "            for bigram in bigramList :\n",
    "                writer.writerow([gameid, roundNum, ' '.join(bigram), counts[bigram]])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
