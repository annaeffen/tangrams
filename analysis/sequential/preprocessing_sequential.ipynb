{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div id=\"toc\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import lots of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import textacy\n",
    "import textacy.io\n",
    "from zss import simple_distance\n",
    "from nltk import Tree\n",
    "#from utils import nlp_utils as utils\n",
    "#from nltk import bigrams\n",
    "from collections import Counter\n",
    "#from pycorenlp import StanfordCoreNLP\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the lg spacy corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import annotated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_raw = pd.read_csv('../../data/tangrams.csv')\n",
    "d = d_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all game ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b', '1567-e80221b4-f349-4b3b-9921-224ee47eea1f', '1670-7a1fc24b-6599-4efb-8a98-22ce7368261a', '5108-ab2d0b72-034a-4f00-991a-416faf93c98c', '3419-7061f84e-30ba-48dc-9bc3-133c5fdf8fa3']\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "gameidList = pd.unique(d.gameid.ravel()).tolist()\n",
    "print(gameidList[0:5])\n",
    "print(len(gameidList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all tangram names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n"
     ]
    }
   ],
   "source": [
    "tangramList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
    "print(tangramList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are most common words & phrases to reduce? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, spacy has no n-gram function, so we use textacy, a convenience wrapper around spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-87edf0eb05da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-179-87edf0eb05da>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/textacy/doc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, content, metadata, lang)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpacyDoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_spacy_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/textacy/doc.py\u001b[0m in \u001b[0;36m_init_from_text\u001b[0;34m(self, content, metadata, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_stringstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         self.spacy_doc.user_data['textacy'] = {\n\u001b[1;32m    145\u001b[0m             \u001b[0;34m'lang'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__call__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad),\n\u001b[0;32m--> 280\u001b[0;31m                                          drop=drop)\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mresidual_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/maxout.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X__bi, drop)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdrop\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0moutput__boc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX__bi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput__boc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput__boc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d['doc'] = [textacy.Doc(textacy.preprocess_text(row, lowercase = True), lang='en') for row in d['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['unigrams'] = [list(doc.to_terms_list(ngrams=1, as_strings=True, \n",
    "                                       normalize = 'lower',filter_stops = False, named_entities = False)) for doc in d['doc']]\n",
    "\n",
    "d['bigrams'] = [list(doc.to_terms_list(ngrams=2, as_strings=True, \n",
    "                                       normalize = 'lower',filter_stops = False, named_entities = False)) for doc in d['doc']]\n",
    "\n",
    "d['trigrams'] = [list(doc.to_terms_list(ngrams=3, as_strings=True, \n",
    "                                       normalize = 'lower',filter_stops = False, named_entities = False)) for doc in d['doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(countType, df, gameid, repetitionNum, tangram = None) :\n",
    "    roundCond = 'repetitionNum == ' + repetitionNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'tangramRef == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([item for sublist in relevantRow[countType]\n",
    "                    for item in sublist])\n",
    "\n",
    "for countType in ['unigrams', 'bigrams', 'trigrams'] :\n",
    "    countDict = Counter([item for sublist in d[countType]\n",
    "                         for item in sublist])\n",
    "    wordList = [v for (v,count) in countDict.items() if count > 20]\n",
    "\n",
    "    with open(countType + 'Counts.csv', 'w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['gameid', 'repetitionNum', 'word', 'count'])\n",
    "        for gameid in gameidList:  \n",
    "            for repetitionNum in ['1', '2', '3', '4', '5', '6'] :\n",
    "                counts = getCounts(countType, d, gameid, repetitionNum)\n",
    "                for word in wordList :\n",
    "                    writer.writerow([gameid, repetitionNum, word, counts[word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.attrs import POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['doc'] = [nlp(text) for text in d['contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get counts for each POS label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'INTJ': 90, 'PUNCT': 96, 'PRON': 94, 'VERB': 99, 'PART': 93, 'ADP': 84, 'NOUN': 91, 'ADV': 85, 'DET': 89, 'ADJ': 83, 'CCONJ': 88, 'NUM': 92, 'SYM': 98, 'SPACE': 102, 'PROPN': 95, 'X': 100}\n"
     ]
    }
   ],
   "source": [
    "tag_dict = {}\n",
    "for doc in d['doc'] :\n",
    "    for w in doc :\n",
    "        if w.pos not in tag_dict :\n",
    "            tag_dict[w.pos_] = w.pos\n",
    "d['posCounts'] = [doc.count_by(POS) for doc in d['doc']]\n",
    "print(tag_dict)\n",
    "for posStr in [\"NOUN\", \"DET\", \"PRON\", \"VERB\", \"ADJ\", \"CCONJ\", \"ADP\"] :\n",
    "    key_id = tag_dict[posStr]\n",
    "    d[posStr + 'count'] = [counts[key_id] if key_id in counts else 0 for counts in d['posCounts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d['tokens'] = [[element.text for element in l] for l in d['doc']]\n",
    "d['pos'] = [[element.pos_ for element in l] for l in d['doc']]\n",
    "d['noun_chunks'] = [list(l.noun_chunks) for l in d['doc']]\n",
    "d['numWords'] = [len(l) for l in d['doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'ADV', 'VERB', 'ADP', 'PRON', 'VERB', 'VERB', 'DET', 'ADJ', 'ADV', 'CCONJ', 'VERB', 'ADP', 'DET', 'NOUN', 'PART']\n",
      "['This', 'one', 'kinda', 'looks', 'like', 'they', 'are', 'looking', 'a', 'little', 'down', 'and', 'kneeling', 'with', 'both', 'arms', 'out']\n"
     ]
    }
   ],
   "source": [
    "print(d['pos'][3])\n",
    "print(d['tokens'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gameid', 'msgTime', 'trialNum', 'repetitionNum', 'role',\n",
       "       'intendedName', 'timeElapsed', 'contents', 'totalLength', 'thinksHuman',\n",
       "       'comments', 'ratePartner', 'nativeEnglish', 'time', 'intendedObj',\n",
       "       'clickedObj', 'objBox', 'correct', 'numRawWords', 'repetitionScore',\n",
       "       'taskVersion', 'doc', 'posCounts', 'NOUNcount', 'DETcount', 'PRONcount',\n",
       "       'VERBcount', 'ADJcount', 'CCONJcount', 'ADPcount', 'tokens', 'pos',\n",
       "       'noun_chunks', 'numWords', 'nouns'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to csv for plotting in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d.drop([\"tokens\", \"nouns\"], 1)\n",
    " .to_csv(\"posTagged.csv\", index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract constituency parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'>\n",
      "<class 'allennlp.modules.span_extractors.span_extractor.SpanExtractor'>\n",
      "<class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'>\n",
      "<class 'allennlp.modules.feedforward.FeedForward'>\n",
      "<class 'allennlp.modules.token_embedders.embedding.Embedding'>\n",
      "<class 'allennlp.nn.initializers.InitializerApplicator'>\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "archive = load_archive(\n",
    "            \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\"\n",
    "        )\n",
    "predictor = Predictor.from_archive(archive, 'constituency-parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edit distances on successive rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distance (label1, label2) :\n",
    "    if label1 == label2 :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "\n",
    "def get_root(doc) :\n",
    "    asdf = dict(('label' if key == 'nodeType' else key, value) for (key, value) in doc.items())\n",
    "    return asdf\n",
    "\n",
    "def get_children(subtree) :\n",
    "    if 'children' in subtree.keys() :\n",
    "        return [dict(('label' if key == 'nodeType' else key, value) for (key, value) in d.items()) for d in subtree['children']]\n",
    "    else :\n",
    "        return []\n",
    "\n",
    "def get_label(node) :\n",
    "    return node['label']\n",
    "\n",
    "def edit_distance(tree1, tree2, return_operations = False) :\n",
    "    return simple_distance(get_root(tree1), get_root(tree2), \n",
    "                           get_children, get_label, label_distance, return_operations=return_operations)\n",
    "\n",
    "def example() :\n",
    "    s1 = predictor.predict_json({\"sentence\": \"you are a dog with a big bone\"})\n",
    "    s2 = predictor.predict_json({\"sentence\": \"I am a cat\"})\n",
    "    print('tree1:', Tree.fromstring(s1['trees']))\n",
    "    print('tree2:', Tree.fromstring(s2['trees']))\n",
    "    print('operations:', edit_distance(s1, s2, return_operations=True)[1])\n",
    "    print('cost:', edit_distance(s1, s2, return_operations=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1459990132712.0]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = d['msgTime'].value_counts()\n",
    "vc[vc > 1].index.tolist()\n",
    "#set([x for x in d['msgTime'] if d['msgTime'].count(x) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13314 / 13315\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "tiny_d = d.query('role == \"director\"')[['gameid', 'msgTime', 'repetitionNum', 'contents']]\n",
    "parses = []\n",
    "for i, s in enumerate(tiny_d['contents']) :\n",
    "    clear_output(wait=True)\n",
    "    print(i, '/', len(tiny_d['contents']))\n",
    "    parses.append(predictor.predict_json({'sentence' : s})['hierplane_tree']['root'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_d['tree_parse'] = parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>msgTime</th>\n",
       "      <th>repetitionNum</th>\n",
       "      <th>contents</th>\n",
       "      <th>tree_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b</td>\n",
       "      <td>1.490375e+12</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello, I am going to describe them as people a...</td>\n",
       "      <td>{'word': 'Hello , I am going to describe them ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b</td>\n",
       "      <td>1.490375e+12</td>\n",
       "      <td>1</td>\n",
       "      <td>Is that okay?</td>\n",
       "      <td>{'word': 'Is that okay ?', 'nodeType': 'SQ', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b</td>\n",
       "      <td>1.490375e+12</td>\n",
       "      <td>1</td>\n",
       "      <td>This one kinda looks like they are looking a l...</td>\n",
       "      <td>{'word': 'This one kinda looks like they are l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b</td>\n",
       "      <td>1.490375e+12</td>\n",
       "      <td>1</td>\n",
       "      <td>wearing a dress maybe.</td>\n",
       "      <td>{'word': 'wearing a dress maybe .', 'nodeType'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b</td>\n",
       "      <td>1.490375e+12</td>\n",
       "      <td>1</td>\n",
       "      <td>Good job1 This one is sitting down</td>\n",
       "      <td>{'word': 'Good job1 This one is sitting down',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      gameid       msgTime  repetitionNum  \\\n",
       "0  7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b  1.490375e+12              1   \n",
       "1  7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b  1.490375e+12              1   \n",
       "3  7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b  1.490375e+12              1   \n",
       "4  7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b  1.490375e+12              1   \n",
       "5  7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b  1.490375e+12              1   \n",
       "\n",
       "                                            contents  \\\n",
       "0  Hello, I am going to describe them as people a...   \n",
       "1                                      Is that okay?   \n",
       "3  This one kinda looks like they are looking a l...   \n",
       "4                             wearing a dress maybe.   \n",
       "5                 Good job1 This one is sitting down   \n",
       "\n",
       "                                          tree_parse  \n",
       "0  {'word': 'Hello , I am going to describe them ...  \n",
       "1  {'word': 'Is that okay ?', 'nodeType': 'SQ', '...  \n",
       "3  {'word': 'This one kinda looks like they are l...  \n",
       "4  {'word': 'wearing a dress maybe .', 'nodeType'...  \n",
       "5  {'word': 'Good job1 This one is sitting down',...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finditem(obj, value, initLevel = True):\n",
    "    if obj['nodeType'] == value and not initLevel:\n",
    "        return True\n",
    "    elif 'children' in obj :\n",
    "        for child in obj['children'] :\n",
    "            item = finditem(child, value, initLevel = False)\n",
    "            if item is not None:\n",
    "                return item\n",
    "tiny_d['SBAR'] = [finditem(s, 'SBAR') for s in tiny_d['tree_parse']]\n",
    "tiny_d['PP'] = [finditem(s, 'PP') for s in tiny_d['tree_parse']]\n",
    "tiny_d['CC'] = [finditem(s, 'CC') for s in tiny_d['tree_parse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_d.to_json('constituency_parses.json')\n",
    "tiny_d.drop('tree_parse', 1).to_csv('constituency_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0057-414228f8-c268-40d6-9349-b35df4f080d9\n",
      "0 / 91\n",
      "0349-951c1418-40e9-48b3-8290-7ed4461f4d54\n",
      "1 / 91\n",
      "0413-e4a76b36-4367-4e30-abf9-93e823913630\n",
      "2 / 91\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6843    {'word': 'looks like a man with both arms rais...\n",
      "6844    {'word': 'leaning left', 'nodeType': 'S', 'att...\n",
      "Name: tree_parse, dtype: object\n",
      "0413-e4a76b36-4367-4e30-abf9-93e823913630 C 1\n",
      "0461-f522f8f4-37dc-4bb0-89bf-9f6bcf43274a\n",
      "3 / 91\n",
      "0711-b03679d3-9904-4263-bd2f-8ec8e7a45af7\n",
      "4 / 91\n",
      "0723-9b842133-a121-4b53-9c37-784e6023e022\n",
      "5 / 91\n",
      "0785-68fedef2-3b2f-466f-a7c3-ab464e5811ca\n",
      "6 / 91\n",
      "1202-a64916b2-49d2-4ca4-bd76-cfd3e1ec3954\n",
      "7 / 91\n",
      "1242-e6fcc1c9-ab33-44c7-b31c-4a391beb2b8c\n",
      "8 / 91\n",
      "1526-db8d7e61-c668-4fe2-9119-cbf018ccb1c1\n",
      "9 / 91\n",
      "1567-e80221b4-f349-4b3b-9921-224ee47eea1f\n",
      "10 / 91\n",
      "1598-17655913-31a4-4ca6-b86c-383db06bfde9\n",
      "11 / 91\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "9100    {'word': 'very plain figure', 'nodeType': 'NP'...\n",
      "Name: tree_parse, dtype: object\n",
      "1598-17655913-31a4-4ca6-b86c-383db06bfde9 H 1\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "9095    {'word': 'Triangle head', 'nodeType': 'NP', 'a...\n",
      "9096    {'word': 'one shouldder pointing towards right...\n",
      "9098    {'word': 'one trangular leg', 'nodeType': 'NP'...\n",
      "9099    {'word': 'both on right side', 'nodeType': 'FR...\n",
      "Name: tree_parse, dtype: object\n",
      "1598-17655913-31a4-4ca6-b86c-383db06bfde9 J 1\n",
      "9076    {'word': 'kind of praying but one leg pointing...\n",
      "9077    {'word': 'square head', 'nodeType': 'NP', 'att...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "1598-17655913-31a4-4ca6-b86c-383db06bfde9 K 1\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "9125    {'word': 'pray with one leg folded', 'nodeType...\n",
      "Name: tree_parse, dtype: object\n",
      "1598-17655913-31a4-4ca6-b86c-383db06bfde9 K 2\n",
      "1624-fe1e73c3-3feb-4949-8e71-31e9f955db53\n",
      "12 / 91\n",
      "1670-7a1fc24b-6599-4efb-8a98-22ce7368261a\n",
      "13 / 91\n",
      "1747-31c92e68-6900-4f75-b1d5-f0fe4ea823a3\n",
      "14 / 91\n",
      "2002-d128dee1-72d2-4585-9488-fcedfabe7722\n",
      "15 / 91\n",
      "2081-2275809d-523b-432f-b3aa-7c7870c4c18e\n",
      "16 / 91\n",
      "2223-ecc53ef3-7e9a-4776-b7c5-6811400d1a65\n",
      "17 / 91\n",
      "2403-abcbe4e3-634c-40ec-89f8-95a95baedff9\n",
      "18 / 91\n",
      "2535-a47c997d-4aac-43f5-adad-6d8e492ba2eb\n",
      "19 / 91\n",
      "3296-212e4ca6-9ec7-4e38-bacf-a739c9812e48\n",
      "20 / 91\n",
      "3321-5540179d-7955-4363-b9d2-e392f26a1256\n",
      "21 / 91\n",
      "3364-2e384b58-3bda-439a-97f8-925808493940\n",
      "22 / 91\n",
      "3419-7061f84e-30ba-48dc-9bc3-133c5fdf8fa3\n",
      "23 / 91\n",
      "3453-2eab60c9-d1cd-4245-919c-c499f303740e\n",
      "24 / 91\n",
      "3617-9ac86786-87e4-468d-bbd9-ffdb2a55bf4e\n",
      "25 / 91\n",
      "3752-a8b52b62-f711-4601-a8cc-a8afddd35560\n",
      "26 / 91\n",
      "3980-e156a142-fc10-4a2f-8b6a-03d4f18343b9\n",
      "27 / 91\n",
      "4011-501727ae-d5dc-405f-8450-3cca55fc99e7\n",
      "28 / 91\n",
      "4045-c97cdcce-7538-408d-b8dd-5de0f2de8574\n",
      "29 / 91\n",
      "4076-4cf65365-3773-4e94-abc7-06f8f15081f3\n",
      "30 / 91\n",
      "4090-bf4b8726-1596-4231-84d9-718a2a37eee3\n",
      "31 / 91\n",
      "4369-cd2d1d26-3bf9-4e54-a79a-2aacea53b331\n",
      "32 / 91\n",
      "4505-8cf1d946-db8a-413a-93bd-97bb5ba3fda6\n",
      "33 / 91\n",
      "4513-c541e940-73fd-46b6-b19c-221458a22970\n",
      "34 / 91\n",
      "4676-a65ba686-b220-4f71-91d6-0fbf1138e726\n",
      "35 / 91\n",
      "4729-d4a13957-3f36-496d-8eea-8c528e25740e\n",
      "36 / 91\n",
      "4803-a27d2cc6-e504-4f67-8216-22679d81b1a6\n",
      "37 / 91\n",
      "4824-58d5c545-a718-4653-89db-4be922da59d7\n",
      "38 / 91\n",
      "4843-98b6ce35-ee60-474f-8281-9a944213f01e\n",
      "39 / 91\n",
      "4853-e6b0e457-f035-428e-be9a-9191a3a4a222\n",
      "40 / 91\n",
      "5032-f97819b4-cbef-4293-9d27-81134172edac\n",
      "41 / 91\n",
      "5100-ae37a906-ac5a-4cf2-b89a-aeb68b66e2b2\n",
      "42 / 91\n",
      "5108-ab2d0b72-034a-4f00-991a-416faf93c98c\n",
      "43 / 91\n",
      "5154-e7f636b5-f787-4bef-be2b-83083fd48c28\n",
      "44 / 91\n",
      "5192-8023d95b-29c5-49aa-a245-42924607dd4b\n",
      "45 / 91\n",
      "5315-f238c198-20c0-4162-932c-76cb0ad9e5b8\n",
      "46 / 91\n",
      "5693-668d2b0f-d15b-4769-acdf-8017caffe1e5\n",
      "47 / 91\n",
      "5774-0e16513f-9938-4b1e-a647-86933d7ef911\n",
      "48 / 91\n",
      "5792-f5aa5cf6-b3d0-4444-8442-4ef42e3b7c08\n",
      "49 / 91\n",
      "5851-2c495782-8cef-496e-abba-1730655488cb\n",
      "50 / 91\n",
      "5964-47d6642b-ed6a-4458-8592-8919b41eda6e\n",
      "51 / 91\n",
      "6064-6e0d77e9-b8e8-4a65-97d2-320567c486cc\n",
      "52 / 91\n",
      "6254-50e82f4c-5a88-4a7a-a1f1-53f6b5d4f778\n",
      "53 / 91\n",
      "6335-f2ee42fb-60a5-4238-9bfc-e23b14160049\n",
      "54 / 91\n",
      "6402-3db6c45b-0b53-4906-b5cf-17517d013671\n",
      "55 / 91\n",
      "6413-5ebde90e-fb15-4c9b-a6f9-0934ba1cc2a1\n",
      "56 / 91\n",
      "6488-1c981017-2f98-48e2-88b2-b8a814c93bc1\n",
      "57 / 91\n",
      "6524-a23c203d-4591-4fa2-aabd-d9c0063cea73\n",
      "58 / 91\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f\n",
      "59 / 91\n",
      "2803    {'word': 'two legs walking left square on back...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f A 4\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f A 5\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2776    {'word': 'square triangle on top', 'nodeType':...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f B 1\n",
      "2782    {'word': 'nose pointing left square triangle o...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f B 3\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f B 4\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f B 5\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f C 1\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2797    {'word': 'whole image leaniong far left , two ...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f C 2\n",
      "2822    {'word': 'yes', 'nodeType': 'INTJ', 'attribute...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f C 5\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2778    {'word': 'man pointing left', 'nodeType': 'S',...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f D 1\n",
      "2802    {'word': 'pointing left flat bottom , and back...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f D 4\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f D 5\n",
      "2792    {'word': 'rabbit or windmill', 'nodeType': 'NP...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f E 3\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2823    {'word': 'leaning to left', 'nodeType': 'S', '...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f E 4\n",
      "2823    {'word': 'leaning to left', 'nodeType': 'S', '...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f E 5\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2772    {'word': 'n', 'nodeType': 'X', 'attributes': [...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f F 1\n",
      "2811    {'word': 'bflpe', 'nodeType': 'NP', 'attribute...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f F 5\n",
      "2755    {'word': 'flat botton pointing to the left 2 e...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f G 1\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2793    {'word': 'flat bottom square on left top', 'no...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f G 2\n",
      "2793    {'word': 'flat bottom square on left top', 'no...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f G 3\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f G 4\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f G 5\n",
      "2768    {'word': 'square on head flat ledt side', 'nod...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f H 2\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2804    {'word': 'straight left angled towards botton ...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f H 3\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2774    {'word': 'feet', 'nodeType': 'NP', 'attributes...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f I 1\n",
      "2801    {'word': 'dancer shape leaning left leg to the...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f I 4\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2831    {'word': 'walking ft', 'nodeType': 'S', 'attri...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f I 5\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2779    {'word': 'flat left pointing right', 'nodeType...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f J 1\n",
      "2795    {'word': 'straight left , straight bottom stra...\n",
      "2796    {'word': 'on top', 'nodeType': 'PP', 'attribut...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f J 3\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2812    {'word': 'tting to the right', 'nodeType': 'S'...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f J 4\n",
      "2812    {'word': 'tting to the right', 'nodeType': 'S'...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f J 5\n",
      "2770    {'word': 'st', 'nodeType': 'X', 'attributes': ...\n",
      "Name: tree_parse, dtype: object\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f K 2\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f K 3\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f K 4\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2836    {'word': 'left', 'nodeType': 'X', 'attributes'...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f K 5\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "2775    {'word': 'pointing right', 'nodeType': 'S', 'a...\n",
      "Name: tree_parse, dtype: object\n",
      "6938-63086ce4-ea10-43bf-9511-e0a4a519592f L 1\n",
      "6956-00137ff0-3569-4135-81ab-8c0453a2f6ce\n",
      "60 / 91\n",
      "7026-d36b3d5a-1ba3-403c-bcbd-31d0d1a0aa6b\n",
      "61 / 91\n",
      "7171-cf9d40d1-2a66-4c80-8db6-58ac192916f2\n",
      "62 / 91\n",
      "7194-9e2d0432-1fda-4f7d-8faa-908b7f14cb8d\n",
      "63 / 91\n",
      "7229-0a7402b0-329e-4b26-bc3a-ae26a388a6cc\n",
      "64 / 91\n",
      "7391-0be22306-9b90-402c-82b0-6bf7ce3ac35e\n",
      "65 / 91\n",
      "7484-32a69501-b5bc-4166-ac2b-43214360890e\n",
      "66 / 91\n",
      "7543-8662d7c6-aa4c-46a2-8125-7fad6ccd5afe\n",
      "67 / 91\n",
      "7547-22755035-a3a9-4044-aca1-efacfbd02163\n",
      "68 / 91\n",
      "7642-536146ba-7c31-4590-832e-905c0126f60e\n",
      "69 / 91\n",
      "7919-c64a8035-5c54-4696-9fab-21367611c8b6\n",
      "70 / 91\n",
      "7971-85165980-35a3-4070-8ebf-ca2d159dc715\n",
      "71 / 91\n",
      "8505-2fa3d6f9-8e28-49ed-8cd1-de703e113a82\n",
      "72 / 91\n",
      "8553-87ac5b2f-ee64-443b-a3a9-d54b50a60f77\n",
      "73 / 91\n",
      "8654-d24d0e84-d1d0-455c-8f51-b3102bf9240c\n",
      "74 / 91\n",
      "8656-2ce86ed6-891b-459e-b44f-f8d0ba10c8e2\n",
      "75 / 91\n",
      "8659-24ecfb2c-6a3d-4607-ab5e-a37e6183634c\n",
      "76 / 91\n",
      "8717-5275ed0c-7fcc-4114-8327-2e32a234c2e0\n",
      "77 / 91\n",
      "8740-51fc5610-27fd-4f47-a2a2-ce204bd572d1\n",
      "78 / 91\n",
      "8806-7b0fde86-a7ed-4821-9482-22cb3454e156\n",
      "79 / 91\n",
      "8915-5ba403bf-9d53-44e0-a5e3-2682167d0da8\n",
      "80 / 91\n",
      "9021-ad91b3af-fb13-4360-931d-dc6bc2744e7a\n",
      "81 / 91\n",
      "9116-d8b8c5c1-f549-48bf-a362-ed159302a106\n",
      "82 / 91\n",
      "9214-a3ed3b49-fe0c-4c41-b40f-d1d6e7fd0b5d\n",
      "83 / 91\n",
      "9275-8cd69f6d-2b0e-494b-8fea-ab647d9f4b9c\n",
      "84 / 91\n",
      "9286-508b20ce-1cbb-409f-bd5a-f2e6b3ffa7a8\n",
      "85 / 91\n",
      "9471-f19badf8-f934-48be-af51-c28ad91605b9\n",
      "86 / 91\n",
      "9477-662e7f6d-0f2a-4041-a4e7-6d33eba91fb0\n",
      "87 / 91\n",
      "9559-04e2b195-6540-4711-9830-5178487fc07c\n",
      "88 / 91\n",
      "Series([], Name: tree_parse, dtype: object)\n",
      "3168    {'word': 'this is the most nonhuman like one ....\n",
      "Name: tree_parse, dtype: object\n",
      "9559-04e2b195-6540-4711-9830-5178487fc07c G 1\n",
      "9567-370cab8f-7744-4910-8e41-e0a50b191fac\n",
      "89 / 91\n",
      "9684-5db90167-5d0b-40a4-9ec2-1c80b928608b\n",
      "90 / 91\n"
     ]
    }
   ],
   "source": [
    "tiny_d = pd.read_json('constituency_parses.json')\n",
    "\n",
    "gameids = np.unique(tiny_d['gameid'])\n",
    "tangramids = np.unique(tiny_d['intendedName'])\n",
    "\n",
    "transitions = np.zeros([len(gameids), len(tangramids) ,5])\n",
    "for i, gameid in enumerate(gameids) :\n",
    "    print(gameid)\n",
    "    game_d = tiny_d.query('gameid == \"{0}\"'.format(gameid))\n",
    "    print(i, '/', len(gameids))\n",
    "    for j, intendedName in enumerate(tangramids) :\n",
    "        for k, init_occurrenceNum in enumerate(range(1,6)) :\n",
    "            dist = []\n",
    "            sub1 = game_d.query('intendedName == \"{0}\" and occurrenceNum == {1}'.format(intendedName, init_occurrenceNum))['tree_parse']\n",
    "            sub2 = game_d.query('intendedName == \"{0}\" and occurrenceNum == {1}'.format(\n",
    "                intendedName, init_occurrenceNum+1\n",
    "            ))['tree_parse']\n",
    "            for tree1 in sub1 :\n",
    "                for tree2 in sub2 :\n",
    "                    dist.append(edit_distance(tree1, tree2))\n",
    "            if(dist == []) :\n",
    "                print(sub1)\n",
    "                print(sub2)\n",
    "                print(gameid, intendedName, init_occurrenceNum)\n",
    "            transitions[i, j, k] = np.max(dist) if dist else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['parse'] = [[w for w in utils.stanford_parsetree(text)] for text in d['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree.fromstring(d['parse'][10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.04395604, 14.35164835,  8.04395604,  4.52222222,  4.76666667],\n",
       "       [25.03333333, 16.68131868, 11.64444444,  7.91111111,  6.18888889],\n",
       "       [15.49438202,  7.97777778,  5.17582418,  2.78021978,  3.74444444],\n",
       "       [19.7       , 12.07692308,  7.52747253,  4.3       ,  3.62222222],\n",
       "       [17.43956044,  7.94505495,  4.86666667,  2.54444444,  2.37777778],\n",
       "       [18.54444444, 14.20879121,  7.93406593,  5.24175824,  3.63333333],\n",
       "       [25.16853933, 16.83333333,  9.44444444,  6.24444444,  6.2       ],\n",
       "       [21.65555556, 11.71111111,  6.42222222,  4.56043956,  3.52747253],\n",
       "       [21.47777778, 13.18681319,  7.74725275,  5.36666667,  3.55555556],\n",
       "       [22.21348315, 13.64835165,  7.92222222,  5.65555556,  3.96666667],\n",
       "       [24.76666667, 17.88764045, 11.9       ,  7.58888889,  6.22222222],\n",
       "       [21.03333333, 13.69230769,  9.34065934,  6.6043956 ,  5.67032967]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(transitions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result 2: Calculate indicator words for tangrams/rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, get list of words in first round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to first round\n",
    "d_round1 = d[d['occurrenceNum'] == 1]\n",
    "\n",
    "# Pull out all tokens and collapse into count dict\n",
    "tokenDict = Counter([item for sublist in d_round1['tokens'].tolist()\n",
    "                     for item in sublist])\n",
    "# Pull out all words that occur more than once\n",
    "wordList = [word for (word,count) in tokenDict.items() if count > 1 and not word.isdigit()]\n",
    "print(wordList[0:10])\n",
    "print(len(wordList))\n",
    "\n",
    "# Get POS map; will be longer because it doesn't require count > 1, but it doesn't matter\n",
    "POSdict = {word: POS for lemma in d_round1['lemmas'] for (word, POS) in lemma}\n",
    "print(len(POSdict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to select words & counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCounts(df, gameid, occurrenceNum, tangram = None) :\n",
    "    roundCond = 'occurrenceNum == ' + occurrenceNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'intendedObj == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([item for sublist in relevantRow['tokens'].tolist() \n",
    "                    for item in sublist])\n",
    "\n",
    "#creates mini dataframe that grabs the words used in round n for a given tangram and gameid\n",
    "def selectTangramRoundWords(df, tangram, roundNum, gameid):\n",
    "    wordCounts = getWordCounts(df, gameid, roundNum, tangram)\n",
    "    return list(wordCounts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to compute PMIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that merging is really costly -- if we need to speed it up, this might be the first target. Can also vectorize the log operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a table with the all words above 0 PMI and their counts for a given tangram\n",
    "#calculate the probability for words given tangram A ------ p(x|y)\n",
    "def makeMyPMI(df, tangram, roundNum, gameid, totals):\n",
    "\n",
    "    # count words w/in tangram\n",
    "    tangramCounts = getWordCounts(df, gameid, roundNum, tangram)\n",
    "\n",
    "    #total number of words \n",
    "    tangramNumWords = (1 if sum(tangramCounts.values()) == 0 \n",
    "                       else sum(tangramCounts.values()))\n",
    "\n",
    "    #dataframe to compare \n",
    "    indicatorDF = pd.merge(pd.DataFrame(list(tangramCounts.items()), columns=['word', 'count']),\n",
    "                           pd.DataFrame(list(totals[\"counts\"].items()), columns=['word', 'totalCount']),\n",
    "                           on='word', how = 'inner')\n",
    "\n",
    "    #calculate PMI without log first. Having trouble with float issues. \n",
    "    indicatorDF['roughPMI'] = ((indicatorDF['count']/tangramNumWords)\n",
    "                                / (indicatorDF['totalCount']/totals[\"numWords\"]))\n",
    "    indicatorDF['logPMI'] = [math.log10(num) for num in indicatorDF['roughPMI']]\n",
    "    \n",
    "    #remove column rough PMI\n",
    "    indicatorDF = indicatorDF.drop('roughPMI', 1)\n",
    "    \n",
    "    return indicatorDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out PMIs & matching rates for all words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do a sloppy optimization by only computing total counts once and only when necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoize(d, gameid, counts) : \n",
    "    if \"counts\" not in counts : \n",
    "        counts[\"counts\"] = getWordCounts(d, gameid, \"1\")\n",
    "        counts[\"numWords\"] = float(sum(counts[\"counts\"].values()))\n",
    "        return counts\n",
    "    else :\n",
    "        return counts\n",
    "\n",
    "with open('sequential_matchAndPMI.csv', 'a', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['word', 'POS', 'match', 'pmi', 'total'])\n",
    "    for word in wordList :\n",
    "        print(word + \":\" + POSdict[word])\n",
    "        pmi = 0\n",
    "        match = 0\n",
    "        total = 0\n",
    "        for gameid in gameidList:  \n",
    "            memoizedCounts = {}\n",
    "            for tangram in tangramList:\n",
    "                memoizedCounts = memoize(d, gameid, memoizedCounts)\n",
    "                round1WordList = selectTangramRoundWords(d, tangram, \"1\", gameid)\n",
    "                total = total + 1 if word in round1WordList else total\n",
    "                if word in round1WordList :\n",
    "                    PMI_df = makeMyPMI(d, tangram, \"1\", gameid, memoizedCounts)\n",
    "                    pmi = pmi + PMI_df[PMI_df['word'] == word]['logPMI'].tolist()[0]\n",
    "                    round6WordList = selectTangramRoundWords(d, tangram, \"6\", gameid)\n",
    "                    match = (match + 1 if (word in round1WordList and word in round6WordList)\n",
    "                             else match)\n",
    "        writer.writerow([word, POSdict[word], float(match) / float(total), pmi/total, total])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also write out raw word counts on each round (so we can see what most likely words to be dropped are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in d :\n",
    "    print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: exclude numbers earlier in the pipeline, \n",
    "\n",
    "TODO: don't average over matches and pmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSamples = 1000\n",
    "\n",
    "#grab words with highestPMI for a given tangram/gameid\n",
    "def highestPMIWords(d, tangram, roundNum, gameid):\n",
    "    allTangramCounts = {}\n",
    "    allTangramCounts['counts'] = getWordCounts(d, gameid, \"1\")\n",
    "    allTangramCounts['numWords'] = float(sum(allTangramCounts[\"counts\"].values()))\n",
    "\n",
    "    PMIdf = (makeMyPMI(d, tangram, roundNum, gameid, allTangramCounts))\n",
    "\n",
    "    # Remove numbers\n",
    "    PMIdf['POS'] = [POSdict[word] for word in PMIdf['word']]\n",
    "    PMIdf = PMIdf.query('POS != \"CD\"'.format())\n",
    "\n",
    "    #if PMIdf has words, pull out max values, it is empty return it as is\n",
    "    if len(PMIdf.index) > 0:\n",
    "        PMI_values = PMIdf.logPMI.unique()\n",
    "        maxPMI = PMI_values.max()\n",
    "        PMIdf = PMIdf.loc[PMIdf['logPMI'] == maxPMI]\n",
    "        PMIdfword = PMIdf['word']\n",
    "        return PMIdfword.tolist()\n",
    "    else: \n",
    "        return PMIdf\n",
    "\n",
    "with open('PMIbootstrap.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['sampleNum', 'tangram', 'gameid', 'numCandidates', 'match', 'highest'])\n",
    "    for gameid in gameidList :\n",
    "        for tangram in tangramList :\n",
    "            round1Words = selectTangramRoundWords(d, tangram, \"1\", gameid)\n",
    "            if len(round1Words) > 0:\n",
    "                # First, write highest PMI match\n",
    "                highPMIWords = highestPMIWords(d, tangram, \"1\", gameid)\n",
    "                round6Words = selectTangramRoundWords(d, tangram, \"6\", gameid)\n",
    "                match = np.mean([1 if word in round6Words else 0 for word in highPMIWords ])\n",
    "                writer.writerow([0, tangram, gameid, len(highPMIWords), match, \"highest\"])\n",
    "\n",
    "                # Next, take a bunch of null samples\n",
    "                for i in range(numSamples) :\n",
    "                    randomWord = np.random.choice(round1Words)\n",
    "                    match = np.mean([1 if randomWord in round6Words else 0])\n",
    "                    writer.writerow([i + 1, tangram, gameid, 1, match, \"null\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
