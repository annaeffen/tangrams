---
title: "tangramsReference"
output: 
  html_document:
    toc: true
    toc_depth: 2

---

# Import data

```{r}
library(tidyverse)
library(lme4)
library(lmerTest)
library(tm)
library(stringr)
library(knitr)
library(NLP)
library(readr)
library(tidyboot)
library(xtable)
library(ggthemes)
library(entropy)
```

# Dynamics of structure/syntax

Which bigrams decrease the most?

```{r}
read_csv('../analysis/sequential/bigramCounts.csv')
```

# Dynamics of semantics

We've already done most of the work using nltk in the ipython notebook, so here we just read in the csv files we created there, do a bit of post-processing, and make plots.

## What are the unigrams/bigrams/trigrams that drop the most in frequency?

```{r}
# Function takes a countType ['unigrams', 'bigrams', 'trigrams'] and returns a table of most reduced n-gram
getMostReduced = function(countType) {
  filename <- paste0("sequential/", countType, "Counts.csv")
  print(filename)
  return(read_csv(filename, col_names = T) %>%
    group_by(word, repetitionNum) %>%
    summarize(count = sum(count)) %>%
    rowwise() %>%
    mutate(repetitionNum = paste0("rep", repetitionNum, collapse = "")) %>%
    spread(repetitionNum, count) %>%
    mutate(diffSize = rep1 - rep6,
           diffPct = (rep1 - rep6)/rep1) %>%
    arrange(desc(diffSize))) 
}
```

```{r}
getMostReduced('unigrams')
```

```{r}
getMostReduced('bigrams')

```

```{r}
getMostReduced('trigrams')
```

Make little table to put in supplemental?

```{r}
topWords <- t(cbind(getMostReduced('unigrams') %>% select(word) %>% head(10), 
                    getMostReduced('bigrams') %>% select(word) %>% head(10),
                    getMostReduced('trigrams') %>% select(word) %>% head(10)))

colnames(topWords) <- paste0(paste('#', seq(10), sep = ''))
rownames(topWords) <- c('unigrams', 'bigrams', 'trigrams')
topWords.toPrint = xtable(topWords, label = 'tab:words',
                          caption = 'Top 10 unigrams and bigrams with the highest reduction')
align(topWords.toPrint) <- paste0(c("|r||", rep('l|', 10)), collapse = "")
print(topWords.toPrint, floating.environment = "table*", comment=F, table.placement = 't')
```

Seems to be markers of figurative descriptions, determiners, and modifying phrases like 'on the left'?

## How do various properties reduce over time?

Reduction phenomena. From left: (1) mean message length in words per tangram, (2) mean number of listener messages, (3) proportion of utterances containing adjectival clauses, (4) proportion of utterances containing subordinate clauses. Error bars are bootstrapped 95\\% CIs.

```{r, cache = T}
posTags <- read_csv('sequential/sequential_posTagged.csv')

lengthReduction = tangramCombined %>%
   filter(role == "director") %>%
   group_by(gameid, roundNum) %>%
   summarize(individualM = sum(numRawWords)/12) %>%
   group_by(roundNum) %>%
   multi_boot_standard("individualM") %>%
   mutate(measure = '# words per tangram')

clauseReduction = posTags %>%
  group_by(roundNum) %>%
  mutate(clauses=as.logical(clauses)) %>%
  multi_boot_standard('clauses') %>%
  mutate(measure = '% subordinate clauses')

aclReduction = posTags %>%
  group_by(roundNum) %>%
  mutate(acl=as.logical(acl)) %>%
  multi_boot_standard('acl') %>%
  mutate(measure = '% adjectival clauses')

listenerMsgs <- tangramCombined %>%
  group_by(gameid, roundNum, role) %>%
  summarize(individualM = n()) %>%
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>%
  spread(role, individualM)

listenerReduction <- listenerMsgs %>%
   group_by(roundNum) %>%
   multi_boot_standard("matcher") %>%
   mutate(measure = '# listener messages')  

listenerReduction %>%
  rbind(lengthReduction) %>%
  rbind(clauseReduction) %>%
  rbind(aclReduction) %>%
  mutate(measure = factor(measure, levels = c(
    "# words per tangram", "# listener messages",
    "% adjectival clauses","% subordinate clauses"))) %>%
  ggplot(aes(x = roundNum, y = mean)) +
    geom_line() +
    xlab('round #') +
    ylab('metric') +
    geom_errorbar(aes(ymax = summary_ci_upper, ymin = summary_ci_lower), width = .1) +
    facet_wrap(~ measure, nrow = 1, ncol = 4, scales = 'free') +
    ylim(c(0, NA)) +
    theme_few(9) +
    theme(aspect.ratio = .75)
```

```{r}
listenerMsg_lm = summary(lmer(matcher ~ roundNum + (1 | gameid), data = listenerMsgs))
print(listenerMsg_lm)
```
Reduction rates for different parts of speech. Error bars are bootstrapped 95\\% CIs.

```{r}
posReduction = read_csv('posTagged.csv') %>%
  # Count occurences of each POS on each round within games
  group_by(roundNum, gameid) %>%
  summarize(numWords = sum(numWords),
            nouns = sum(nouns),
            #numbers = sum(numbers),
            verbs = sum(verbs),
            dets= sum(determiners),
            pronouns = sum(pronouns),
            preps = sum(prepositions),
            adjectives = sum(adjectives),
            adverbs = sum(adverbs)) %>%
  gather(POS, count, nouns:adverbs) %>%
  select(gameid, roundNum, POS, count) %>%
  # Need to put in ids to spread
  rowwise() %>%
  mutate(id = row_number()) %>%
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, count) %>%
  # Compute % reduction from first to last round
  mutate(diffPct = (round1 - round6)/round1) %>%
  group_by(POS) %>%
  # Filter out handful of people who skipped first round w/ negative %...
  filter(diffPct >= 0) %>%
  filter(POS != "OTHER") %>%
  # Take mean & se over participants
  multi_boot_standard('diffPct') %>%
  mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps'), 'closed', 'open')) %>%
  # rearrange
  transform(POS=reorder(POS, -mean) )

detReductionRate <- (posReduction %>% filter(POS == 'dets'))$mean * 100
nounReductionRate <- (posReduction %>% filter(POS == 'nouns'))$mean * 100

ggplot(posReduction, aes(x = POS, y = mean, fill = cat)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymax = summary_ci_upper, ymin = summary_ci_lower), width = .1)+
  theme_bw(9) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("% reduction") +
  xlab("Part of Speech category")
```

## Arbitrariness and stability

Calculate entropy under permutation tests

```{r}
getCounts <- function(contents) {
  corpus <- Corpus(VectorSource(paste(contents, collapse = " ")))
  return(colSums(as.matrix(DocumentTermMatrix(corpus))))
}

permutationTest <- function(df) {
  permuted = df %>%
    group_by(roundNum) %>%
    mutate(permutation = sample(contents)) %>%
    group_by(gameid) %>%
    summarize(ent = entropy(getCounts(permutation))) %>%
    ungroup()
  return(mean(permuted$ent))
}

permutations <- replicate(1000, {
  permutationTest(tangramCombined)
})

# Note that we explicitly do *not* normalize entropies, as this
# removes the contribution of a bigger vocabulary (as we would expect)
trueEntropy <- mean((tangramCombined %>%
  group_by(gameid) %>%
  summarize(ent = entropy(getCounts(contents))) %>%
  ungroup())$ent)

ggplot() +
  geom_histogram(aes(x = permutations), binwidth = .005) +
  geom_vline(xintercept = trueEntropy, color = 'red') +
  theme_few(9) +
  xlab('entropy')
```

## Did pairs who talked more become more efficient?

```{r}
turnTaking <- listenerMsgs %>%
  filter(roundNum %in% c(1)) %>%
  group_by(gameid) %>%
  summarize(numListenerMsgs = mean(matcher)) %>%
  filter(numListenerMsgs < mean(numListenerMsgs) + 3*sd(numListenerMsgs)) %>%
  select(gameid, numListenerMsgs)

efficiency <- tangramCombined %>%
   filter(role == "director") %>%
   group_by(gameid, roundNum) %>%
   summarize(individualM = sum(numRawWords)/12) %>%
  rowwise() %>%
  mutate(id = row_number()) %>%
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, individualM) %>%
  mutate(diffPct = (round1 - round6)/round1) %>%
  filter(diffPct >= 0) %>% # Filter out handful of people who skipped first round...
  select(gameid, diffPct)

turnTakingEfficiencyPlot <- ggplot(turnTaking %>% left_join(efficiency), aes(x = numListenerMsgs, y = diffPct)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_bw(9) +
  ylab("% reduction") +
  xlab("# listener messages on 1st round")

turnTakingEfficiency_lm <- summary(lm(diffPct ~ numListenerMsgs, data = efficiency %>% left_join(turnTaking)))
turnTakingdf <- turnTakingEfficiency_lm$df[1]
turnTakingCoefs <- turnTakingEfficiency_lm$coefficients[2,]
turnTakingResult <- paste0("t(", turnTakingdf, ") = ", round(turnTakingCoefs[3],2), ", p = ", round(turnTakingCoefs[4],2))
turnTakingEfficiencyPlot
```


### Supplemental: Director vs. matcher proportions

At beginning, directors send about 60% of total messages (close to equal!) At end, they send 80% -- listeners stop talking as much. This is just another way of looking at the total number of listener messages dropping.

```{r}
tangramCombined %>% 
  group_by(gameid, roundNum, role) %>% 
  summarize(individualM = n()) %>% 
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>% 
  spread(role, individualM) %>% 
  mutate(ratio = director / (director + matcher)) %>%
   group_by(roundNum) %>% 
   summarize(m = mean(ratio), 
             se = sd(ratio)/sqrt(length(ratio))) %>%
ggplot(aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("% of total messages sent by director") +
  xlab("trials") +
  ylim(.5,1) +
  xlim(0, 7) +
  theme_bw() 
```

# Reduction per tangram

```{r}
library(directlabels)

lengthReduction <- tangramCombined %>%
   filter(role == "director") %>%
   group_by(gameid, occurrenceNum, intendedName) %>%
   summarize(individualM = sum(numRawWords)) %>%
   group_by(occurrenceNum, intendedName) %>%
   summarize(m = mean(individualM)) %>%
   #multi_boot_standard("individualM") %>%
   mutate(measure = '# words per tangram')

ggplot(lengthReduction, aes(x = occurrenceNum, y = m,group = intendedName)) +
  geom_line() +
  theme_few() +
  geom_dl(aes(label = intendedName), method = list(dl.trans(x = x - .1), "first.points")) +
  xlab("repetition #") +
  ylab("mean # words") +
  theme(aspect.ratio = 1)
  
ggsave('../writing/tangrams/figs/num_words_sequential.pdf')
```

Questions: 

-- is just this a result of motivation, i.e. people get sloppy and just want to finish as game goes on? (probably not; accuracy goes up and no comparable reduction in other ref games of similar length w/o repetition, e.g. chairs, colors)

-- is this just a result of meta-discourse? Maybe people are just chatting each other up a lot at the beginning (e.g. 'hi, how are you? what's your name?') and stop doing that over time? (probably not, but we should do this restricted to just noun phrases...)

```{r}
summary(lmer(numRawWords ~ poly(occurrenceNum,2) + 
               (1 + occurrenceNum | gameid) + 
               (1 + occurrenceNum | intendedObj), 
             data = tangramCombined %>% 
               group_by(gameid, roundNum, occurrenceNum, intendedObj) %>% 
               summarize(numRawWords = sum(numRawWords))))
```

# PMI

Scatter plot (all datapoints)

```{r}
distinctiveness_d <- read.csv("sequential_matchAndPMI.csv", header = TRUE) %>%
  mutate(POS = as.character(POS)) %>%
  mutate(POS = ifelse(POS %in% c('NN', 'NNS', 'NNP', 'NNPS'), "noun", POS)) %>%
  mutate(POS = ifelse(POS %in% c('MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG'), "verb", POS)) %>%
  mutate(POS = ifelse(POS %in% c('DT', 'WDT'), 'det', POS)) %>%
  mutate(POS = ifelse(POS %in% c('PRP', 'PRP$', 'WP', 'WP$'), 'pronoun', POS)) %>%
  mutate(POS = ifelse(POS %in% c('CC'), 'conjunction', POS)) %>%
  mutate(POS = ifelse(POS %in% c('JJ', 'JJR', 'JJS'), 'adjective', POS)) %>%
  mutate(POS = ifelse(POS == 'IN', 'preposition', POS)) %>%
  mutate(POS = ifelse(POS %in% c('noun', 'verb', 'det', 'pronoun', 'conjunction', 'adjective', 'preposition'), 
                      POS, 'other')) %>%
  filter(pmi > 0) %>%
  rename(num_occurrences = total) %>%
  filter(num_occurrences > 1) %>%
  #filter(POS == "NN") %>%
  mutate(bunny = word == "bunny") %>%
  mutate(a_match = word == "a")
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# Split out into binary values:
binary.d <- distinctiveness_d %>% 
  mutate(numOnes = match * num_occurrences,
         numZeros = num_occurrences - numOnes) %>%
  do(rbind(data.frame(.[rep(1:nrow(.), .$numOnes),]) %>% mutate(singleMatch = 1),
           data.frame(.[rep(1:nrow(.), .$numZeros),]) %>% mutate(singleMatch = 0))) 

summary(glm(singleMatch ~ pmi, data=binary.d, family = 'binomial'))

ggplot(distinctiveness_d, aes(x = pmi, y = match)) +
  geom_point(aes(size = num_occurrences)) +
  geom_smooth(data = binary.d, aes(x = pmi, y = singleMatch),
    method = "glm", method.args = list(family = "binomial"), 
    se = FALSE) +
  theme_few()
```  

### Restrict to words with more occurences & common POS

```{r}
restricted_distinctiveness_d <- distinctiveness_d %>% 
   filter(POS %in% c('noun', 'preposition', 'det', 'adjective', 'verb'))%>%
   filter(num_occurrences > 1)
                
restricted_binary.d <- restricted_distinctiveness_d %>% 
  mutate(numOnes = match * num_occurrences,
         numZeros = num_occurrences - numOnes) %>%
  do(rbind(data.frame(.[rep(1:nrow(.), .$numOnes),]) %>% mutate(singleMatch = 1),
           data.frame(.[rep(1:nrow(.), .$numZeros),]) %>% mutate(singleMatch = 0))) 

summary(glm(singleMatch ~ pmi, data=restricted_binary.d, family = 'binomial'))

ggplot(restricted_distinctiveness_d, aes(x = pmi, y = match)) +
  geom_point(aes(size = num_occurrences)) +
  geom_point(aes(x = pmi, y = match), color = 'red', data = subset(distinctiveness_d, bunny == TRUE)) +
  geom_point(aes(x = pmi, y = match), stroke = 3, color = 'yellow', data = subset(distinctiveness_d, a_match == TRUE)) +
  #geom_smooth(method = 'loess', span = .8) +
  geom_smooth(data = restricted_binary.d, aes(x = pmi, y = singleMatch),
    method = "glm", method.args = list(family = "binomial"), 
    se = FALSE) +
  theme_few() +
  scale_colour_manual(values=cbbPalette)+
  guides(color=FALSE)
```

Look at pmi across POS...

```{r}
pos_d <- read.csv("sequential_matchAndPMI.csv", header = TRUE) %>%
  filter(pmi > 0) %>%
  mutate(POS = as.character(POS)) %>%
  mutate(POS = ifelse(POS %in% c('NN', 'NNS', 'NNP', 'NNPS'), "noun", POS)) %>%
  mutate(POS = ifelse(POS %in% c('MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG'), "verb", POS)) %>%
  mutate(POS = ifelse(POS %in% c('DT', 'WDT'), 'det', POS)) %>%
  mutate(POS = ifelse(POS %in% c('PRP', 'PRP$', 'WP', 'WP$'), 'pronoun', POS)) %>%
  mutate(POS = ifelse(POS %in% c('CC'), 'conjunction', POS)) %>%
  mutate(POS = ifelse(POS %in% c('JJ', 'JJR', 'JJS'), 'adjective', POS)) %>%
  mutate(POS = ifelse(POS == 'IN', 'preposition', POS)) %>%
  mutate(POS = ifelse(POS %in% c('noun', 'verb', 'det', 'pronoun', 'conjunction', 'adjective', 'preposition'), POS, 'other')) %>%
  group_by(POS) %>%
  summarize(se = sd(pmi)/sqrt(length(pmi)),
            mean_pmi = mean(pmi),
            num = sum(total),
            mean_match = mean(match)) %>%
  filter(num > 200)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(pos_d %>% filter(POS != 'other'), aes(x = reorder(POS,mean_pmi,
                     function(x)-x), y = mean_pmi)) +
  geom_bar(stat='identity') +
  geom_errorbar(aes(ymax = mean_pmi + se, ymin = mean_pmi - se)) +
  theme_few() +
  xlab("part of speech") +
  ylab("pointwise mutual information")
```


Alternatively, can do a nonparametric analysis: draw a random word from each tangram/gameid pair and look at the percentage that match with round 6... This gives a null distribution. Then we can take the highest PMI word (or words) for each tangram/gameid pair and look at the percentage of *those* that match. We see that it's much higher than expected under the null.

```{r}
# TODO: get red to show up in legend
nonparametric_d = read.csv("PMIbootstrap.csv", header = TRUE) %>%
  mutate(PMI = factor(highest, levels = c('null', 'highest'), labels = c('random', 'top')))

highestValAvg = nonparametric_d %>% filter(highest == 'highest') %>% summarize(avg = mean(match))

nonparametric_d %>%
  group_by(sampleNum, PMI) %>%
  filter(PMI == 'random') %>%
  summarize(avgMatchRate = mean(match)) %>%
  ungroup() %>%
  ggplot(aes(x = avgMatchRate, fill = PMI)) +
  geom_histogram(binwidth = .0075) +
  geom_vline(aes(xintercept = highestValAvg), 
             color = 'red', linetype = "dashed", size = 2) +
  xlab("probability of match b/w Rounds 1 & 6") +
  scale_fill_manual(values = c("random" = "black", "top"= "red")) +
  theme_few()  +
  guides(fill=FALSE)
```

## Arbitrariness and stability

Calculate entropy under permutation tests (TODO: want to do a version of this restricted to extracted NPs, or, more simply, with stop words taken out. When we use distribution over *all* words, it could be driven by differences in frequency of function word use or something!)

```{r}
getCounts <- function(contents) {
  corpus <- Corpus(VectorSource(paste(contents, collapse = " ")))
  return(colSums(as.matrix(DocumentTermMatrix(corpus))))
}

tangramCombined %>%
    group_by(occurrenceNum, intendedObj) %>%
    mutate(permutation = sample(contents)) %>%
    group_by(gameid, intendedObj) %>%
    summarize(ent = entropy(getCounts(permutation))) %>%
    group_by(intendedObj) %>%
    summarize(meanEnt = mean(ent))

permutationTest <- function(df) {
  permuted = df %>%
    group_by(occurrenceNum, intendedObj) %>%
    mutate(permutation = sample(contents)) %>%
    group_by(gameid, intendedObj) %>%
    summarize(ent = entropy(getCounts(permutation))) %>%
    group_by(intendedObj) %>%
    summarize(meanEnt = mean(ent))
  return(permuted$meanEnt)
}

numPermutations = 1000
permutations <- replicate(numPermutations, {
  permutationTest(tangramCombined)
})

# Note that we explicitly do *not* normalize entropies, as this
# removes the contribution of a bigger vocabulary (as we would expect)
trueEntropy <- (tangramCombined %>%
  group_by(gameid, intendedObj) %>%
  summarize(trueEntropy = entropy(getCounts(contents))) %>%
  group_by(intendedObj) %>%
  summarize(meanEnt = mean(trueEntropy))
)

allEntropies <- cbind(trueEntropy, permutations) %>% 
  gather(permutationNum, ent, `1`:`1000`) %>%
  mutate(permutation = ifelse(permutationNum != 'trueEntropy', TRUE, FALSE))# %>%
  # group_by(permutation, intendedObj) %>%
  # mutate(meanEnt = mean(ent)) 

allEntropies %>%
  ggplot(aes(x = ent)) +
    geom_histogram(binwidth = .01) +
    geom_vline(aes(xintercept = meanEnt), color = 'red') +
    theme_few(9) +
    facet_wrap( ~ intendedObj, scale = "free_y") +
    xlab('entropy') +
    theme(aspect.ratio = .5) 
```

```{r}
summary(lmer(ent ~ permutation + (1 + permutation | gameid) + (1 +permutation | intendedObj), data = allEntropies))
```

```{r}
posTags <- read_csv('sequential_posTagged.csv')

clauseReduction = posTags %>%
  group_by(occurrenceNum, intendedObj) %>%
  #mutate(num_sbar=as.logical(num_sbar)) %>%
  summarize(m = mean(num_sbar > 0)) %>%
  #multi_boot_standard('num_sbar') %>%
  mutate(measure = '% messages \nw/ subordinate clauses')

ppReduction = posTags %>%
  group_by(occurrenceNum, intendedObj) %>%
  #mutate(num_pp=as.logical(num_pp)) %>%
  summarize(m = mean(num_pp > 0)) %>%
  #multi_boot_standard('acl') %>%
  mutate(measure = '% messages \nw/ adjectival clauses')

ccReduction = posTags %>%
  group_by(occurrenceNum, intendedObj) %>%
  #mutate(num_pp=as.logical(num_pp)) %>%
  summarize(m = mean(num_cc > 0)) %>%
  #multi_boot_standard('acl') %>%
  mutate(measure = '% messages \nw/ conjunctions')

listenerMsgs <- tangramCombined %>%
  group_by(gameid, occurrenceNum, intendedObj, role) %>%
  summarize(individualM = n()) %>%
  ungroup() %>%
  complete(role, occurrenceNum, intendedObj, gameid, fill = list(individualM = 0)) %>%
  spread(role, individualM)

listenerReduction <- listenerMsgs %>%
   group_by(occurrenceNum, intendedObj) %>%
  summarize(m = mean(matcher)) %>%
   #multi_boot_standard("matcher") %>%
   mutate(measure = '# listener messages')  

listenerReduction %>%
  rbind(lengthReduction %>% rename(intendedObj = intendedName)) %>%
  rbind(clauseReduction) %>%
  rbind(ppReduction) %>%
  rbind(ccReduction) %>%
  mutate(measure = factor(measure, levels = c(
    '# words per tangram', "# listener messages",
    '% messages \nw/ adjectival clauses',"% messages \nw/ subordinate clauses", '% messages \nw/ conjunctions'))) %>%
  ggplot(aes(x = occurrenceNum, y = m, group = intendedObj)) +
    geom_line() +
    xlab('round #') +
    ylab('metric') +
    geom_dl(aes(label = intendedObj), method = list(cex = .5, dl.trans(x = x - .25), "first.points")) +
    #geom_errorbar(aes(ymax = summary_ci_upper, ymin = summary_ci_lower), width = .1) +
    facet_wrap(~ measure,  scales = 'free') +
    xlim(-.01,6) +
    ylim(c(0, NA)) +
    theme_few(9) +
    theme(aspect.ratio = .75)
```

## Supplemental: look at wordclouds!

```{r}
library(wordcloud)   

# oldGrams = read.csv("handTagged.csv", quote = '"') %>%
  # mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  # mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  # do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
  #                                      separate = F))) %>%
  # mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  # filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

textPerGram = tangramCombined %>%
  filter(occurrenceNum == 6) %>%
  group_by(intendedObj) %>%
  # summarize(a = paste(contents, collapse = " ")) %>%
  summarize(text = paste(contents, collapse = " ")) %>%
  rename(docs = intendedObj) %>%
  mutate(docs = paste("doc ", docs))

corpus = Corpus(VectorSource(textPerGram$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('english'))

dtm = DocumentTermMatrix(corpus)

numDocs = dim(dtm)[1]
numTerms = dim(dtm)[2]
  
for(i in 1:numDocs) {
  png(paste("wordcloudForTangram", i, ".png", sep = ""), bg = "transparent")
  freq <- sort(colSums(as.matrix(dtm[i,])), decreasing=TRUE)
  # print(entropy(freq))
   wordcloud(names(freq), freq, min.freq = 1, colors=brewer.pal(6, "Dark2"))   
  dev.off()
}
```

## Supplemental: across-pair entropy and within-pair entropy

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  group_by(gameid, tangramRef) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangramRef) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangramRef != 0) %>%
  filter(tangramRef != 'None') %>%
  filter(tangramRef != '*') %>%
  group_by(tangramRef, roundNum) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = "tangramRef") %>%
  gather(type, entropy, acrossEnt, withinEnt)

ggplot(acrossPair, aes(x = roundNum, y = entropy, 
                       color = type, linetype = tangramRef)) +
  geom_line()
```

Or we could look at both on each half? 

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(gameid, tangram, half) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangram, half) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(tangram, half) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = c("tangram", "half")) %>%
  gather(type, entropy, acrossEnt, withinEnt)

acrossPair

ggplot(acrossPair,
       aes(x = half, y = entropy, 
           color = tangram, linetype = type, group = interaction(tangram, type))) +
  geom_line()
```
