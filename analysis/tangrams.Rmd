---
title: "tangramsReference"
output: 
  html_document:
    toc: true
    toc_depth: 2

---

# Import data

```{r}
library(tidyverse)
library(lme4)
library(lmerTest)
library(tm)
library(tidyboot)
library(xtable)
library(ggthemes)
library(entropy)
library(useful)
library(viridis)
library(gganimate)

source('./utils/analysis_helpers.R')
```

# Dynamics of content

```{r}
d <- read_csv('../data/tangramsSequential_collapsed.csv')
```

## semantic similarity analyses

```{r}
source('./utils/helpers.R')
library(broom)
detach('package:lmerTest')
M_mat = read_csv('./outputs/meta_tangrams_embeddings.csv') %>%
  #select(gameID, target, repetition, trial_num, feature_ind) %>%
  mutate(feature_ind = X1 + 1,
         gameID = gameid,
         target = intendedName,
         #trial_num = as.numeric(trial_num),
         repetition = as.numeric(repetitionNum)) %>% # Have to correct for R 1-indexing...
  arrange(gameid, intendedName, repetitionNum) 
F_mat = as.matrix(read_delim('outputs/feats_tangrams_embeddings.txt',
                                   delim=',', col_names=F))
colnames(F_mat) <- NULL
```

## do tangrams cluster?

```{r}
M_mat %>%
  group_by(target, gameid) %>%
  do(flatten_sim_matrix(get_sim_matrix(., F_mat, method = 'euclidean'),
                          .$repetition))  %>%
  group_by(target) 
```

```{r}
true_lmer.drift <- make_within_df(M_mat, F_mat, 'euclidean') %>% filter(rep1 == 1) %>% ungroup()
true_lmer.drift.out <- lmer(sim ~ poly(rep2,2) + (1  | gameID) + (1 | target), data = true_lmer.drift)
compute_within_drift(M_mat, F_mat, 'empirical', method = 'euclidean', nboot = 1000) %>%
  ggplot(aes(x = `rep diff`, y = empirical_stat, 
               fill = sample_id, color = sample_id, group = sample_id)) +
      geom_point(size = 2) +
      geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
      geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
      scale_x_discrete(labels = c('(1,2)','(1,3)', '(1,4)', '(1,5)', '(1,6)' ,'(1,7)','(1,8)')) +
      #ylim(.5, .8) +
      ylab('correlation') +
      ggtitle('drift from initial') +
      theme_few() +
      xlab('repetition pair') +
      guides(color = F, fill = F) +
      theme(legend.position = c(0.5, 0.8), text = element_text(size=18), 
            element_line(size=1), element_rect(size=2, color="#00000"))
summary(true_lmer.drift.out)
```

## Within-interaction convergence

```{r}
true_lmer.within <- make_within_df(M_mat, F_mat, 'euclidean') %>% filter(rep2 == rep1 + 1) %>% ungroup()
summary(lmer(sim ~ poly(rep1,2) + (1  | gameID) + (1 | target), data = true_lmer.within))
compute_within_convergence(M_mat, F_mat, 'empirical', method = 'euclidean', nboot = 1000) %>%
  ggplot(aes(x = `rep diff`, y = empirical_stat, fill = sample_id, color = sample_id, group = sample_id)) +
      geom_line(size = 2) +
      geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
      #geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
      scale_x_discrete(labels = c('(1,2)','(2,3)', '(3,4)', '(4,5)', '(5,6)')) +
      #ylim(.5, 1) +
      ylab('correlation') +
      ggtitle('stability') +
      theme_few() +
      xlab('repetition pair') +
      guides(color = F, fill = F) +
      theme(legend.position = c(0.5, 0.8), text = element_text(size=18), 
            element_line(size=1), element_rect(size=2, color="#00000"))

```

## Between-interaction divergence 


```{r}
M_mat %>%
  group_by(target, repetition) %>%
  do(flatten_sim_matrix(get_sim_matrix(., F_mat, method = 'cor'),
                        .$gameID)) %>%
  unite(col = 'gamepair', dim1, dim2)

compute_across_similarity(M_mat, F_mat,   'raw_cor_nonorm', method = 'euclidean', nboot = 1000) %>%
  ggplot(aes(x = repetition, y = empirical_stat, 
             fill = sample_id, color = sample_id, group = sample_id)) +
    geom_line(size = 2) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    #geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
    scale_x_continuous(breaks = c(1,2,3,4,5,6), labels = c(1,2,3,4,5,6)) +
    #ylim(.5, 1) +
    ylab('correlation') +
    ggtitle('across pairs') +
    theme_few() +
    xlab('repetition') +
    guides(color = F, fill = F) +
    theme(legend.position = c(0.5, 0.8), text = element_text(size=18), 
          element_line(size=1), element_rect(size=2, color="#00000"))
```

## tsne visualizations

```{r}
tsne <- read_csv('outputs/embeddings.csv') %>%
  left_join(read_csv('../data/tangramsSequential_collapsed.csv') ) %>%
  select(gameid, intendedName, repetitionNum, x_tsne, y_tsne, x_mds, y_mds, contents) %>%
  mutate(r = useful::cart2pol(x_tsne, y_tsne)$r,
         theta = useful::cart2pol(x_tsne, y_tsne)$theta) %>%
  group_by(gameid, intendedName) %>%
  arrange(repetitionNum) %>%
  mutate(finalTheta = last(theta)) 

```

```{r}
ggplot(tsne %>% filter(repetitionNum %in% c(6)), aes(x = x_tsne, y = y_tsne)) +
  #geom_line(alpha = 0.2) +
  geom_point(aes(#shape = factor(-repetitionNum),
                 color = finalTheta,
                 ),
             size = 1.5, stroke = 2
             ) +
  facet_wrap(~ intendedName) +
  theme_few(20) +
  ggtitle('pca + tsne embeddings') +
  scale_shape_manual(values = c(21)) +
  scale_alpha_continuous(range = c(0.5, 1))+
  scale_color_gradientn(colours = viridis(5))+
    theme(axis.title=element_blank(),
      axis.text=element_blank(),
      axis.ticks=element_blank()) +
  labs(x = "", y = "") +
  guides(color = F, shape = F, alpha = F) +
  theme(aspect.ratio = 1)  #+

ggsave(filename = '../writing/tangrams/figs/tsne-animation_final.pdf')
```

```{r}
ggplot(tsne, aes(x = x_mds, y = y_mds, group = gameid, frame = paste0('repetition: ', repetitionNum)) +
  geom_line(alpha = 0.2) +
  geom_point(aes(shape = factor(-repetitionNum), 
                 color = finalTheta, 
                 alpha = factor(repetitionNum)),
             size = 1.5, stroke = 2,
             data = tsne %>% filter(repetitionNum %in% c(1,6))) +
  facet_wrap(~ intendedName) +
  theme_few(20) +
  ggtitle('mds embeddings') +
  scale_shape_manual(values = c(16, 21)) +
  scale_alpha_discrete(range = c(0.5, 1))+
  scale_color_gradientn(colours = tableau_color_pal()(3))+
  guides(color = F, shape = F, alpha = F) +
  theme(aspect.ratio = 1) 

ggsave('../writing/tangrams/figs/mds_embeddings.pdf', width = 15, height = 15)
```


## How much is within-game similarity disrupted by scrambling utterances across games?

Calculate entropy under permutation tests

Note that we explicitly do *not* normalize entropies, as this
removes the contribution of a bigger vocabulary (as we would expect)

```{r}
getCounts <- function(contents) {
  corpus <- Corpus(VectorSource(paste(contents, collapse = " ")))
  return(colSums(as.matrix(DocumentTermMatrix(corpus))))
}

permutationTest <- function(d, sample_id) {
  # scramble across games within each repetition/tangram
  permuted = d %>%
    group_by(repetitionNum, intendedName) %>%
    mutate(permutation = sample(contents)) %>%
    group_by(gameid, intendedName) %>%
    summarize(ent = entropy(getCounts(permutation))) %>%
    group_by(intendedName) %>%
    summarize(meanEnt = mean(ent)) %>%
    mutate(sample_id = sample_id)
  return(permuted)
}

permutations <- map_dfr(seq_len(1000), ~permutationTest(d, .x))

trueEntropy <- (tangramCombined %>%
  group_by(gameid, intendedName) %>%
  summarize(trueEntropy = entropy(getCounts(contents))) %>%
  group_by(intendedName) %>%
  summarize(meanEnt = mean(trueEntropy))
)


```

Plotted 

```{r}
ggplot(rbind(permutations, trueEntropy), aes(x = 0, y = permuted)) +
  geom_violin(fill = '#d07e93', alpha = 0.25, color = NA) +
  geom_point(aes(x = 0, y = true), size = 4, color = "#163c4e") +
  theme_bw() +
  ylab('entropy of word distribution') +
  coord_flip() +
  scale_x_continuous(breaks = NULL) +
  theme(aspect.ratio = 1/5)
```

```{r}
summary(lmer(ent ~ permutation + (1 + permutation | gameid) + (1 + permutation | intendedName), 
             data = allEntropies))
```


## What determines whether a label gets conventionalized?

Scatter plot (all datapoints)

TODO: glm is a little weird since it breaks out match % by num occurrences but still aggregates pmi -- should it entirely be at granularity of participant (i.e. gameid, word, pmi, match (T/F) so we can do glmer?) granted, that might be pretty noisy and hard to estimate pmi of individual words per participant... 

```{r}
distinctiveness_d <- read.csv("outputs/sequential_matchAndPMI.csv", header = TRUE) %>%
  filter(pmi > 0) %>%
  rename(num_occurrences = total) %>%
  filter(num_occurrences > 1) %>%
  #filter(POS == "NN") %>%
  mutate(bunny = word == "bunny") %>%
  mutate(a_match = word == "a")
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# Split out into binary values:
binary.d <- distinctiveness_d %>% 
  mutate(numOnes = match * num_occurrences,
         numZeros = num_occurrences - numOnes) %>%
  do(rbind(data.frame(.[rep(1:nrow(.), .$numOnes),]) %>% mutate(singleMatch = 1),
           data.frame(.[rep(1:nrow(.), .$numZeros),]) %>% mutate(singleMatch = 0))) 

ggplot(distinctiveness_d %>% 
         filter(match > 0) %>%
         filter(match < 1),
         aes(x = pmi, y = log(match))) +
  geom_point(aes(size = num_occurrences), color = 'white') +
  geom_smooth(method = "lm") +
  scale_y_continuous() +#continuous(trans = 'log1p') +
  # geom_smooth(data = binary.d, aes(x = pmi, y = singleMatch),
  #   method = "glm", method.args = list(family = "binomial"),
  #   se = TRUE) +
  ylab('log P(match)') +
  xlab('pointwise mutual information on first repetition') +
  mytheme(24) +
  theme(aspect.ratio = 1) +
  guides(size = FALSE)

ggsave("../writing/tangrams/figs/PMI_by_match.png", bg = 'transparent')
```  

```{r}
distinctiveness_d %>% 
         filter(match > 0) %>%
         filter(match < 1) %>%
         summarize(r = cor(match, pmi))
summary(lm(match ~ pmi + num_occurrences, data=distinctiveness_d))
```

### Look at pmi across POS...

```{r}
pos_d <- read.csv("sequential_matchAndPMI.csv", header = TRUE) %>%
  filter(pmi > 0) %>%
  mutate(POS = as.character(POS)) %>%
  mutate(POS = ifelse(POS %in% c('NN', 'NNS', 'NNP', 'NNPS'), "noun", POS)) %>%
  mutate(POS = ifelse(POS %in% c('MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG'), "verb", POS)) %>%
  mutate(POS = ifelse(POS %in% c('DT', 'WDT'), 'det', POS)) %>%
  mutate(POS = ifelse(POS %in% c('PRP', 'PRP$', 'WP', 'WP$'), 'pronoun', POS)) %>%
  mutate(POS = ifelse(POS %in% c('CC'), 'conjunction', POS)) %>%
  mutate(POS = ifelse(POS %in% c('JJ', 'JJR', 'JJS'), 'adjective', POS)) %>%
  mutate(POS = ifelse(POS == 'IN', 'preposition', POS)) %>%
  mutate(POS = ifelse(POS %in% c('noun', 'verb', 'det', 'pronoun', 'conjunction', 'adjective', 'preposition'), POS, 'other')) %>%
  group_by(POS) %>%
  summarize(se = sd(pmi)/sqrt(length(pmi)),
            mean_pmi = mean(pmi),
            num = sum(total),
            mean_match = mean(match)) %>%
  filter(num > 200)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(pos_d %>% filter(POS != 'other'), aes(x = reorder(POS,mean_pmi,
                     function(x)-x), y = mean_pmi)) +
  geom_bar(stat='identity') +
  geom_errorbar(aes(ymax = mean_pmi + se, ymin = mean_pmi - se)) +
  theme_few() +
  xlab("part of speech") +
  ylab("pointwise mutual information")
```


Alternatively, can do a nonparametric analysis: draw a random word from each tangram/gameid pair and look at the percentage that match with round 6... This gives a null distribution. Then we can take the highest PMI word (or words) for each tangram/gameid pair and look at the percentage of *those* that match. We see that it's much higher than expected under the null.

```{r}
# TODO: get red to show up in legend
nonparametric_d = read.csv("outputs/PMIbootstrap.csv", header = TRUE) %>%
  mutate(PMI = factor(highest, levels = c('null', 'highest'), labels = c('random', 'top')))

highestValAvg = (nonparametric_d %>% filter(highest == 'highest') %>% summarize(avg = mean(match)))$avg

nonparametric_d %>%
  group_by(sampleNum, PMI) %>%
  filter(PMI == 'random') %>%
  summarize(avgMatchRate = mean(match)) %>%
  ungroup() %>%
  ggplot(aes(x = avgMatchRate, fill = PMI)) +
  geom_histogram(binwidth = .0075) +
  geom_vline(aes(xintercept = highestValAvg), 
             color = 'red', linetype = "dashed", size = 2) +
  xlab("probability of match b/w Rounds 1 & 6") +
  scale_fill_manual(values = c("random" = "white", "top"= "red")) +
  guides(fill=FALSE) +
  mytheme(24) +
  theme(aspect.ratio = 1)

ggsave('../writing/tangrams/figs/bootstrappedPMI.png',
       bg = 'transparent')
```

# Dynamics of structure/syntax

We've already done most of the pre-processing work in preprocessing.Rmd and the jupyter notebook, so here we just read in the csv files we created there, do a bit of post-processing, and make plots.

## What are the unigrams/bigrams/trigrams that drop the most in frequency across whole data set?

```{r}
getMostReduced('unigrams')
```

```{r}
getMostReduced('bigrams')
```

```{r}
getMostReduced('trigrams')
```

Make little table to put in supplemental?

```{r}
makeNGramTable(10)
```

Seems to be markers of figurative descriptions, determiners, and modifying phrases like 'on the left'?

## How do various properties reduce over time?

### Reduction in utterance length

```{r, cache = T, warning=F}
tagged_df <- read_csv('./outputs/posTagged_tangramsSequential_collapsed.csv', 
                      col_types = 'ciicciiiiiiiccii') %>%
  #filter(role == 'director') %>%
  #select(-text, -contents, -posCounts, -pos) #%>%
  left_join(read_csv('./outputs/constituency_tags.csv', col_names = T , col_types = '-cicccccc') %>%
      replace_na(list(SBAR = FALSE, PP = FALSE, CC = FALSE, NP = FALSE)) %>%
      mutate(SBAR = ifelse(SBAR == 'True', TRUE, FALSE),
           PP = ifelse(PP == 'True', TRUE, FALSE),
           NP = ifelse(NP == 'True', TRUE, FALSE),
           CC = ifelse(CC == 'True', TRUE, FALSE)))
```

```{r}
tagged_df %>%
   # group_by(gameid, repetitionNum) %>%#, taskVersion
   # summarize(individualM = sum(numWords)/12) %>%
   group_by(repetitionNum) %>% #taskVersion
   tidyboot_mean(numWords, na.rm = T) %>%
   mutate(measure = '# words per tangram') %>%
   ggplot(aes(x = repetitionNum, y = empirical_stat)) + #, color = taskVersion
    geom_ribbon(aes(ymax = ci_upper, ymin = ci_lower), fill = 'gray50') +
    geom_line(color = 'white') +
    xlab('repetition #') +
    ylab('mean # words used \n per tangram') +
    #facet_wrap(~ taskVersion, nrow = 1, ncol = 2) +
    scale_x_continuous(breaks = round(seq(1, 6, by = 1),1)) +
    ylim(c(0, NA)) +
    mytheme(24) 
ggsave('../writing/tangrams/figs/wordReduction.png', bg = 'transparent')
```

Statistics:

```{r}
summary(lmer(log(numRawWords) ~ poly(repetitionNum,2) + 
               (1 + repetitionNum | gameid) + 
               (1 + repetitionNum | intendedName), 
             data = tagged_df %>% 
               #filter(taskVersion == 'cued') %>%
               group_by(gameid, trialNum, repetitionNum, intendedName) %>% 
               summarize(numRawWords = sum(numWords))))
```

### Reduction in POS features 

examine reduction in different parts of speech, i.e. to argue that it's not just small-talk going down or something. 

```{r}
cum_pos.d <- tagged_df %>%
  group_by(repetitionNum) %>%
  summarize(numWords = sum(numWords),
            numMessages = length(gameid),
            nouns = sum(NOUNcount),
            verbs = sum(VERBcount),
            dets= sum(DETcount),
            pronouns = sum(PRONcount),
            preps = sum(ADPcount),
            adverbs = sum(ADVcount),
            conjunctions = sum(CCONJcount),
            adjectives = sum(ADJcount)) %>%
  mutate(OTHER = (numWords - nouns - verbs - dets - pronouns -
                      preps - adjectives - conjunctions - adverbs)) %>%
  gather(POS, total, nouns:OTHER) %>%
  mutate(total = total/numMessages) %>% # normalize to # / message
  mutate(POS = factor(POS, levels = c('nouns', 'verbs',  'preps', 'dets', 
                                      'adjectives', 'adverbs', 'pronouns', 'conjunctions', 'OTHER'))) %>%
  select(repetitionNum, POS, total) 

ggplot(cum_pos.d, aes(x = repetitionNum, y = total, fill = POS)) +
    geom_area(alpha=0.6 , size=.5, colour="white") +
    geom_text(data=cum_pos.d %>% 
                filter(repetitionNum == 1) %>%
                arrange(POS) %>%
                mutate(cum = rev(cumsum(rev(total)))), 
              aes(x=1, y=cum-.5, label=POS),
              hjust = 0, color = 'white', size=7) +
    scale_fill_brewer(palette = "Set1") +
    guides(fill = FALSE) +
    mytheme(24)

ggsave('../writing/tangrams/figs/wordReduction_by_POS.png', bg = 'transparent')
```


```{r}
tagged_df %>%
  group_by(repetitionNum) %>%
  summarize(numWords = sum(numWords),
            nouns = sum(NOUNcount)/sum(numWords),
            verbs = sum(VERBcount)/sum(numWords),
            dets= sum(DETcount)/sum(numWords),
            pronouns = sum(PRONcount)/sum(numWords),
            preps = sum(ADPcount)/sum(numWords),
            adverbs = sum(ADVcount)/sum(numWords),
            conjunctions = sum(CCONJcount)/sum(numWords),
            adjectives = sum(ADJcount)/sum(numWords)) %>%
  mutate(OTHER = (1 - nouns - verbs - dets - pronouns -
                      preps - adjectives - conjunctions - adverbs)) %>%
  gather(POS, prop, nouns:OTHER) %>%
  mutate(POS = factor(POS, levels = c('nouns',  'dets', 'verbs', 'adjectives', 'adverbs', 
                              'preps', 'pronouns', 'conjunctions', 'OTHER'))) %>%

  select(repetitionNum, POS, prop) %>%
  ggplot(aes(x = repetitionNum, y = prop, fill = POS)) +
    geom_area(alpha=0.6 , size=1, colour="black") +
    scale_fill_brewer(palette = "Set1") +
    theme_bw()
```

TODO: distance in depth of the tree between the nouns

TODO: consider only showing full curve like this for ones we want to highlight (e.g. nouns vs. verbs vs. adjectives or something?), and a supplemental bar plot version with % decrease in appearance of each POS (like in the cogsci paper).

```{r}
posReduction <- tagged_df %>%
  group_by(gameid, repetitionNum) %>%
  summarize(numWords = sum(numWords),
            numMessages = length(gameid),
            nouns = sum(NOUNcount),
            verbs = sum(VERBcount),
            dets= sum(DETcount),
            pronouns = sum(PRONcount),
            preps = sum(ADPcount),
            adverbs = sum(ADVcount),
            conjunctions = sum(CCONJcount),
            adjectives = sum(ADJcount)) %>%
  gather(POS, count, nouns:adjectives) %>%
  select(gameid, repetitionNum, POS, count) %>%
  # Need to put in ids to spread
  rowwise() %>%
  mutate(id = row_number()) %>%
  mutate(repetitionNum = paste0('rep', repetitionNum, collapse = '')) %>%
  spread(repetitionNum, count) %>%
  # Compute % reduction from first to last round
  mutate(diffPct = (rep1 - rep6)/rep1) %>%
  group_by(POS) %>%
  # Filter out handful of people who skipped first round w/ negative %...
  filter(diffPct >= 0) %>%
  filter(POS != "OTHER") %>%
  group_by(POS) %>%
  # Take mean & se over participants
  tidyboot_mean(column = diffPct, na.rm = T) %>%
  mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps', 'conjunctions'), 'closed', 
                      ifelse(POS == 'adverbs', '?', 'open'))) %>%
  mutate(cat = factor(cat, levels = c('closed', '?', 'open'))) %>%
  # rearrange
  transform(POS=reorder(POS, mean))

detReductionRate <- (posReduction %>% filter(POS == 'dets'))$mean * 100
nounReductionRate <- (posReduction %>% filter(POS == 'nouns'))$mean * 100

ggplot(posReduction, aes(x = POS, y = mean, fill = cat)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1,
                color = 'white')+
  ylab("% reduction") +
  xlab("Part of Speech category")  +
  mytheme(24) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values=c('tomato3', 'gray', 'orange'))


ggsave('../writing/tangrams/figs/posReduction.png', bg = 'transparent')
```

### Reduction in syntactic features 

TODO: include % including NPs? (shouldn't go down as much)

```{r}
tagged_df %>%
  #filter(taskVersion == 'cued') %>%
  select(gameid, repetitionNum, SBAR, CC, PP, NP) %>% #, taskVersion
  gather(measure, occurrence, SBAR:NP) %>%
  group_by(repetitionNum, gameid, measure) %>%#taskVersion, 
  summarize(m = mean(occurrence)) %>%
  group_by(repetitionNum, measure) %>%#taskVersion, 
  tidyboot_mean(m, na.rm = T) %>%
  filter(measure != "NP") %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat)) +
    geom_ribbon(aes(ymax = ci_upper, ymin = ci_lower), width = .1,
                  color = 'grey50') +
    geom_line(color = 'white') +
    xlab('repetition #') +
    ylab('% of messages \n with syntactic feature') +
    facet_grid(. ~ measure) +#taskVersion
    ylim(c(0, NA)) +
    mytheme(24) +
    scale_x_continuous(breaks = round(seq(1, 6, by = 1),1)) +
    theme(aspect.ratio = .75)

ggsave('../writing/tangrams/figs/syntacticReduction.pdf', width = 8, height = 3, bg = 'transparent')
```


TODO: compare starting to ending proportions;

### Reduction in listener exchanges

```{r}
# /data/tangrams_sequential/message/sequential_message_filtered.csv'
# /data/tangramsSequential.csv'
message_df = read_csv('../data/tangramsSequential.csv') %>%
    #rename(repetitionNum = occurrenceNum, role = sender)  %>%
    mutate(numRawWords = str_count(contents, "\\S+")) 

listenerMsgs <- message_df %>%
  group_by(gameid, repetitionNum, role, intendedName) %>% #taskVersion
  summarize(individualM = n()) %>%
  ungroup() %>%
  complete(role, repetitionNum, gameid, intendedName, fill = list(individualM = 0)) %>%
  spread(role, individualM)

listenerReduction <- listenerMsgs %>%
   group_by(repetitionNum, gameid) %>%
   summarize(matcherTalks = sum(matcher) > 0) %>%
   group_by(repetitionNum) %>%
   tidyboot_mean(matcherTalks) %>%
   mutate(measure = '# listener messages')  
```

Plot it.

```{r}
ggplot(listenerReduction, aes(x = repetitionNum, y = mean*100)) +
  geom_ribbon(aes(ymax = 100*ci_upper, ymin = 100*ci_lower), fill = 'gray50') +
  geom_line(color = 'white') +
  ylab('% games where \n matcher sent message') +
  xlab('repetition #') +
  ylim(0,100) +
  scale_x_continuous(breaks = round(seq(1, 6, by = 1),1)) +
  mytheme(24)

ggsave('../writing/tangrams/figs/listenerResponses.png', bg = "transparent")

listenerMsg_lm = summary(glmer(matcherBin ~ repetitionNum + (1 + repetitionNum | gameid) + (1  + repetitionNum | intendedName), 
                               data = listenerMsgs %>% mutate(matcherBin = matcher > 0), 
                               family = 'binomial'))
print(listenerMsg_lm)
```

### Did pairs who talked more become more efficient?

```{r}
turnTaking <- listenerMsgs %>%
  filter(repetitionNum %in% c(1,2,3,4,5)) %>%
  group_by(gameid) %>% # taskVersion
  summarize(numListenerMsgs = sum(matcher)) %>%
  ungroup() %>%
  #filter(numListenerMsgs < mean(numListenerMsgs) + 3*sd(numListenerMsgs)) %>%
  select(gameid, numListenerMsgs)#taskVersion

efficiency <- message_df %>%
  filter(role == "director") %>%
  group_by(gameid, repetitionNum) %>%  #taskVersion
  summarize(individualM = sum(numRawWords, na.rm = T)) %>%
  rowwise() %>%
  mutate(id = row_number()) %>%
  mutate(repetitionNum = paste0('round', repetitionNum, collapse = '')) %>%
  spread(repetitionNum, individualM) %>%
  mutate(diffPct = (round1 - round6)/round1) %>%
  filter(diffPct >= 0) %>% # Filter out handful of people who skipped first round...
  select(gameid, diffPct, round1,round6) #taskVersion

qplot(log1p(efficiency$round6))
qplot(log1p(turnTaking$numListenerMsgs))
```

```{r}
turnTakingEfficiencyPlot <- ggplot(turnTaking %>% left_join(efficiency), 
                                   aes(x = log1p(numListenerMsgs), y = log1p(round6))) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_bw(9) +
  ylab("% reduction") +
  xlab("log # listener messages on 1st round")
turnTakingEfficiencyPlot
```

```{r}
cor.test(x = log1p((efficiency %>% left_join(turnTaking))$round6), log1p((efficiency %>% left_join(turnTaking))$numListenerMsgs))

summary(lmer(log1p(round6) ~ log1p(numListenerMsgs) + (1|gameid), data = efficiency %>% left_join(turnTaking)))

turnTakingdf <- turnTakingEfficiency_lm$df[2]
turnTakingCoefs <- turnTakingEfficiency_lm$coefficients[2,]
turnTakingResult <- paste0("b = ", round(turnTakingCoefs[1],2),
                           ", t(", turnTakingdf, ") = ", round(turnTakingCoefs[3],2), 
                           ", p = ", round(turnTakingCoefs[4],2))
turnTakingResult
turnTakingEfficiencyPlot
```

### Broken out by tangram

Broken out by tangrams for cued condition

```{r}
library(directlabels)

lengthReduction <- tagged_df %>%
   filter(taskVersion == "cued") %>%
   group_by(gameid, repetitionNum, intendedName) %>%
   summarize(individualM = sum(numRawWords)) %>%
   group_by(repetitionNum, intendedName) %>%
   tidyboot_mean(individualM) %>%
   mutate(measure = '# words per tangram')

ggplot(lengthReduction, aes(x = repetitionNum, y = empirical_stat ,group = intendedName)) +
  geom_line() +
  theme_few() +
  geom_dl(aes(label = intendedName), method = list(dl.trans(x = x - .1), "first.points")) +
  xlab("repetition #") +
  ylab("mean # words") +
  theme(aspect.ratio = 1)
  
ggsave('../writing/tangrams/figs/num_words_sequential.pdf')
```

Just noun broken out by tangram

```{r}
tagged_df %>% 
  filter(taskVersion == 'cued') %>%
  mutate(numNPWords = ifelse(noun_chunks == "[]", 0, str_count(noun_chunks, "\\S+"))) %>%
  group_by(gameid, repetitionNum, intendedName) %>% 
  summarize(NOUN = sum(NOUNcount)) %>%
  group_by(repetitionNum, intendedName) %>%
  tidyboot_mean(NOUN) %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat, group = intendedName)) +
    geom_line() +
    theme_few() +
    # geom_ribbon(aes(ymax = ci_upper, ymin = ci_lower), alpha = .2) +
    geom_dl(aes(label = intendedName), method = list(dl.trans(x = x - .1), "first.points")) +
    xlab("repetition #") +
    ylab("# nouns per description") +
    theme(aspect.ratio = 1)
```

Same thing broken out by tangram

```{r}
tagged_df %>%
  filter(taskVersion == 'cued') %>%
  select(gameid, intendedName, repetitionNum, SBAR, CC, PP) %>%
  gather(measure, occurrence, SBAR:PP) %>%
  group_by(repetitionNum, intendedName, gameid, measure) %>%
  summarize(m = mean(occurrence)) %>%
  group_by(intendedName, repetitionNum, measure) %>%
  tidyboot_mean(m) %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat, group = intendedName)) +
    geom_line() +
    xlab('round #') +
    ylab('% of messages containing syntactic feature') +
    #geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1) +
    facet_wrap(~ measure, nrow = 1, ncol = 3) +
    ylim(c(0, NA)) +
    theme_few(9) +
    theme(aspect.ratio = .75)
```

# Supplemental analyses

## Compare cleaned vs. uncleaned on basic measures

```{r}
read_csv('../data/tangramsSequential_nocleaning.csv') %>%
   group_by(gameid, repetitionNum) %>% #taskVersion
  filter(role != 'matcher') %>%
   summarize(numRawWords = sum(numRawWords)/12) %>%
    group_by(repetitionNum) %>%
   tidyboot_mean(numRawWords, na.rm = T) 
```

```{r}
read_csv('../data/tangramsSequential_collapsed.csv') %>%
   group_by(repetitionNum) %>% #taskVersion
  #  summarize(numRawWords = sum(numRawWords)/12) %>%
  # group_by(repetitionNum) %>%
   tidyboot_mean(numRawWords, na.rm = T) 
```

## What proportion of messages sent by director vs. matcher, respectively?

At beginning, directors send about 60% of total messages (close to equal!) At end, they send 80% -- listeners stop talking as much. This is just another way of looking at the total number of listener messages dropping.

```{r}
tangramCombined %>% 
  group_by(gameid, repetitionNum, role) %>% 
  summarize(individualM = n()) %>% 
  ungroup() %>%
  complete(role, repetitionNum, gameid, fill = list(individualM = 0)) %>% 
  spread(role, individualM) %>% 
  mutate(ratio = director / (director + matcher)) %>%
   group_by(repetitionNum) %>% 
   summarize(m = mean(ratio), 
             se = sd(ratio)/sqrt(length(ratio))) %>%
ggplot(aes(x = repetitionNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("% of total messages sent by director") +
  xlab("trials") +
  ylim(.5,1) +
  xlim(0, 7) +
  theme_bw() 
```

## Visualize word frequencies on final round...

```{r}
library(wordcloud)   

textPerGram = read_csv('../data/tangrams.csv') %>% 
  filter(role == "director" & taskVersion == 'cued') %>%
  filter(repetitionNum %in% c(1,6)) %>%
  group_by(repetitionNum, intendedName) %>%
  # summarize(a = paste(contents, collapse = " ")) %>%
  summarize(text = paste(contents, collapse = " ")) %>%
  rename(docs = intendedName) %>%
  mutate(docs = paste("doc ", docs))

corpus = Corpus(VectorSource(textPerGram$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
#corpus <- tm_map(corpus, removeWords, stopwords('english'))

dtm = DocumentTermMatrix(corpus)

numDocs = dim(dtm)[1]
numTerms = dim(dtm)[2]
  
for(i in 1:numDocs) {
  round = ifelse(floor((i-1) / 12) < 1, 'first', 'last')
  print(round)
  tangramNum = ((i-1) %% 12) + 1
  print(tangramNum)
  pdf(paste("../writing/tangrams/figs/wordclouds/wordcloudForTangram", tangramNum, "on", round ,"Round.pdf", sep = ""), 
      bg = "transparent")
  freq <- sort(colSums(as.matrix(dtm[i,])), decreasing=TRUE)
  # print(entropy(freq))
   wordcloud(names(freq), freq, min.freq = 1, colors=brewer.pal(6, "Dark2"))   
  dev.off()
}
```

## Slightly sketchy way of looking at arbitrariness by comparing across-pair and within-pair entropy

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  group_by(gameid, tangramRef) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangramRef) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangramRef != 0) %>%
  filter(tangramRef != 'None') %>%
  filter(tangramRef != '*') %>%
  group_by(tangramRef, repetitionNum) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = "tangramRef") %>%
  gather(type, entropy, acrossEnt, withinEnt)

ggplot(acrossPair, aes(x = repetitionNum, y = entropy, 
                       color = type, linetype = tangramRef)) +
  geom_line()
```

Or we could look at both on each half? 

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(repetitionNum <= 3, "beg", "end"))) %>%
  group_by(gameid, tangram, half) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangram, half) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(repetitionNum <= 3, "beg", "end"))) %>%
  group_by(tangram, half) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = c("tangram", "half")) %>%
  gather(type, entropy, acrossEnt, withinEnt)

acrossPair

ggplot(acrossPair,
       aes(x = half, y = entropy, 
           color = tangram, linetype = type, group = interaction(tangram, type))) +
  geom_line()
```

## Look at raw number of POS over time

Advantage to this is that you can see error bars...

Disadvantage is that it doesn't really help you see how the *proportions* change over time... i.e. what portion of the total these are...

```{r}
tagged_df %>% 
  #filter(taskVersion == 'cued') %>%
  mutate(numNPWords = ifelse(noun_chunks == "[]", 0, str_count(noun_chunks, "\\S+"))) %>%
  group_by(gameid, repetitionNum) %>% 
  summarize(NOUN = sum(NOUNcount),
            PRON = sum(PRONcount),
            ADJ = sum(ADJcount),
            DET = sum(DETcount),
            VERB = sum(VERBcount)) %>%
  gather(POS, count, NOUN, ADJ, DET, VERB, PRON) %>%
  mutate(count = count / 12) %>%
  group_by(repetitionNum, POS) %>%
  tidyboot_mean(count) %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat, color = POS)) +
    geom_line() +
    theme_few() +
    geom_ribbon(aes(ymax = ci_upper, ymin = ci_lower), alpha = .2) +
    #geom_dl(aes(label = intendedName), method = list(dl.trans(x = x - .1), "first.points")) +
    #facet_wrap(~ taskVersion) +
    xlab("repetition #") +
    ylab("# words in POS category per tangram") +
    theme(aspect.ratio = 1)

ggsave('../writing/tangrams/figs/POS_reduction.pdf', width = 5, height = 4)
```

## tsne animation

```{r}
p <- ggplot(tsne %>% filter(repetitionNum %in% c(1,6)) %>% group_by(gameid, intendedName) %>% filter(length(gameid) == 2), aes(x = x_tsne, y = y_tsne, group = interaction(gameid,intendedName))) +
  #geom_line(alpha = 0.2) +
  geom_point(aes(#shape = factor(-repetitionNum),
                 color = finalTheta,
                 alpha = 1
    ),
             size = 1.5, stroke = 2
             ) +
  facet_wrap(~ intendedName) +
  theme_few(20) +
  ggtitle('pca + tsne embeddings') +
  scale_shape_manual(values = c(21)) +
  scale_alpha_continuous(range = c(0.5, 1))+
  scale_color_gradientn(colours = viridis(5))+
  guides(color = F, shape = F, alpha = F) +
  theme(aspect.ratio = 1)  +
  labs(title = "Rep. {floor(frame_time)}", x = "", y = "") +
  theme(axis.title=element_blank(),
      axis.text=element_blank(),
      axis.ticks=element_blank()) +
  transition_time(as.numeric(repetitionNum)) # speed it up!

   
animate(p, nframes = 50)
anim_save(filename = '../writing/tangrams/figs/tsne-animation.gif')
```
