---
title: "tangramsReference"
output: 
  html_document:
    toc: true
    toc_depth: 2

---

# Import data

```{r}
library(ggplot2)
library(lme4)
library(lmerTest)
library(tidyr)
library(dplyr)
library(tm)
library(stringr)
library(knitr)
library(NLP)
library(readr)
library(langcog)
library(xtable)
library(ggthemes)
library(entropy)
```

We've already done most of the work using nltk in the ipython notebook, so here we just read in the csv files we created there, do a bit of post-processing, and make plots.

# Exp. 1: unconstrained

## Import and pre-process data

```{r}
tangramMsgs = read_csv("../data/tangrams_unconstrained/message/rawUnconstrainedMessages.csv") %>%
  rename(msgTime = time, role = sender)

tangramSubjInfo = read.csv("../data/tangrams_unconstrained/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

rawTangramCombined <- tangramMsgs %>% left_join(tangramSubjInfo, by = c('gameid', 'role'))
```

Set exclusion criteria

```{r}
nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- unique((rawTangramCombined %>% group_by(gameid) %>% 
                           filter(length(unique(roundNum)) != 6))$gameid)
badGames <- union(c('0574-6', incompleteIDs), nonNativeSpeakerIDs)
print(paste0('excluding ', length(badGames), ' of ', length(unique(rawTangramCombined$gameid)), ' games that were either incomplete or had non-native english speakers'))
```

Count numbers of words, examine size of dataset

```{r}
tangramCombined <- rawTangramCombined %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(numRawWords = str_count(contents, "\\S+")) %>%
  filter(!is.na(numRawWords)) # filter out pure punctuation messages
  #filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

write_csv(tangramCombined, '../data/tangramsUnconstrained.csv')

numGames <- length(unique(tangramCombined$gameid))
numUtterances <- length(tangramCombined$contents)
print(paste0(numUtterances, ' utterances in ', numGames, ' games'))
```

## What are the unigrams/bigrams that drop the most in frequency?

```{r}
unigrams <- read_csv("wordCounts.csv", col_names = T) %>%
  group_by(word, POS, roundNum) %>%
  summarize(count = sum(count)) %>%
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>%
  arrange(desc(diffSize)) %>%
  select(word) %>%
  filter(word != ',') %>%
  filter(word != '#') %>%
  head(n = 10)

bigrams <- read_csv("bigramCounts.csv", col_names = T) %>%
  group_by(word, roundNum) %>%
  summarize(count = sum(count)) %>%
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>%
  arrange(desc(diffSize)) %>%
  select(word) %>%
  rename(bigrams = word) %>%
  head(n = 10)

topWords <- t(cbind(unigrams, bigrams))
colnames(topWords) <- paste0(paste('#', seq(10), sep = ''))
rownames(topWords) <- c('unigrams', 'bigrams')
topWords.toPrint = xtable(topWords, label = 'tab:words',
                          caption = 'Top 10 unigrams and bigrams with the highest reduction')
align(topWords.toPrint) <- paste0(c("|r||", rep('l|', 10)), collapse = "")
print(topWords.toPrint, floating.environment = "table*", comment=F, table.placement = 't')
```

## How do various properties reduce over time?

Reduction phenomena. From left: (1) mean message length in words per tangram, (2) mean number of listener messages, (3) proportion of utterances containing adjectival clauses, (4) proportion of utterances containing subordinate clauses. Error bars are bootstrapped 95\\% CIs.

```{r, cache = T}
posTags <- read_csv('posTagged.csv')

lengthReduction = tangramCombined %>%
   filter(role == "director") %>%
   group_by(gameid, roundNum) %>%
   summarize(individualM = sum(numRawWords)/12) %>%
   group_by(roundNum) %>%
   multi_boot_standard("individualM") %>%
   mutate(measure = '# words per tangram')

clauseReduction = posTags %>%
  group_by(roundNum) %>%
  mutate(clauses=as.logical(clauses)) %>%
  multi_boot_standard('clauses') %>%
  mutate(measure = '% subordinate clauses')

aclReduction = posTags %>%
  group_by(roundNum) %>%
  mutate(acl=as.logical(acl)) %>%
  multi_boot_standard('acl') %>%
  mutate(measure = '% adjectival clauses')

listenerMsgs <- tangramCombined %>%
  group_by(gameid, roundNum, role) %>%
  summarize(individualM = n()) %>%
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>%
  spread(role, individualM)

listenerReduction <- listenerMsgs %>%
   group_by(roundNum) %>%
   multi_boot_standard("matcher") %>%
   mutate(measure = '# listener messages')  

listenerReduction %>%
  rbind(lengthReduction) %>%
  rbind(clauseReduction) %>%
  rbind(aclReduction) %>%
  mutate(measure = factor(measure, levels = c(
    "# words per tangram", "# listener messages",
    "% adjectival clauses","% subordinate clauses"))) %>%
  ggplot(aes(x = roundNum, y = mean)) +
    geom_line() +
    xlab('round #') +
    ylab('metric') +
    geom_errorbar(aes(ymax = summary_ci_upper, ymin = summary_ci_lower), width = .1) +
    facet_wrap(~ measure, nrow = 1, ncol = 4, scales = 'free') +
    ylim(c(0, NA)) +
    theme_few(9) +
    theme(aspect.ratio = .75)
```

```{r}
listenerMsg_lm = summary(lmer(matcher ~ roundNum + (1 | gameid), data = listenerMsgs))
print(listenerMsg_lm)
```
Reduction rates for different parts of speech. Error bars are bootstrapped 95\\% CIs.

```{r}
posReduction = read_csv('posTagged.csv') %>%
  # Count occurences of each POS on each round within games
  group_by(roundNum, gameid) %>%
  summarize(numWords = sum(numWords),
            nouns = sum(nouns),
            #numbers = sum(numbers),
            verbs = sum(verbs),
            dets= sum(determiners),
            pronouns = sum(pronouns),
            preps = sum(prepositions),
            adjectives = sum(adjectives),
            adverbs = sum(adverbs)) %>%
  gather(POS, count, nouns:adverbs) %>%
  select(gameid, roundNum, POS, count) %>%
  # Need to put in ids to spread
  rowwise() %>%
  mutate(id = row_number()) %>%
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, count) %>%
  # Compute % reduction from first to last round
  mutate(diffPct = (round1 - round6)/round1) %>%
  group_by(POS) %>%
  # Filter out handful of people who skipped first round w/ negative %...
  filter(diffPct >= 0) %>%
  filter(POS != "OTHER") %>%
  # Take mean & se over participants
  multi_boot_standard('diffPct') %>%
  mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps'), 'closed', 'open')) %>%
  # rearrange
  transform(POS=reorder(POS, -mean) )

detReductionRate <- (posReduction %>% filter(POS == 'dets'))$mean * 100
nounReductionRate <- (posReduction %>% filter(POS == 'nouns'))$mean * 100

ggplot(posReduction, aes(x = POS, y = mean, fill = cat)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymax = summary_ci_upper, ymin = summary_ci_lower), width = .1)+
  theme_bw(9) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("% reduction") +
  xlab("Part of Speech category")
```

## Arbitrariness and stability

Calculate entropy under permutation tests

```{r}
getCounts <- function(contents) {
  corpus <- Corpus(VectorSource(paste(contents, collapse = " ")))
  return(colSums(as.matrix(DocumentTermMatrix(corpus))))
}

permutationTest <- function(df) {
  permuted = df %>%
    group_by(roundNum) %>%
    mutate(permutation = sample(contents)) %>%
    group_by(gameid) %>%
    summarize(ent = entropy(getCounts(permutation))) %>%
    ungroup()
  return(mean(permuted$ent))
}

permutations <- replicate(1000, {
  permutationTest(tangramCombined)
})

# Note that we explicitly do *not* normalize entropies, as this
# removes the contribution of a bigger vocabulary (as we would expect)
trueEntropy <- mean((tangramCombined %>%
  group_by(gameid) %>%
  summarize(ent = entropy(getCounts(contents))) %>%
  ungroup())$ent)

ggplot() +
  geom_histogram(aes(x = permutations), binwidth = .005) +
  geom_vline(xintercept = trueEntropy, color = 'red') +
  theme_few(9) +
  xlab('entropy')
```

## Did pairs who talked more become more efficient?

```{r}
turnTaking <- listenerMsgs %>%
  filter(roundNum %in% c(1)) %>%
  group_by(gameid) %>%
  summarize(numListenerMsgs = mean(matcher)) %>%
  filter(numListenerMsgs < mean(numListenerMsgs) + 3*sd(numListenerMsgs)) %>%
  select(gameid, numListenerMsgs)

efficiency <- tangramCombined %>%
   filter(role == "director") %>%
   group_by(gameid, roundNum) %>%
   summarize(individualM = sum(numRawWords)/12) %>%
  rowwise() %>%
  mutate(id = row_number()) %>%
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, individualM) %>%
  mutate(diffPct = (round1 - round6)/round1) %>%
  filter(diffPct >= 0) %>% # Filter out handful of people who skipped first round...
  select(gameid, diffPct)

turnTakingEfficiencyPlot <- ggplot(turnTaking %>% left_join(efficiency), aes(x = numListenerMsgs, y = diffPct)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_bw(9) +
  ylab("% reduction") +
  xlab("# listener messages on 1st round")

turnTakingEfficiency_lm <- summary(lm(diffPct ~ numListenerMsgs, data = efficiency %>% left_join(turnTaking)))
turnTakingdf <- turnTakingEfficiency_lm$df[1]
turnTakingCoefs <- turnTakingEfficiency_lm$coefficients[2,]
turnTakingResult <- paste0("t(", turnTakingdf, ") = ", round(turnTakingCoefs[3],2), ", p = ", round(turnTakingCoefs[4],2))
turnTakingEfficiencyPlot
```


### Supplemental: Director vs. matcher proportions

At beginning, directors send about 60% of total messages (close to equal!) At end, they send 80% -- listeners stop talking as much. This is just another way of looking at the total number of listener messages dropping.

```{r}
tangramCombined %>% 
  group_by(gameid, roundNum, role) %>% 
  summarize(individualM = n()) %>% 
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>% 
  spread(role, individualM) %>% 
  mutate(ratio = director / (director + matcher)) %>%
   group_by(roundNum) %>% 
   summarize(m = mean(ratio), 
             se = sd(ratio)/sqrt(length(ratio))) %>%
ggplot(aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("% of total messages sent by director") +
  xlab("trials") +
  ylim(.5,1) +
  xlim(0, 7) +
  theme_bw() 
```

### Supplemental: accuracy over time

Bring boards into it?

```{r}
tangramBoards = read.csv("../data/tangrams_unconstrained/finalBoard/tangramsFinalBoards.csv") %>%
  gather(tangram, location, subA:trueL) %>% 
  separate(tangram, into = c("type", "tangramName"), -2) %>% 
  spread(key = type, value = location)  %>%
  rename(matcherLoc = sub, trueLoc = true) %>%
  mutate(match = matcherLoc == trueLoc)
write.csv(tangramBoards, "reformattedBoards.csv", row.names = F)
```

subject-level performance over time?

```{r}
tangramBoards %>% 
  group_by(gameid, roundNum) %>% 
  summarize(matchProp = sum(match)/12) %>%
  ggplot(aes(x = roundNum, y = matchProp, color = gameid)) +
    geom_line()
```

average performance over time (participants get better)

```{r}
tangramBoards %>% 
  group_by(gameid, roundNum) %>% 
  summarize(matchProp = sum(match)/(12)) %>%
  filter(matchProp > .2) %>%
  group_by(roundNum) %>%
  summarize(m = mean(matchProp), se = sd(matchProp)/sqrt(length(matchProp))) %>%
  ggplot(aes(x = roundNum, y = m)) +
    geom_line() +
    geom_errorbar(aes(ymax = m + se, ymin = m - se), width = 0 )
```

# Exp. 2: Sequential

## Imports & preprocessing

```{r}
tangramMsgs = read_csv("../data/tangrams_sequential/message/sequential_message.csv") %>%
  rename(msgTime = time, role = sender)

tangramClicks = read_csv("../data/tangrams_sequential/clickedObj/sequential_clicks.csv")# %>% mutate(roundNum = as.numeric(roundNum))
tangramSubjInfo = read.csv("../data/tangrams_sequential/tangrams_sequential-subject_information.csv") 

rawTangramCombined <- tangramMsgs %>% left_join(tangramSubjInfo, by = c('gameid', 'role')) %>% left_join(tangramClicks, by = c('gameid', 'roundNum', 'occurrenceNum'))
```

Set exclusion criteria

```{r}
nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- unique((rawTangramCombined %>% group_by(gameid) %>% 
                           filter(length(unique(roundNum)) != 72))$gameid)
badGames <- union(incompleteIDs, nonNativeSpeakerIDs)
print(paste0('excluding ', length(badGames), ' of ', length(unique(rawTangramCombined$gameid)), ' games that were either incomplete or had non-native english speakers'))
```

Check and remove all games of duplicate turkers (note that I accidentally excluded some of the partial games before compiling rawMessages, so there are gameids in subjInfo that never appear in rawMessages...)

`9477-662e7f6d-0f2a-4041-a4e7-6d33eba91fb0`	is probably okay: they only did one round in their previous game before getting kicked... also `6402-3db6c45b-0b53-4906-b5cf-17517d013671` is probably okay: this is their first game.

```{r}
duplicate_turkers = (tangramSubjInfo %>% group_by(workerid_uniq) %>% tally() %>% filter(n > 1))$workerid_uniq
# subjInfoids = unique((tangramSubjInfo %>% filter(workerid_uniq %in% duplicates))$gameid)
# realids = unique((rawTangramCombined %>% filter(workerid_uniq %in% duplicates))$gameid)
duplicate_gameids = (rawTangramCombined %>% 
  filter(workerid_uniq %in% duplicate_turkers)%>% 
  group_by(workerid_uniq, gameid) %>% 
  summarize(time = first(msgTime), numRounds = last(roundNum)) %>%
  arrange(workerid_uniq, time))$gameid
duplicate_gameids
```

Count numbers of words, examine size of dataset

```{r}
tangramCombined <- rawTangramCombined %>%
  filter(!(gameid %in% badGames)) %>%
  filter(!(gameid %in% duplicate_gameids)) %>%
  mutate(numRawWords = str_count(contents, "\\S+")) %>%
  filter(!is.na(numRawWords)) # filter out pure punctuation messages
  #filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

write_csv(tangramCombined, '../data/tangramsSequential.csv')

numGames <- length(unique(tangramCombined$gameid))
numUtterances <- length(tangramCombined$contents)
print(paste0(numUtterances, ' utterances in ', numGames, ' games'))
```


# Accuracy

Accuracy tends to go up on average (collapsed across everyone!)

```{r}
tangramSingleUtt = tangramCombined %>%
  group_by(gameid,roundNum) %>%
  mutate(contents = paste0(contents, collapse = '.')) %>%
  filter(row_number() == 1)

tangramSingleUtt %>%
  group_by(roundNum) %>%
  summarize(propCorrect = sum(correct)/length(correct)) %>%
  ggplot(aes(x = roundNum, y = propCorrect, group = 1)) +
    geom_point() +
    geom_vline(xintercept = (seq(12, 12*6, 12))) +
    geom_smooth() +
    theme_few() 
```

Break out by occurence of each tangram (some start out harder than others; everyone gets the rabbit right but that weird angel one (B) is terrible)

```{r}
tangramSingleUtt %>%
  group_by(occurrenceNum, intendedObj) %>%
  summarize(propCorrect = sum(correct)/length(correct)) %>%
  ggplot(aes(x = occurrenceNum, y = propCorrect, group = intendedObj)) +#, group = gameid)) +#, group = gameid)) +
    geom_line(alpha = .2) +
    geom_text(aes(label = intendedObj)) +
    theme_few() 

```

Look at confusion matrix of errors

TODO: figure out how to rearrange by cluster?

```{r}
confusionMatrix <- tangramSingleUtt %>%
  group_by(intendedObj, clickedObj) %>%
  tally() %>%
  ungroup() %>%
  complete(intendedObj, clickedObj, fill = list(n = 1)) %>%
  spread(intendedObj, n)
as.matrix(confusionMatrix %>%select(-clickedObj) )

# dist(confusionMatrix)
# clickedOrder <- hclust(as.matrix(confusionMatrix %>% select(-clickedObj)))$order 
# 
# cbind(confusionMatrix, clickedOrder) %>% gather(intendedObj, count, A:L) %>% arrange(clickedOrder, intendedObj) %>% mutate(clickedObj = factor(clickedObj, unique(clickedObj)), intendedObj = factor(intendedObj, unique(clickedObj))) %>%
tangramSingleUtt %>%
  group_by(intendedObj, clickedObj) %>%
  tally() %>%
  ungroup() %>%
  complete(intendedObj, clickedObj, fill = list(n = 1)) %>%
  ggplot(aes(x = intendedObj, y = clickedObj, fill = log(n))) +
    geom_bin2d() +
    theme(aspect.ratio = 1)
```

Reduction per tangram

```{r}
library(directlabels)

lengthReduction <- tangramCombined %>%
   filter(role == "director") %>%
   group_by(gameid, occurrenceNum, intendedName) %>%
   summarize(individualM = sum(numRawWords)) %>%
   group_by(occurrenceNum, intendedName) %>%
   summarize(m = mean(individualM)) %>%
   #multi_boot_standard("individualM") %>%
   mutate(measure = '# words per tangram')

ggplot(lengthReduction, aes(x = occurrenceNum, y = m,group = intendedName)) +
  geom_line() +
  theme_few() +
  geom_dl(aes(label = intendedName), method = list(dl.trans(x = x - .25), "first.points")) +
  xlab("repetition #") +
  ylab("mean # words")
  
```

Questions: 

-- is just this a result of motivation, i.e. people get sloppy and just want to finish as game goes on? (probably not; accuracy goes up and no comparable reduction in other ref games of similar length w/o repetition, e.g. chairs, colors)

-- is this just a result of meta-discourse? Maybe people are just chatting each other up a lot at the beginning (e.g. 'hi, how are you? what's your name?') and stop doing that over time? (probably not, but we should do this restricted to just noun phrases...)

```{r}
summary(lmer(numRawWords ~ poly(occurrenceNum,2) + 
               (1 + occurrenceNum | gameid) + 
               (1 + occurrenceNum | intendedObj), 
             data = tangramCombined %>% 
               group_by(gameid, roundNum, occurrenceNum, intendedObj) %>% 
               summarize(numRawWords = sum(numRawWords))))
```

# PMI

Scatter plot (all datapoints)

```{r}
distinctiveness_d <- read.csv("sequential_matchAndPMI.csv", header = TRUE) %>%
  mutate(POS = as.character(POS)) %>%
  mutate(POS = ifelse(POS %in% c('NN', 'NNS', 'NNP', 'NNPS'), "noun", POS)) %>%
  mutate(POS = ifelse(POS %in% c('MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG'), "verb", POS)) %>%
  mutate(POS = ifelse(POS %in% c('DT', 'WDT'), 'det', POS)) %>%
  mutate(POS = ifelse(POS %in% c('PRP', 'PRP$', 'WP', 'WP$'), 'pronoun', POS)) %>%
  mutate(POS = ifelse(POS %in% c('CC'), 'conjunction', POS)) %>%
  mutate(POS = ifelse(POS %in% c('JJ', 'JJR', 'JJS'), 'adjective', POS)) %>%
  mutate(POS = ifelse(POS == 'IN', 'preposition', POS)) %>%
  mutate(POS = ifelse(POS %in% c('noun', 'verb', 'det', 'pronoun', 'conjunction', 'adjective', 'preposition'), 
                      POS, 'other')) %>%
  filter(pmi > 0) %>%
  rename(num_occurrences = total) %>%
  filter(num_occurrences > 1) %>%
  #filter(POS == "NN") %>%
  mutate(bunny = word == "bunny") %>%
  mutate(a_match = word == "a")
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# Split out into binary values:
binary.d <- distinctiveness_d %>% 
  mutate(numOnes = match * num_occurrences,
         numZeros = num_occurrences - numOnes) %>%
  do(rbind(data.frame(.[rep(1:nrow(.), .$numOnes),]) %>% mutate(singleMatch = 1),
           data.frame(.[rep(1:nrow(.), .$numZeros),]) %>% mutate(singleMatch = 0))) 

summary(glm(singleMatch ~ pmi, data=binary.d, family = 'binomial'))

ggplot(distinctiveness_d, aes(x = pmi, y = match)) +
  geom_point(aes(size = num_occurrences)) +
  geom_smooth(data = binary.d, aes(x = pmi, y = singleMatch),
    method = "glm", method.args = list(family = "binomial"), 
    se = FALSE) +
  theme_few()
```  

### Restrict to words with more occurences & common POS

```{r}
restricted_distinctiveness_d <- distinctiveness_d %>% 
   filter(POS %in% c('noun', 'preposition', 'det', 'adjective'))%>%
   filter(num_occurrences > 1)
                
restricted_binary.d <- restricted_distinctiveness_d %>% 
  mutate(numOnes = match * num_occurrences,
         numZeros = num_occurrences - numOnes) %>%
  do(rbind(data.frame(.[rep(1:nrow(.), .$numOnes),]) %>% mutate(singleMatch = 1),
           data.frame(.[rep(1:nrow(.), .$numZeros),]) %>% mutate(singleMatch = 0))) 

summary(glm(singleMatch ~ pmi, data=restricted_binary.d, family = 'binomial'))

ggplot(restricted_distinctiveness_d, aes(x = pmi, y = match)) +
  geom_point(aes(size = num_occurrences)) +
  geom_point(aes(x = pmi, y = match), color = 'red', data = subset(distinctiveness_d, bunny == TRUE)) +
  geom_point(aes(x = pmi, y = match), stroke = 3, color = 'yellow', data = subset(distinctiveness_d, a_match == TRUE)) +
  #geom_smooth(method = 'loess', span = .8) +
  geom_smooth(data = restricted_binary.d, aes(x = pmi, y = singleMatch),
    method = "glm", method.args = list(family = "binomial"), 
    se = FALSE) +
  theme_few() +
  scale_colour_manual(values=cbbPalette)+
  guides(color=FALSE)
```

Look at pmi across POS...

```{r}
pos_d <- read.csv("sequential_matchAndPMI.csv", header = TRUE) %>%
  filter(pmi > 0) %>%
  mutate(POS = as.character(POS)) %>%
  mutate(POS = ifelse(POS %in% c('NN', 'NNS', 'NNP', 'NNPS'), "noun", POS)) %>%
  mutate(POS = ifelse(POS %in% c('MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG'), "verb", POS)) %>%
  mutate(POS = ifelse(POS %in% c('DT', 'WDT'), 'det', POS)) %>%
  mutate(POS = ifelse(POS %in% c('PRP', 'PRP$', 'WP', 'WP$'), 'pronoun', POS)) %>%
  mutate(POS = ifelse(POS %in% c('CC'), 'conjunction', POS)) %>%
  mutate(POS = ifelse(POS %in% c('JJ', 'JJR', 'JJS'), 'adjective', POS)) %>%
  mutate(POS = ifelse(POS == 'IN', 'preposition', POS)) %>%
  mutate(POS = ifelse(POS %in% c('noun', 'verb', 'det', 'pronoun', 'conjunction', 'adjective', 'preposition'), POS, 'other')) %>%
  group_by(POS) %>%
  summarize(se = sd(pmi)/sqrt(length(pmi)),
            mean_pmi = mean(pmi),
            num = sum(total),
            mean_match = mean(match)) %>%
  filter(num > 200)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(pos_d, aes(x = reorder(POS,mean_pmi,
                     function(x)-x), y = mean_pmi)) +
  geom_bar(stat='identity') +
  geom_errorbar(aes(ymax = mean_pmi + se, ymin = mean_pmi - se)) +
  theme_few() +
  xlab("part of speech") +
  ylab("pointwise mutual information")
```


Alternatively, can do a nonparametric analysis: draw a random word from each tangram/gameid pair and look at the percentage that match with round 6... This gives a null distribution. Then we can take the highest PMI word (or words) for each tangram/gameid pair and look at the percentage of *those* that match. We see that it's much higher than expected under the null.

```{r}
# TODO: get red to show up in legend
nonparametric_d = read.csv("tangrams/PMIbootstrap.csv", header = TRUE) %>%
  mutate(PMI = factor(highest, levels = c('null', 'highest'), labels = c('random', 'top')))

highestValAvg = nonparametric_d %>% filter(highest == 'highest') %>% summarize(avg = mean(match))

nonparametric_d %>%
  group_by(sampleNum, PMI) %>%
  filter(PMI == 'random') %>%
  summarize(avgMatchRate = mean(match)) %>%
  ungroup() %>%
  ggplot(aes(x = avgMatchRate, fill = PMI)) +
  geom_histogram(binwidth = .0075) +
  geom_vline(aes(xintercept = highestValAvg), 
             color = 'red', linetype = "dashed", size = 2) +
  xlab("probability of match b/w Rounds 1 & 6") +
  scale_fill_manual(values = c("random" = "black", "top"= "red")) +
  theme_bw() 
  #guides(color=FALSE)
```

Supplemental: look at wordclouds!

```{r}
library(wordcloud)   

oldGrams = read.csv("handTagged.csv", quote = '"') %>%
  mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
                                       separate = F))) %>%
  mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

textPerGram = oldGrams %>%
  group_by(gameid, tangramRef) %>%
  filter(tangramRef != 0) %>%
  filter(tangramRef != 'None') %>%
  filter(roundNum == 6) %>%
  summarize(a = paste(cleanMsg, collapse = " ")) %>%
  group_by(tangramRef) %>%
  summarize(text = paste(a, collapse = " ")) %>%
  rename(docs = tangramRef) %>%
  mutate(docs = paste("doc ", docs))

corpus = Corpus(VectorSource(textPerGram$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('english'))

dtm = DocumentTermMatrix(corpus)

numDocs = dim(dtm)[1]
numTerms = dim(dtm)[2]
  
for(i in 1:numDocs) {
  png(paste("wordcloudForTangram", i, ".png", sep = ""), bg = "transparent")
  freq <- sort(colSums(as.matrix(dtm[i,])), decreasing=TRUE)
  # print(entropy(freq))
   wordcloud(names(freq), freq, min.freq = 1, colors=brewer.pal(6, "Dark2"))   
  dev.off()
}
```

## Supplemental: across-pair entropy and within-pair entropy

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  group_by(gameid, tangramRef) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangramRef) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangramRef != 0) %>%
  filter(tangramRef != 'None') %>%
  filter(tangramRef != '*') %>%
  group_by(tangramRef, roundNum) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = "tangramRef") %>%
  gather(type, entropy, acrossEnt, withinEnt)

ggplot(acrossPair, aes(x = roundNum, y = entropy, 
                       color = type, linetype = tangramRef)) +
  geom_line()
```

Or we could look at both on each half? 

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(gameid, tangram, half) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangram, half) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(tangram, half) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = c("tangram", "half")) %>%
  gather(type, entropy, acrossEnt, withinEnt)

acrossPair

ggplot(acrossPair,
       aes(x = half, y = entropy, 
           color = tangram, linetype = type, group = interaction(tangram, type))) +
  geom_line()
```
