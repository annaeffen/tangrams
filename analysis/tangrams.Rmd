---
title: "tangramsReference"
output: 
  html_document:
    toc: true
    toc_depth: 2

---

# Import data

```{r}
library(tidyverse)
library(lme4)
library(lmerTest)
library(tm)
library(stringr)
library(knitr)
library(NLP)
library(readr)
library(tidyboot)
library(xtable)
library(ggthemes)
library(entropy)
```

# Dynamics of structure/syntax

We've already done most of the work using nltk in the ipython notebook, so here we just read in the csv files we created there, do a bit of post-processing, and make plots.

## What are the unigrams/bigrams/trigrams that drop the most in frequency across whole data set?

```{r}
# Function takes a countType ['unigrams', 'bigrams', 'trigrams'] and returns a table of most reduced n-gram
getMostReduced = function(countType) {
  filename <- paste0("sequential/", countType, "Counts.csv")
  print(filename)
  return(read_csv(filename, col_names = T) %>%
    group_by(word, repetitionNum) %>%
    summarize(count = sum(count)) %>%
    rowwise() %>%
    mutate(repetitionNum = paste0("rep", repetitionNum, collapse = "")) %>%
    spread(repetitionNum, count) %>%
    mutate(diffSize = rep1 - rep6,
           diffPct = (rep1 - rep6)/rep1) %>%
    arrange(desc(diffSize))) 
}
```

```{r}
getMostReduced('unigrams')
```

```{r}
getMostReduced('bigrams')

```

```{r}
getMostReduced('trigrams')
```

Make little table to put in supplemental?

```{r}
topWords <- t(cbind(getMostReduced('unigrams') %>% select(word) %>% head(10), 
                    getMostReduced('bigrams') %>% select(word) %>% head(10),
                    getMostReduced('trigrams') %>% select(word) %>% head(10)))

colnames(topWords) <- paste0(paste('#', seq(10), sep = ''))
rownames(topWords) <- c('unigrams', 'bigrams', 'trigrams')
topWords.toPrint = xtable(topWords, label = 'tab:words',
                          caption = 'Top 10 unigrams and bigrams with the highest reduction')
align(topWords.toPrint) <- paste0(c("|r||", rep('l|', 10)), collapse = "")
print(topWords.toPrint, floating.environment = "table*", comment=F, table.placement = 't')
```

Seems to be markers of figurative descriptions, determiners, and modifying phrases like 'on the left'?

## How do various properties reduce over time?

### Reduction in utterance length

```{r, cache = T, warning=F}
tagged_df <- read_csv('sequential/posTagged.csv') %>%
  filter(role == 'director') %>%
  select(-role, -doc, -posCounts, -pos) %>%
  left_join(read_csv('sequential/constituency_tags.csv') %>%
      replace_na(list(SBAR = FALSE, PP = FALSE, CC = FALSE)) %>%
      mutate(SBAR = ifelse(SBAR == 'True', TRUE, FALSE),
           PP = ifelse(PP == 'True', TRUE, FALSE), 
           CC = ifelse(CC == 'True', TRUE, FALSE)) %>%
      select(-X1))
```

```{r}
tagged_df %>%
   group_by(gameid, repetitionNum, taskVersion) %>%
   summarize(individualM = sum(numRawWords)/12) %>%
   group_by(repetitionNum, taskVersion) %>%
   tidyboot_mean(individualM, na.rm = T) %>%
   mutate(measure = '# words per tangram') %>%
   ggplot(aes(x = repetitionNum, y = empirical_stat, color = taskVersion)) +
    geom_line() +
    xlab('repetition #') +
    ylab('log # words') +
    geom_ribbon(aes(ymax = ci_upper, ymin = ci_lower), alpha = .2) +
    #facet_wrap(~ taskVersion, nrow = 1, ncol = 2) +
    ylim(c(0, NA)) +
    theme_few(12) +
    theme(aspect.ratio = .75)
```

Broken out by tangrams for cued condition

```{r}
library(directlabels)

lengthReduction <- tagged_df %>%
   filter(taskVersion == "cued") %>%
   group_by(gameid, repetitionNum, intendedName) %>%
   summarize(individualM = sum(numRawWords)) %>%
   group_by(repetitionNum, intendedName) %>%
   tidyboot_mean(individualM) %>%
   mutate(measure = '# words per tangram')

ggplot(lengthReduction, aes(x = repetitionNum, y = empirical_stat ,group = intendedName)) +
  geom_line() +
  theme_few() +
  geom_dl(aes(label = intendedName), method = list(dl.trans(x = x - .1), "first.points")) +
  xlab("repetition #") +
  ylab("mean # words") +
  theme(aspect.ratio = 1)
  
ggsave('../writing/tangrams/figs/num_words_sequential.pdf')
```

Statistics:

```{r}
summary(lmer(numRawWords ~ poly(repetitionNum,2) + 
               (1 + repetitionNum | gameid) + 
               (1 + repetitionNum | intendedName), 
             data = tagged_df %>% 
               filter(taskVersion == 'cued') %>%
               group_by(gameid, trialNum, repetitionNum, intendedName) %>% 
               summarize(numRawWords = sum(numRawWords))))
```

### Reduction in POS features 

examine reduction in different parts of speech, i.e. to argue that it's not just small-talk going down or something. 

```{r}
tagged_df %>% 
  #filter(taskVersion == 'cued') %>%
  mutate(numNPWords = ifelse(noun_chunks == "[]", 0, str_count(noun_chunks, "\\S+"))) %>%
  group_by(gameid, repetitionNum, taskVersion) %>% 
  summarize(NOUN = sum(NOUNcount),
            PRON = sum(PRONcount),
            ADJ = sum(ADJcount),
            DET = sum(DETcount),
            VERB = sum(VERBcount)) %>%
  gather(POS, count, NOUN, ADJ, DET, VERB, PRON) %>%
  mutate(count = count / 12) %>%
  group_by(repetitionNum, POS, taskVersion) %>%
  tidyboot_mean(count) %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat, color = POS)) +
    geom_line() +
    theme_few() +
    geom_ribbon(aes(ymax = ci_upper, ymin = ci_lower), alpha = .2) +
    #geom_dl(aes(label = intendedName), method = list(dl.trans(x = x - .1), "first.points")) +
    facet_wrap(~ taskVersion) +
    xlab("repetition #") +
    ylab("# words in POS category per description") +
    theme(aspect.ratio = 1)
```

Just noun broken out by tangram

```{r}
tagged_df %>% 
  filter(taskVersion == 'cued') %>%
  mutate(numNPWords = ifelse(noun_chunks == "[]", 0, str_count(noun_chunks, "\\S+"))) %>%
  group_by(gameid, repetitionNum, intendedName) %>% 
  summarize(NOUN = sum(NOUNcount)) %>%
  group_by(repetitionNum, intendedName) %>%
  tidyboot_mean(NOUN) %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat, group = intendedName)) +
    geom_line() +
    theme_few() +
    # geom_ribbon(aes(ymax = ci_upper, ymin = ci_lower), alpha = .2) +
    geom_dl(aes(label = intendedName), method = list(dl.trans(x = x - .1), "first.points")) +
    xlab("repetition #") +
    ylab("# nouns per description") +
    theme(aspect.ratio = 1)
```

TODO: distance in depth of the tree between the nouns

TODO: consider only showing full curve like this for ones we want to highlight (e.g. nouns vs. verbs vs. adjectives or something?), and a supplemental bar plot version with % decrease in appearance of each POS (like in the cogsci paper).

```{r}
# df_tagged %>%
#   group_by(roundNum, gameid) %>%
#   summarize(numWords = sum(numWords),
#             nouns = sum(nouns),
#             #numbers = sum(numbers),
#             verbs = sum(verbs),
#             dets= sum(determiners),
#             pronouns = sum(pronouns),
#             preps = sum(prepositions),
#             adjectives = sum(adjectives),
#             adverbs = sum(adverbs)) %>%
#   gather(POS, count, nouns:adverbs) %>%
#   select(gameid, roundNum, POS, count) %>%
#   # Need to put in ids to spread
#   rowwise() %>%
#   mutate(id = row_number()) %>%
#   mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
#   spread(roundNum, count) %>%
#   # Compute % reduction from first to last round
#   mutate(diffPct = (round1 - round6)/round1) %>%
#   group_by(POS) %>%
#   # Filter out handful of people who skipped first round w/ negative %...
#   filter(diffPct >= 0) %>%
#   filter(POS != "OTHER") %>%
#   # Take mean & se over participants
#   multi_boot_standard('diffPct') %>%
#   mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps'), 'closed', 'open')) %>%
#   # rearrange
#   transform(POS=reorder(POS, -mean) )
# 
# detReductionRate <- (posReduction %>% filter(POS == 'dets'))$mean * 100
# nounReductionRate <- (posReduction %>% filter(POS == 'nouns'))$mean * 100
# 
# ggplot(posReduction, aes(x = POS, y = mean, fill = cat)) +
#   geom_bar(stat = 'identity') +
#   geom_errorbar(aes(ymax = summary_ci_upper, ymin = summary_ci_lower), width = .1)+
#   theme_bw(9) +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
#   ylab("% reduction") +
#   xlab("Part of Speech category")
```

### Reduction in syntactic features 

TODO: include % including NPs? (shouldn't go down as much)

```{r}
tagged_df %>%
  select(gameid, repetitionNum, taskVersion, SBAR, CC, PP) %>%
  gather(measure, occurrence, SBAR:PP) %>%
  group_by(taskVersion, repetitionNum, gameid, measure) %>%
  summarize(m = mean(occurrence)) %>%
  group_by(taskVersion, repetitionNum, measure) %>%
  tidyboot_mean(m) %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat)) +
    geom_line() +
    xlab('round #') +
    ylab('% of messages containing syntactic feature') +
    geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1) +
    facet_grid(taskVersion ~ measure) +
    ylim(c(0, NA)) +
    theme_few(9) +
    theme(aspect.ratio = .75)
```


TODO: compare starting to ending proportions;

Same thing broken out by tangram

```{r}
tagged_df %>%
  filter(taskVersion == 'cued') %>%
  select(gameid, intendedName, repetitionNum, SBAR, CC, PP) %>%
  gather(measure, occurrence, SBAR:PP) %>%
  group_by(repetitionNum, intendedName, gameid, measure) %>%
  summarize(m = mean(occurrence)) %>%
  group_by(intendedName, repetitionNum, measure) %>%
  tidyboot_mean(m) %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat, group = intendedName)) +
    geom_line() +
    xlab('round #') +
    ylab('% of messages containing syntactic feature') +
    #geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1) +
    facet_wrap(~ measure, nrow = 1, ncol = 3) +
    ylim(c(0, NA)) +
    theme_few(9) +
    theme(aspect.ratio = .75)
```

### Reduction in listener exchanges

```{r}
listenerMsgs <- read_csv('../data/tangrams.csv') %>%
  group_by(gameid, repetitionNum, role, taskVersion) %>%
  summarize(individualM = n()) %>%
  ungroup() %>%
  complete(role, repetitionNum, gameid, fill = list(individualM = 0)) %>%
  spread(role, individualM)

listenerReduction <- listenerMsgs %>%
   group_by(repetitionNum) %>%
   tidyboot_mean(matcher) %>%
   mutate(measure = '# listener messages')  

listenerMsg_lm = summary(lmer(matcher ~ repetitionNum + (1 | gameid), data = listenerMsgs))
print(listenerMsg_lm)
```

### Did pairs who talked more become more efficient?

```{r}
turnTaking <- listenerMsgs %>%
  filter(repetitionNum %in% c(1)) %>%
  group_by(gameid, taskVersion) %>%
  summarize(numListenerMsgs = mean(matcher)) %>%
  ungroup() %>%
  #filter(numListenerMsgs < mean(numListenerMsgs) + 3*sd(numListenerMsgs)) %>%
  select(gameid, numListenerMsgs, taskVersion)

efficiency <- read_csv('../data/tangrams.csv') %>%
   filter(role == "director") %>%
   group_by(gameid, repetitionNum, taskVersion) %>%
   summarize(individualM = sum(numRawWords)/12) %>%
  rowwise() %>%
  mutate(id = row_number()) %>%
  mutate(repetitionNum = paste0('round', repetitionNum, collapse = '')) %>%
  spread(repetitionNum, individualM) %>%
  mutate(diffPct = (round1 - round6)/round1) %>%
  filter(diffPct >= 0) %>% # Filter out handful of people who skipped first round...
  select(gameid, diffPct, taskVersion)

turnTakingEfficiencyPlot <- ggplot(turnTaking %>% left_join(efficiency), 
                                   aes(x = numListenerMsgs, y = diffPct, color = taskVersion)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_bw(9) +
  ylab("% reduction") +
  xlab("# listener messages on 1st round")

turnTakingEfficiency_lm <- summary(lm(diffPct ~ numListenerMsgs + taskVersion, data = efficiency %>% left_join(turnTaking)))
turnTakingdf <- turnTakingEfficiency_lm$df[1]
turnTakingCoefs <- turnTakingEfficiency_lm$coefficients[2,]
turnTakingResult <- paste0("t(", turnTakingdf, ") = ", round(turnTakingCoefs[3],2), ", p = ", round(turnTakingCoefs[4],2))
turnTakingEfficiencyPlot
```

# Dynamics of semantics

## How much is within-game similarity disrupted by scrambling utterances across games?

Calculate entropy under permutation tests

```{r}
getCounts <- function(contents) {
  corpus <- Corpus(VectorSource(paste(contents, collapse = " ")))
  return(colSums(as.matrix(DocumentTermMatrix(corpus))))
}

permutationTest <- function(df) {
  permuted = df %>%
    group_by(roundNum) %>%
    mutate(permutation = sample(contents)) %>%
    group_by(gameid) %>%
    summarize(ent = entropy(getCounts(permutation))) %>%
    ungroup()
  return(mean(permuted$ent))
}

permutations <- replicate(1000, {
  permutationTest(tangramCombined)
})

# Note that we explicitly do *not* normalize entropies, as this
# removes the contribution of a bigger vocabulary (as we would expect)
trueEntropy <- mean((tangramCombined %>%
  group_by(gameid) %>%
  summarize(ent = entropy(getCounts(contents))) %>%
  ungroup())$ent)

ggplot() +
  geom_histogram(aes(x = permutations), binwidth = .005) +
  geom_vline(xintercept = trueEntropy, color = 'red') +
  theme_few(9) +
  xlab('entropy')
```

More targeted tangram-by-tangram analysis of cued version only

```{r}
tangramCombined %>%
    group_by(occurrenceNum, intendedObj) %>%
    mutate(permutation = sample(contents)) %>%
    group_by(gameid, intendedObj) %>%
    summarize(ent = entropy(getCounts(permutation))) %>%
    group_by(intendedObj) %>%
    summarize(meanEnt = mean(ent))

permutationTest <- function(df) {
  permuted = df %>%
    group_by(occurrenceNum, intendedObj) %>%
    mutate(permutation = sample(contents)) %>%
    group_by(gameid, intendedObj) %>%
    summarize(ent = entropy(getCounts(permutation))) %>%
    group_by(intendedObj) %>%
    summarize(meanEnt = mean(ent))
  return(permuted$meanEnt)
}

numPermutations = 1000
permutations <- replicate(numPermutations, {
  permutationTest(tangramCombined)
})

# Note that we explicitly do *not* normalize entropies, as this
# removes the contribution of a bigger vocabulary (as we would expect)
trueEntropy <- (tangramCombined %>%
  group_by(gameid, intendedObj) %>%
  summarize(trueEntropy = entropy(getCounts(contents))) %>%
  group_by(intendedObj) %>%
  summarize(meanEnt = mean(trueEntropy))
)

allEntropies <- cbind(trueEntropy, permutations) %>% 
  gather(permutationNum, ent, `1`:`1000`) %>%
  mutate(permutation = ifelse(permutationNum != 'trueEntropy', TRUE, FALSE))

allEntropies %>%
  ggplot(aes(x = ent)) +
    geom_histogram(binwidth = .01) +
    geom_vline(aes(xintercept = meanEnt), color = 'red') +
    theme_few(9) +
    facet_wrap( ~ intendedObj, scale = "free_y") +
    xlab('entropy') +
    theme(aspect.ratio = .5) 
```

```{r}
summary(lmer(ent ~ permutation + (1 + permutation | gameid) + (1 + permutation | intendedObj), 
             data = allEntropies))
```

## What determines whether a label gets conventionalized?

Scatter plot (all datapoints)

```{r}
distinctiveness_d <- read.csv("sequential_matchAndPMI.csv", header = TRUE) %>%
  mutate(POS = as.character(POS)) %>%
  mutate(POS = ifelse(POS %in% c('NN', 'NNS', 'NNP', 'NNPS'), "noun", POS)) %>%
  mutate(POS = ifelse(POS %in% c('MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG'), "verb", POS)) %>%
  mutate(POS = ifelse(POS %in% c('DT', 'WDT'), 'det', POS)) %>%
  mutate(POS = ifelse(POS %in% c('PRP', 'PRP$', 'WP', 'WP$'), 'pronoun', POS)) %>%
  mutate(POS = ifelse(POS %in% c('CC'), 'conjunction', POS)) %>%
  mutate(POS = ifelse(POS %in% c('JJ', 'JJR', 'JJS'), 'adjective', POS)) %>%
  mutate(POS = ifelse(POS == 'IN', 'preposition', POS)) %>%
  mutate(POS = ifelse(POS %in% c('noun', 'verb', 'det', 'pronoun', 'conjunction', 'adjective', 'preposition'), 
                      POS, 'other')) %>%
  filter(pmi > 0) %>%
  rename(num_occurrences = total) %>%
  filter(num_occurrences > 1) %>%
  #filter(POS == "NN") %>%
  mutate(bunny = word == "bunny") %>%
  mutate(a_match = word == "a")
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# Split out into binary values:
binary.d <- distinctiveness_d %>% 
  mutate(numOnes = match * num_occurrences,
         numZeros = num_occurrences - numOnes) %>%
  do(rbind(data.frame(.[rep(1:nrow(.), .$numOnes),]) %>% mutate(singleMatch = 1),
           data.frame(.[rep(1:nrow(.), .$numZeros),]) %>% mutate(singleMatch = 0))) 

summary(glm(singleMatch ~ pmi, data=binary.d, family = 'binomial'))

ggplot(distinctiveness_d, aes(x = pmi, y = match)) +
  geom_point(aes(size = num_occurrences)) +
  geom_smooth(data = binary.d, aes(x = pmi, y = singleMatch),
    method = "glm", method.args = list(family = "binomial"), 
    se = FALSE) +
  theme_few()
```  

### Restrict to words with more occurences & common POS

```{r}
restricted_distinctiveness_d <- distinctiveness_d %>% 
   filter(POS %in% c('noun', 'preposition', 'det', 'adjective', 'verb'))%>%
   filter(num_occurrences > 1)
                
restricted_binary.d <- restricted_distinctiveness_d %>% 
  mutate(numOnes = match * num_occurrences,
         numZeros = num_occurrences - numOnes) %>%
  do(rbind(data.frame(.[rep(1:nrow(.), .$numOnes),]) %>% mutate(singleMatch = 1),
           data.frame(.[rep(1:nrow(.), .$numZeros),]) %>% mutate(singleMatch = 0))) 

summary(glm(singleMatch ~ pmi, data=restricted_binary.d, family = 'binomial'))

ggplot(restricted_distinctiveness_d, aes(x = pmi, y = match)) +
  geom_point(aes(size = num_occurrences)) +
  geom_point(aes(x = pmi, y = match), color = 'red', data = subset(distinctiveness_d, bunny == TRUE)) +
  geom_point(aes(x = pmi, y = match), stroke = 3, color = 'yellow', data = subset(distinctiveness_d, a_match == TRUE)) +
  #geom_smooth(method = 'loess', span = .8) +
  geom_smooth(data = restricted_binary.d, aes(x = pmi, y = singleMatch),
    method = "glm", method.args = list(family = "binomial"), 
    se = FALSE) +
  theme_few() +
  scale_colour_manual(values=cbbPalette)+
  guides(color=FALSE)
```

### Look at pmi across POS...

```{r}
pos_d <- read.csv("sequential_matchAndPMI.csv", header = TRUE) %>%
  filter(pmi > 0) %>%
  mutate(POS = as.character(POS)) %>%
  mutate(POS = ifelse(POS %in% c('NN', 'NNS', 'NNP', 'NNPS'), "noun", POS)) %>%
  mutate(POS = ifelse(POS %in% c('MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG'), "verb", POS)) %>%
  mutate(POS = ifelse(POS %in% c('DT', 'WDT'), 'det', POS)) %>%
  mutate(POS = ifelse(POS %in% c('PRP', 'PRP$', 'WP', 'WP$'), 'pronoun', POS)) %>%
  mutate(POS = ifelse(POS %in% c('CC'), 'conjunction', POS)) %>%
  mutate(POS = ifelse(POS %in% c('JJ', 'JJR', 'JJS'), 'adjective', POS)) %>%
  mutate(POS = ifelse(POS == 'IN', 'preposition', POS)) %>%
  mutate(POS = ifelse(POS %in% c('noun', 'verb', 'det', 'pronoun', 'conjunction', 'adjective', 'preposition'), POS, 'other')) %>%
  group_by(POS) %>%
  summarize(se = sd(pmi)/sqrt(length(pmi)),
            mean_pmi = mean(pmi),
            num = sum(total),
            mean_match = mean(match)) %>%
  filter(num > 200)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(pos_d %>% filter(POS != 'other'), aes(x = reorder(POS,mean_pmi,
                     function(x)-x), y = mean_pmi)) +
  geom_bar(stat='identity') +
  geom_errorbar(aes(ymax = mean_pmi + se, ymin = mean_pmi - se)) +
  theme_few() +
  xlab("part of speech") +
  ylab("pointwise mutual information")
```


Alternatively, can do a nonparametric analysis: draw a random word from each tangram/gameid pair and look at the percentage that match with round 6... This gives a null distribution. Then we can take the highest PMI word (or words) for each tangram/gameid pair and look at the percentage of *those* that match. We see that it's much higher than expected under the null.

```{r}
# TODO: get red to show up in legend
nonparametric_d = read.csv("PMIbootstrap.csv", header = TRUE) %>%
  mutate(PMI = factor(highest, levels = c('null', 'highest'), labels = c('random', 'top')))

highestValAvg = nonparametric_d %>% filter(highest == 'highest') %>% summarize(avg = mean(match))

nonparametric_d %>%
  group_by(sampleNum, PMI) %>%
  filter(PMI == 'random') %>%
  summarize(avgMatchRate = mean(match)) %>%
  ungroup() %>%
  ggplot(aes(x = avgMatchRate, fill = PMI)) +
  geom_histogram(binwidth = .0075) +
  geom_vline(aes(xintercept = highestValAvg), 
             color = 'red', linetype = "dashed", size = 2) +
  xlab("probability of match b/w Rounds 1 & 6") +
  scale_fill_manual(values = c("random" = "black", "top"= "red")) +
  theme_few()  +
  guides(fill=FALSE)
```

## tsne visualizations

```{r}
tsne <- read_csv('sequential/embeddings.csv') %>%
  left_join(read_csv('../data/tangrams.csv') %>% filter(role == "director")) %>%
  select(gameid, intendedName, repetitionNum, x_tsne, y_tsne, x_mds, y_mds, contents)
ggplot(tsne, aes(x = x_tsne, y = y_tsne, group = gameid)) +
  geom_line(alpha = 0.1) +
  geom_point(aes(color = repetitionNum), size = 2, 
             data = tsne %>% filter(repetitionNum %in% c(1,6))) +
  facet_wrap(~ intendedName) +
  theme_few(20) +
  ggtitle('pca + tsne embeddings') +
  theme(aspect.ratio = 1)

ggsave('tsne_embeddings.pdf', width = 15, height = 15)
```

```{r}
ggplot(tsne, aes(x = x_mds, y = y_mds, group = gameid)) +
  geom_line(alpha = 0.1) +
  geom_point(aes(color = repetitionNum), size = 2, 
             data = tsne %>% filter(repetitionNum %in% c(1,6))) +
  facet_wrap(~ intendedName) +
  theme_few(20) +
  ggtitle('MDS embeddings') +
  theme(aspect.ratio = 1)

ggsave('mds_embeddings.pdf', width = 15, height = 15)
```

# Supplemental analyses

## What proportion of messages sent by director vs. matcher, respectively?

At beginning, directors send about 60% of total messages (close to equal!) At end, they send 80% -- listeners stop talking as much. This is just another way of looking at the total number of listener messages dropping.

```{r}
tangramCombined %>% 
  group_by(gameid, roundNum, role) %>% 
  summarize(individualM = n()) %>% 
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>% 
  spread(role, individualM) %>% 
  mutate(ratio = director / (director + matcher)) %>%
   group_by(roundNum) %>% 
   summarize(m = mean(ratio), 
             se = sd(ratio)/sqrt(length(ratio))) %>%
ggplot(aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("% of total messages sent by director") +
  xlab("trials") +
  ylim(.5,1) +
  xlim(0, 7) +
  theme_bw() 
```

## Visualize word frequencies on final round...

```{r}
library(wordcloud)   

textPerGram = tangramCombined %>%
  filter(occurrenceNum == 6) %>%
  group_by(intendedObj) %>%
  # summarize(a = paste(contents, collapse = " ")) %>%
  summarize(text = paste(contents, collapse = " ")) %>%
  rename(docs = intendedObj) %>%
  mutate(docs = paste("doc ", docs))

corpus = Corpus(VectorSource(textPerGram$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('english'))

dtm = DocumentTermMatrix(corpus)

numDocs = dim(dtm)[1]
numTerms = dim(dtm)[2]
  
for(i in 1:numDocs) {
  png(paste("wordcloudForTangram", i, ".png", sep = ""), bg = "transparent")
  freq <- sort(colSums(as.matrix(dtm[i,])), decreasing=TRUE)
  # print(entropy(freq))
   wordcloud(names(freq), freq, min.freq = 1, colors=brewer.pal(6, "Dark2"))   
  dev.off()
}
```

## Slightly sketchy way of looking at arbitrariness by comparing across-pair and within-pair entropy

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  group_by(gameid, tangramRef) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangramRef) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangramRef != 0) %>%
  filter(tangramRef != 'None') %>%
  filter(tangramRef != '*') %>%
  group_by(tangramRef, roundNum) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = "tangramRef") %>%
  gather(type, entropy, acrossEnt, withinEnt)

ggplot(acrossPair, aes(x = roundNum, y = entropy, 
                       color = type, linetype = tangramRef)) +
  geom_line()
```

Or we could look at both on each half? 

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(gameid, tangram, half) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangram, half) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(tangram, half) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = c("tangram", "half")) %>%
  gather(type, entropy, acrossEnt, withinEnt)

acrossPair

ggplot(acrossPair,
       aes(x = half, y = entropy, 
           color = tangram, linetype = type, group = interaction(tangram, type))) +
  geom_line()
```
