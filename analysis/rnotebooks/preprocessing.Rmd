---
title: "Organize/pre-process data"
output: html_notebook
---

# Import dependencies 

```{r}
library(tidyverse)
library(tidyboot)
library(ggthemes)
library(jsonlite)
```

# Read in and compile data... 

```{r}
unconstrainedMsgs = read_csv("../../data/tangrams_unconstrained/message/rawUnconstrainedMessages.csv") %>%
  rename(msgTime = time, repetitionNum = roundNum, role = sender)

unconstrainedResponses = read_csv("../../data/tangrams_unconstrained/finalBoard/tangramsFinalBoards.csv") %>%
  rename(finalTime = time, repetitionScore = score, repetitionNum = roundNum) %>%
  group_by(gameid, repetitionNum) %>%
  gather(tangramCat, position, subA:trueL) %>%
  separate(tangramCat, into = c('type', 'tangramID'), sep = -1) %>%
  spread(type, position) %>%
  mutate(correct = sub == true) %>%
  select(-sub, -true) 

unconstrainedSubjInfo = read.csv("../../data/tangrams_unconstrained/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

unconstrainedData.raw <- unconstrainedMsgs %>% 
  left_join(unconstrainedSubjInfo, by = c('gameid', 'role')) %>%
  left_join(unconstrainedResponses %>% group_by(gameid, repetitionNum) %>% 
              summarize(repetitionScore = mean(repetitionScore))) 
```

## How accurate are participants overall? (before applying exclusion criteria)

Total scores on average show that people improve across successive rounds...

```{r}
unconstrainedResponses %>% 
  group_by(repetitionNum) %>%
  mutate(pctScore = repetitionScore/12) %>%
  tidyboot_mean(pctScore) %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat)) +
    geom_line() +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width =0) +
    theme_bw() +
    ylim(0, 1)
```

What is distribution across scores across games for each round? 

```{r}
unconstrainedResponses %>% 
  group_by(repetitionNum, gameid, repetitionScore) %>%
  tally() %>%
  #filter(!(gameid %in% sloppyIDs)) %>%
  select(-n) %>%
  group_by(repetitionNum, repetitionScore) %>%
  tally() %>%
  ggplot(aes(x = repetitionScore, y = n)) +
    geom_bar(stat = 'identity') +
    facet_wrap(~ repetitionNum) +
    theme_bw() +
    xlim(-.5,12.5)
```

Looks like there is a subpopulation who are clicking through with chance accuracy, even on final round...

Implement our 66/66 rule...exclude pairs that got fewer than 2/3 correct on fewer than 2/3 of blocks.

```{r}
badIDs <- unique((unconstrainedResponses %>% 
  ungroup() %>%
  mutate(numBlocks = max(repetitionNum)) %>%
  group_by(repetitionNum, gameid, repetitionScore, numBlocks) %>%
  tally() %>%
  mutate(lessThan66Pct = repetitionScore <= 2/3 * n) %>%
  group_by(gameid) %>%
  filter(sum(lessThan66Pct) >= 2/3 * numBlocks))$gameid)

```

What is variation in accuracy across tangrams?

```{r}
unconstrainedResponses %>% 
  filter(!(gameid %in% badIDs)) %>%
  group_by(repetitionNum, tangramID) %>%
  tidyboot_mean(correct) %>%
  ggplot(aes(x = repetitionNum, y = empirical_stat, color = tangramID)) +
    geom_line() +
    #geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width =0) +
    theme_bw() +
    ylim(0,1)
```

Eye-ball totally disaggregated game-by-game accuracy?

```{r}
unconstrainedResponses %>% 
  group_by(gameid, repetitionNum) %>% 
  filter(!(gameid %in% badIDs)) %>%
  summarize(matchProp = sum(correct)/12) %>%
  ggplot(aes(x = repetitionNum, y = matchProp)) +
    geom_line()+
    facet_wrap(~ gameid) +
    theme_few() +
    ylim(0.5,1) +
    scale_y_continuous(breaks=c(0.5,1))
```

## Apply exclusion criteria

```{r}
nonNativeSpeakerIDs <- unconstrainedSubjInfo %>% 
  filter(nativeEnglish != "yes") %>%
  pull(gameid) %>%
  unique() 

# 0574-6 is also incomplete (stopped after first message of round 6, so not caught by above rule)
incompleteIDs <- unconstrainedData.raw %>% 
  group_by(gameid) %>% 
  filter(length(unique(repetitionNum)) != 6) %>%
  pull(gameid) %>%
  unique() %>%
  c('0574-6')

firstRoundExclusions <- union(incompleteIDs, nonNativeSpeakerIDs)

badGames <- c(incompleteIDs, as.character(nonNativeSpeakerIDs), badIDs) %>%
  unique()

cat(length(incompleteIDs), 'excluded for incompleteness')

cat(length(union(incompleteIDs, nonNativeSpeakerIDs)) - length(incompleteIDs),
    'more for non-native english')

cat(length(union(firstRoundExclusions, badIDs)) - length(firstRoundExclusions), 'for low accuracy')
```

Count numbers of words, examine size of dataset
  
```{r}
unconstrainedCombined <- unconstrainedData.raw %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(numRawWords = str_count(contents, "\\S+")) %>%
  filter(!is.na(numRawWords)) # filter out pure punctuation messages

write_csv(unconstrainedCombined, '../../data/tangramsUnconstrained.csv')

numGames <- length(unique(unconstrainedCombined$gameid))
numUtterances <- length(unconstrainedCombined$contents)
print(paste0(numUtterances, ' utterances in ', numGames, ' games'))
print(paste0(sum(unconstrainedCombined$numRawWords), ' words'))
```

# Import and preprocess *sequential* data

```{r}
sequentialMsgs = read_csv("../../data/tangrams_sequential/message/sequential_message_raw.csv") %>%
  rename(msgTime = time, role = sender, repetitionNum = occurrenceNum, trialNum = roundNum)

sequentialClicks = read_csv("../../data/tangrams_sequential/clickedObj/sequential_clicks.csv") %>% 
  rename(repetitionNum = occurrenceNum, trialNum = roundNum)

sequentialSubjInfo = read.csv("../../data/tangrams_sequential/tangrams_sequential-subject_information.csv") 

sequentialCombined.raw <- sequentialMsgs %>% 
  left_join(sequentialSubjInfo, by = c('gameid', 'role')) %>% 
  left_join(sequentialClicks, by = c('gameid', 'trialNum', 'repetitionNum'))
```

## Examine accuracy

Accuracy is pretty high even at beginning but tends to go up on average (collapsed across everyone!)

```{r}
singleUtts = sequentialCombined.raw %>%
  group_by(gameid,trialNum) %>%
  mutate(contents = paste0(contents, collapse = '. ')) %>%
  filter(row_number() == 1)

singleUtts %>%
  group_by(trialNum) %>%
  summarize(propCorrect = sum(correct, na.rm = T)/length(correct)) %>%
  ggplot(aes(x = trialNum, y = propCorrect, group = 1)) +
    geom_point() +
    geom_vline(xintercept = (seq(0, 12*6, 12))) +
    geom_smooth() +
    theme_few() +
    ylim(0,1)
```

Look at per round histograms to spot outliers

```{r}
singleUtts %>% 
#  filter(!(gameid %in% sloppyIDs)) %>%
  group_by(repetitionNum, gameid) %>%
  summarize(totalScore = sum(correct)) %>%
  group_by(repetitionNum, totalScore) %>%
  tally() %>%
  ggplot(aes(x = totalScore, y = n)) +
    geom_bar(stat = 'identity') +
    facet_wrap(~ repetitionNum) +
    theme_bw() +
    theme(aspect.ratio = .5)

ggsave("~/Downloads/accuracy_histograms.pdf")
```

Break out by occurence of each tangram for each repetition (some start out harder than others; everyone gets the rabbit right but that weird angel one (B) is terrible)

```{r}
library(ggrepel)
single_utt.toplot <- singleUtts %>%
  group_by(repetitionNum, intendedName) %>%
  summarize(propCorrect = sum(correct, na.rm=T)/length(correct)) %>%
  mutate(intendedObjLabel = ifelse(intendedName %in% c('C', 'B', 'G', 'E'), intendedName, ''))

ggplot(single_utt.toplot, aes(x = repetitionNum, y = propCorrect, group = intendedName, label = intendedObjLabel)) +
    geom_line(aes(color = (intendedObjLabel == "")), alpha = 1,  size = 1.5) +
    geom_text_repel(data = subset(single_utt.toplot, repetitionNum == min(repetitionNum)),
                    hjust = 'left', vjust = 'middle') +
    theme_few() +
    ylim(0,1) +
    guides(color=FALSE) +
  theme(aspect.ratio = .75)

ggsave('~/Downloads/tangram-wise-accuracy.pdf')
```

Look at overall confusion matrix of errors

```{r}
confusionMatrix <- singleUtts %>%
  group_by(intendedName, clickedObj) %>%
  tally() %>%
  ungroup() %>%
  complete(intendedName, clickedObj, fill = list(n = 1)) %>%
  spread(intendedName, n)
as.matrix(confusionMatrix %>%select(-clickedObj) )

# dist(confusionMatrix)
# clickedOrder <- hclust(as.matrix(confusionMatrix %>% select(-clickedObj)))$order 
# 
# cbind(confusionMatrix, clickedOrder) %>% gather(intendedObj, count, A:L) %>% arrange(clickedOrder, intendedObj) %>% mutate(clickedObj = factor(clickedObj, unique(clickedObj)), intendedObj = factor(intendedObj, unique(clickedObj))) %>%
singleUtts %>%
  group_by(intendedName, clickedObj) %>%
  tally() %>%
  ungroup() %>%
  complete(intendedName, clickedObj, fill = list(n = 1)) %>%
  ggplot(aes(x = intendedName, y = (clickedObj), fill = log(n))) +
    geom_bin2d() +
    theme(aspect.ratio = 1)
```

Set exclusion criteria

```{r}
# We detect a handful of workerIDs that occurred across multiple games
duplicate_turkers = sequentialSubjInfo %>% 
  group_by(workerid_uniq) %>% 
  tally() %>% 
  filter(n > 1) %>%
  pull(workerid_uniq)

sequentialCombined.raw %>% 
  filter(workerid_uniq %in% duplicate_turkers)%>% 
  group_by(workerid_uniq, gameid) %>% 
  summarize(time = first(msgTime), numTrials = last(trialNum)) %>%
  ungroup() %>%
  arrange(workerid_uniq, time)

# these are the only full games where a participant was a 'repeat'
duplicate_gameids <- c('0210-5d5ad8b6-7c94-4e2a-a3cb-760d2f613953', 
                       '9087-3c0f9d65-0427-406f-9251-94da5d2dee54', 
                       '7261-07035df8-84ee-4381-b72c-a04234c8f5ed')
```

```{r}
# This one somehow got through wihtout director saying anything
garbage_gameids <- c('7840-4bd35c77-f10c-4055-a122-f591efae2826') 

badGames <- union(duplicate_gameids, garbage_gameids)

incompleteIDs <- sequentialCombined.raw %>% 
  group_by(gameid) %>% 
  filter(length(unique(trialNum)) != 72) %>%
  pull(gameid) %>%
  unique()

cat('excluded', 
    length(union(badGames, incompleteIDs)) - length(badGames), 
    'incomplete games\n')
badGames <- union(badGames, incompleteIDs)

nonNativeSpeakerIDs <- sequentialSubjInfo %>% 
  filter(nativeEnglish != "yes") %>%
  pull(gameid) %>%
  unique()

cat('excluded',
    length(union(nonNativeSpeakerIDs, badGames)) - length(badGames), 
    'with non-native speakers\n')
badGames <- union(badGames, nonNativeSpeakerIDs)

lowAccuracyIDs <- singleUtts %>% 
  filter(!(gameid %in% incompleteIDs)) %>%
  group_by(repetitionNum, gameid) %>%
  summarize(totalCorrect = sum(correct)) %>%
  mutate(lessThan66Pct = totalCorrect <= 2/3 * 12) %>%
  group_by(gameid) %>%
  filter(sum(lessThan66Pct) >= 2/3 * 6) %>%
  pull(gameid) %>%
  unique()

cat('excluded', length(union(lowAccuracyIDs, badGames)) - length(badGames), 'for low accuracy\n')
badGames <- union(badGames, lowAccuracyIDs)
```

anecdotal notes on some sketchy-seeming gameids... 
'9116-d8b8c5c1-f549-48bf-a362-ed159302a106' seemed distracted and kept complaining and talking about how they'd skip through rounds...
'7229-0a7402b0-329e-4b26-bc3a-ae26a388a6cc' was having a lot of fun with wordplay...
'1202-a64916b2-49d2-4ca4-bd76-cfd3e1ec3954' &  '7391-0be22306-9b90-402c-82b0-6bf7ce3ac35e' listeners seemed like they might already be familiar with the task? (e.g. by 'taking charge')
'7971-85165980-35a3-4070-8ebf-ca2d159dc715' gave numbers to all of them...
'3453-2eab60c9-d1cd-4245-919c-c499f303740e' this pair explicitly said to guess a lot... (caught by accuracy filter)

Check and remove all games of duplicate turkers (note that I accidentally excluded some of the partial games before compiling rawMessages, so there are gameids in subjInfo that never appear in rawMessages...)

## First, apply spell check and create filtered version of messages data (for potential hand-cleaning)

```{r}
corrector = read_json('../../data/tangrams_sequential/message/spell_correction.json')
print(paste0(length(corrector), ' corrections'))

# only match on full words
names(corrector) <- paste0('\\b', names(corrector), '\\b')

messagesFiltered <- sequentialMsgs %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(contents = tolower(contents)) %>%
  mutate(contents = str_replace_all(contents, unlist(corrector, use.names = T)))

write_csv(messagesFiltered, '../../data/tangrams_sequential/message/sequential_message_filtered.csv')
```

## Then read in hand-cleaned meta data & record that to json

```{r}
no_meta = read_csv('../../data/tangrams_sequential/message/sequential_message_no_meta.csv')

full_messages_to_delete <- messagesFiltered %>%
  anti_join(no_meta, by = c('gameid', 'trialNum', 'msgTime')) %>%
  mutate(new = "~~~") %>%
  select(gameid, trialNum, contents, role, msgTime, new)

partial_messages_to_swap <- messagesFiltered %>%
  right_join(no_meta, by = c('gameid', 'trialNum', 'msgTime', 'role')) %>%
  mutate(contents.x = str_trim(contents.x),
         contents.y = str_trim(contents.y)) %>%
  filter(contents.x != contents.y) %>%
  mutate(contents = contents.x, 
         new = contents.y) %>%
  select(gameid, trialNum, contents, role, msgTime, new) #%>%

write_csv(rbind(partial_messages_to_swap, full_messages_to_delete),
          '../../data/tangrams_sequential/message/meta-cleaning.csv')
```

Finally, now apply both removals, count numbers of words, write out, and examine size of dataset

```{r}
spellcorrector = read_json('../../data/tangrams_sequential/message/spell_correction.json')
additional_spellcorrector = read_json('../../data/tangrams_sequential/message/additional_spell_correction.json')
metacorrector = read_csv('../../data/tangrams_sequential/message/meta-cleaning.csv')

# only match full words
names(spellcorrector) <- paste0('\\b', names(spellcorrector), '\\b')
names(additional_spellcorrector) <- paste0('\\b', names(additional_spellcorrector), '\\b')

cat(length(spellcorrector) + length(additional_spellcorrector), 
    ' spell corrections')
cat(length(metacorrector$gameid), ' messages affected by meta-corrections')

sequentialCombined <- sequentialCombined.raw %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(contents = tolower(contents)) %>%
  mutate(contents = str_replace_all(contents, 
                                    unlist(spellcorrector, 
                                           use.names = T))) %>%
  left_join(metacorrector, 
            by = c('trialNum', 'gameid', 'msgTime', 'role')) %>%
  mutate(contents = ifelse(!is.na(new), new, contents.x)) %>%
  mutate(contents = str_replace_all(contents,
                                    unlist(additional_spellcorrector,
                                           use.names = T))) %>%
  mutate(contents = str_trim(contents)) %>%
  filter(contents != "~~~") %>%
  mutate(numRawWords = str_count(contents, "\\S+")) %>%
  left_join(singleUtts %>% 
    group_by(repetitionNum, gameid) %>%
    summarize(repetitionScore = sum(correct))) %>%
  filter(numRawWords > 0) %>% # filter out empty messages
  select(-score, -workerid_uniq, -contents.y, -contents.x, -new)

sequentialCombined %>%
  write_csv('../../data/tangramsSequential.csv')

sequentialCombined %>%
  filter(role == 'director') %>%
  mutate(contents = str_replace_all(contents, "\\.\\.+", ".")) %>%
  mutate(contents = str_replace_all(contents, "[[:punct:]]$", "")) %>%
  mutate(contents = str_replace_all(contents, fixed('.'), ',')) %>%
  group_by(gameid, trialNum, repetitionNum, intendedName) %>%
  summarize(contents = paste0(contents, collapse = ', '),
            numRawWords = sum(numRawWords),
            correct = mean(correct)) %>%
  write_csv('../../data/tangramsSequential_collapsed.csv')

numGames <- length(unique(sequentialCombined$gameid))
numUtterances <- length(sequentialCombined$contents)
print(paste0(numUtterances, ' utterances in ', numGames, ' games'))
print(paste0(sum(sequentialCombined$numRawWords), ' words'))
```